{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# OCR Model Inference\n",
                "\n",
                "This notebook demonstrates how to load a trained model and run inference on sample images from the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torchvision.transforms.functional as F\n",
                "import json\n",
                "import os\n",
                "import pandas as pd\n",
                "from PIL import Image\n",
                "from torchvision import transforms\n",
                "import matplotlib.pyplot as plt\n",
                "from model import OCRModel\n",
                "from train import LabelConverter\n",
                "from train import resize_and_pad_aspect"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "# Load Config\n",
                "with open('config.json', 'r') as f:\n",
                "    config = json.load(f)\n",
                "\n",
                "DEVICE = torch.device(\"cpu\")\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vocab size: 40\n"
                    ]
                }
            ],
            "source": [
                "# Reconstruct Vocabulary (Ideally save/load this)\n",
                "train_df = pd.read_csv(config['data']['train_csv'])\n",
                "all_text = \"\".join(train_df['GroundTruth'].astype(str).tolist())\n",
                "converter = LabelConverter(all_text)\n",
                "print(f\"Vocab size: {converter.vocab_size}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded model from checkpoints/ocr_model_fold0_best.pth\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
                        "  warnings.warn(\n",
                        "/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
                        "  warnings.warn(msg)\n",
                        "/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "OCRModel(\n",
                            "  (feature_extractor): ResNetFeatureExtractor(\n",
                            "    (backbone): Sequential(\n",
                            "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
                            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (2): ReLU(inplace=True)\n",
                            "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
                            "      (4): Sequential(\n",
                            "        (0): BasicBlock(\n",
                            "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (relu): ReLU(inplace=True)\n",
                            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "        )\n",
                            "        (1): BasicBlock(\n",
                            "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (relu): ReLU(inplace=True)\n",
                            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "        )\n",
                            "      )\n",
                            "      (5): Sequential(\n",
                            "        (0): BasicBlock(\n",
                            "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
                            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (relu): ReLU(inplace=True)\n",
                            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (downsample): Sequential(\n",
                            "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
                            "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          )\n",
                            "        )\n",
                            "        (1): BasicBlock(\n",
                            "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (relu): ReLU(inplace=True)\n",
                            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "        )\n",
                            "      )\n",
                            "      (6): Sequential(\n",
                            "        (0): BasicBlock(\n",
                            "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
                            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (relu): ReLU(inplace=True)\n",
                            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (downsample): Sequential(\n",
                            "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
                            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          )\n",
                            "        )\n",
                            "        (1): BasicBlock(\n",
                            "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (relu): ReLU(inplace=True)\n",
                            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "        )\n",
                            "      )\n",
                            "      (7): Sequential(\n",
                            "        (0): BasicBlock(\n",
                            "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
                            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (relu): ReLU(inplace=True)\n",
                            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (downsample): Sequential(\n",
                            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
                            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          )\n",
                            "        )\n",
                            "        (1): BasicBlock(\n",
                            "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (relu): ReLU(inplace=True)\n",
                            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "        )\n",
                            "      )\n",
                            "    )\n",
                            "  )\n",
                            "  (adapter): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
                            "  (pos_encoder): PositionalEncoding(\n",
                            "    (dropout): Dropout(p=0.1, inplace=False)\n",
                            "  )\n",
                            "  (transformer): Transformer(\n",
                            "    (encoder): TransformerEncoder(\n",
                            "      (layers): ModuleList(\n",
                            "        (0-1): 2 x TransformerEncoderLayer(\n",
                            "          (self_attn): MultiheadAttention(\n",
                            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
                            "          )\n",
                            "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
                            "          (dropout): Dropout(p=0.1, inplace=False)\n",
                            "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
                            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
                            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
                            "        )\n",
                            "      )\n",
                            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                            "    )\n",
                            "    (decoder): TransformerDecoder(\n",
                            "      (layers): ModuleList(\n",
                            "        (0-1): 2 x TransformerDecoderLayer(\n",
                            "          (self_attn): MultiheadAttention(\n",
                            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
                            "          )\n",
                            "          (multihead_attn): MultiheadAttention(\n",
                            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
                            "          )\n",
                            "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
                            "          (dropout): Dropout(p=0.1, inplace=False)\n",
                            "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
                            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
                            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
                            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
                            "        )\n",
                            "      )\n",
                            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                            "    )\n",
                            "  )\n",
                            "  (embedding): Embedding(40, 256)\n",
                            "  (fc_out): Linear(in_features=256, out_features=40, bias=True)\n",
                            ")"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load Model\n",
                "model = OCRModel(\n",
                "    vocab_size=converter.vocab_size,\n",
                "    d_model=config['model']['d_model'],\n",
                "    nhead=config['model']['nhead'],\n",
                "    num_encoder_layers=config['model']['num_encoder_layers'],\n",
                "    num_decoder_layers=config['model']['num_decoder_layers'],\n",
                "    resnet_layers=config['model']['resnet_layers'],\n",
                "    max_len=config['data']['max_len']\n",
                ").to(DEVICE)\n",
                "\n",
                "# Load Weights (Adjust fold index as needed)\n",
                "fold = 0 \n",
                "checkpoint_path = config['training']['save_path'].replace('.pth', f'_fold{fold}_best.pth')\n",
                "\n",
                "if os.path.exists(checkpoint_path):\n",
                "    model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
                "    print(f\"Loaded model from {checkpoint_path}\")\n",
                "else:\n",
                "    print(f\"Checkpoint not found at {checkpoint_path}. Using random weights.\")\n",
                "\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inference Function\n",
                "def predict(model, image_path, converter, device):\n",
                "    transform = transforms.Compose([\n",
                "        transforms.Resize((config['data']['image_height'], config['data']['image_width'])),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    \n",
                "    image = Image.open(image_path).convert('RGB')\n",
                "    #image = resize_and_pad_aspect(image, (config['data']['image_height'], config['data']['image_width']))\n",
                "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
                "    \n",
                "    # Greedy Decoding\n",
                "    # Start with SOS\n",
                "    tgt_indices = [converter.SOS]\n",
                "    \n",
                "    for _ in range(config['data']['max_len']):\n",
                "        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n",
                "        \n",
                "        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            output = model(image_tensor, tgt_tensor, tgt_mask=tgt_mask)\n",
                "        \n",
                "        # Get last token prediction\n",
                "        next_token_logits = output[0, -1, :]\n",
                "        next_token = next_token_logits.argmax().item()\n",
                "        \n",
                "        if next_token == converter.EOS:\n",
                "            break\n",
                "            \n",
                "        tgt_indices.append(next_token)\n",
                "        \n",
                "    return converter.decode(torch.LongTensor(tgt_indices).unsqueeze(0)[0]), image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/jt/_vhzybnn7dg3qwfs2vtf58q40000gn/T/ipykernel_1275/1646321367.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
                        "  img_name = row[0]\n",
                        "/var/folders/jt/_vhzybnn7dg3qwfs2vtf58q40000gn/T/ipykernel_1275/1646321367.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
                        "  gt = row[1]\n"
                    ]
                },
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAXJCAYAAABsWX4xAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs/QncJGdZ7//fVdX7sz+zTzKZyb6SkATCTlhlk+OCigqyuKIgiEc5KiogLqzHDfEoKCjqQUE4ooKiQETWQEJWsk5mMjOZzPrMPGuvVfV73c1/8p/MTH2vh6f7zmTI5/16DUPm6qqu5a6qvrq6+xvleZ47AAAAAAAwdPHwZwkAAAAAADyabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAeRrZs2eLe/OY3u1NBFEWnzLICAHCy0HQDwMPAtm3b3Gte8xp33nnnuUaj0f9z0UUXuVe/+tXupptu6j/maU97Wr/Jsf6spAn653/+Z/fCF77QrVu3zlUqFTc9Pe2e+tSnune/+91ubm7uuKbw6OcbGRlxV111lfvrv/7rwvmnaeo2btzYf/ynPvWpEz7GL7ev+2VYWlo6ru6f97u/+7sf9G9HL0epVOov95VXXule97rXuW9+85tynd/73vf2p3vc4x5X+JiVzH///v39+gUXXODq9bpbu3Ztf/v8r//1v9zCwoIbliPb68ifI2Pm13/914/bZ6f62C9a53K53B8Xr33ta93hw4cLn+MNb3hD//EvfvGLT1jfvn37A/P87d/+7RM+5iUveUm/Pjo6+qB/P/q4jOPYjY+Pu/PPP9/92I/9mPuP//gPue633XZbf7parVa4/Mce935MXXrppe4P/uAPXJZlJ238AQCWr/RtPBYAEMC//Mu/9JsB39T5F/aXXXZZ/8X77bff7j72sY+5P/3TP+03Jm984xvdT/7kTz4w3de+9jX3R3/0R+7Xfu3X3IUXXvjAv/sX5MvlX7T/xE/8hPvgBz/oHvWoR7mf+7mfc5s2bXLz8/Puy1/+cr+B++QnP+k+85nPPGi6Rz/60e5//s//2f//999/v3v/+9/vXv7yl7t2u+1+6qd+6rjn+exnP9t/nG+Q/vZv/9Y973nPK1ymffv29df5yPwtz372s93LXvYyl+e5m52ddTfeeKP7q7/6q35T/fa3v9394i/+4gmn88vhl+faa691d999tzvnnHMGnv/MzIx7zGMe0296f/zHf7zf+Bw8eLDfPPp1+tmf/dnjmrZB+fn6efqG6tOf/rT7nd/5nf72/uIXv9hv0r4Txv7mzZtPuM6Li4v9sfnHf/zH7vrrr3df+MIXjnsOv9/+7//9v/197d9c8mN7bGzshMvjm1//WD/uj+af55/+6Z/69RM5/fTT3e/93u898Fg/nvzy/83f/I37oR/6of7f/g2CY/l/X79+vTt06JD76Ec/+qDju2j+Bw4ccH/3d3/nXv/61/cbbL+/T+b4AwAsQw4AOGnuvvvufGRkJL/wwgvz3bt3H1fvdrv5H/7hH+Y7duw4rvaRj3wk96fxz33ucyt+/t/7vd/rz+P1r399nmXZcXW/TG9729se9G+bN2/OX/CCFzzo3/bt25ePjo721+NEXvayl+VXXHFFf138+i4sLBz3mDe96U39ZXn0ox+dr1u3Ll9aWjKf1z/+1a9+9XHzOnDgQP6EJzyhX//Xf/3X4+r33HNPv/axj30sX7NmTf7mN7/5hMv97c7/He94R//fvvjFLx43zezsbN5sNk/4PMeup98WliPba//+/Q/69+///u/v//uXvvSlwmkXFxfzYfDPs5xlHdbYL1rnF7/4xf1//+pXv3rcfD772c/2a/7vcrmcf/CDHzzuMdu2bes/5si2u+GGGx5U/9u//dv+tC984Qv7y3y0q6++Or/44ouPm2ev18t/7ud+rj+/N7zhDcfV/fG2ZcuW/Bd/8Rfz7/u+78uf9rSnnXA7nWj+fhz5cTI2NtZ/nmGOPwDA8PHxcgA4id7xjnf074x94AMfcBs2bDiu7u8A+o/O+rvPy+Xvxvo7hf5vxX+E29+pvfjii9073/nOE94V9cvkP5ZqWbNmTf+u2tatW4+rNZtN9/GPf9z98A//cP+un/9vf9ewyG/+5m+6vXv39u/MrdSqVavchz/84f72O/pO4NF3uaemptwLXvAC9wM/8AP9/x7G/P36J0niHv/4xx83jf/YcdGd0mF6xjOe0f/b3yE+8vHkSy65xF133XX9rwz4j2/7T0d4/pMJb3rTm/p3+avVan+c+Y9i+38/mv9vf2fV72d/l/h//I//4Xbt2nXC5/djb8eOHQ/p2H/KU57S//tE48/vW/9x9ac//enuWc96ltzXT3jCE9yZZ57Zv5N87Dye+9zn9r9esFx+HPhPovjnfs973nPc8eg/ieA/1u6PC//n85//fOE2PZYfR4997GP7d+39J0MeTuMPAHA8mm4AOMkfr/UNj/pe8bfLN7j+4+b+b8V/FNd/j/RHfuRH+i/UB9Hr9foNg29kj/WJT3yi/9Fn31j4j9L6JlA1Pr6B8o2jb8p8g75SZ5xxhrv66qvdV77yleO+4+yf//u///v731/363/XXXf1P64/6Pz9x6D999c/9KEPuZPlSOPp3xg4wn/E2H+k338twH8X2Deg/qsFvnl+17ve1f8+v/+I9vd+7/e63//93z/uu8/+Y89+uu/6ru9yb3vb2/oflfZvWJyIH3v+4/gP5dj3zat37Pjzbxb84z/+Y38fe/5v/9H7PXv2FM7LP8a/ofKtG/nf+ji3/9j+j/7oj37by+WPKz8//wbXsR9992Pw7LPP7jfPfvv7N0P8R9uX68j30CcnJx/4t4fD+AMAHI+mGwBOEt+o7d69u38X8li+GfYv9o/8GaT5LOLvSHrHPr9/0X70c/s/RxqQI7rd7gO1W265pf/9Ud/I+LvGJ/re6hOf+MQH7lj65ts3Mf77qEX83Vd/t/v//J//M9A6+nXzzeWRpszzd3z9uvvl8J785Cf3vzP77d7tPtH8/Xbwd4Nf8YpX9JtP/x1a30hZnzoYhP8er98Pfhn+/M//vP9dc/9jdEfu/np+37z1rW/t33n96Z/+afc93/M9/bu5//mf/+n+/d//vd9o+3/3jbe/K+s/ifClL32pP63/Drvfh/77/n4b+R84843sicbtQzX2j6zzvffe279T/id/8if97e7v5B/b2Pv5HdnX/k0F/4aBb6qL+Oba36n3d6K9f/iHf+jfIfZvUKzEkXU8+i68P34+8pGPPLBc/gfP/PyLxuDRx+Qdd9zR/zTC17/+dff85z+/P+0RJ2P8AQBsNN0AcJIcuTt6oh828neD/YvnI398U7Fc/gW3b5L93yt5/ptvvvlBz+3/+DulR/NN85Ga/wE2f2ftla98Zf9j6kfz0/mm7sidRu9FL3pR/w6db2aK+ObJ340d9G73kXXzH8M9wjc2vin18/eO/Kq1b8R8czPI/P18fZP6qle9qv/jWP5NA9/E+V+Q9k3vsW9eDIP/pWy/H/zHon/mZ36mf/f4X//1X/t3To/wHx33++dovunzjZn/WsDRTe6Rj6d/7nOf6//tf0jP8x/1Ptov/MIvnHB5/Dpec801Qcf+kXX2P47mG02/zv5X8Y9e5yP72v+w2JEfyfMfjfd36NUbLP7rFv7HCI/cdfZvTvg3KY6d9yBj0C+rPzaOPi78//dj59Zbbz1uHv5NoiPbw+8vf5z5Jt3/AOLRTsb4AwDYaLoB4CQ58gvKJ4rx+bM/+7N+3JC/w/hQP79vUPxz+z8+9uhE/EeCff3f/u3f+h9P9h9x9S/y/ce1j/b3f//3/bt6l19+ef8Xnf0ff5fST2/dWfbxUP4O7SB3u4+s25F19U21b659w+2/83xkmfzy+Dvrx/5K+7c7f89/P9l/H93/Wru/K+nvLvtmyX9X/S/+4i/csPm7zn5f+EbXr4v/5IGPNTvaaaeddty+8R+p9w3esW+w+Ogu78h3hf3dZP+L4v6j0Mc2vidr7B9ZZ98Q++8v+2U9+o6v5+9w+zcM/FcAjuxn/+dJT3pS/y7xnXfeWTh/36j6NyX84/0d/5V8tFyNEb9u/k0S/2bIkeXy29c39ic6LvybC359/RtY/pMMfn/6T4qc6DvaD/X4AwDYiAwDgJNkYmKi/wLZN0nHOvI916M/Fj1s/o6Z55/f38k7+s6c/8Ep70QRTN7q1asfeMxznvOc/rx8hvYf/uEfPihC60gD4RudE7nnnnvcWWedVXi329/19He7/Z27lfDr5r9X6xuco6PLfON9oo8Y++X131te6fyP5u+g+wbW//F3V88999z+/ItioVbKbye/P5RjG1LPfyzef0rhf//v/33Cab6dH+97qMf+0evsvw/t18NHjvmvDvg3CDzfNPvvdPusef/nWH5fvOUtbznh/P1d51/91V/tx9/578Z/O2PiWEfW8cjddn+X30eXtVqt/pg4ln8jwf8439E/bDgyMvLA8XbkeLriiiv6P4jnm+oTeajGHwDARtMNACeRfzHsM659VvRVV131kD63/86vb3588+kbjCPNykrXw99R/N3f/d3+R5x9k+DvJPu7hK95zWv6tWMbPn8X3TcYx2YiH3u32zfe/u7nt8t/L/e//uu/+r9IfeQuo286/EdtT/SRZZ+r7H98zt9ZP1GTupz5F/FvLPgf+fIN/8OFv7PqP4r8zGc+U+Z5+x/n8vvLfyf56Lvb/i7qw2Hs+zeJ/G8A+I/P+68sHPmetN/X/vvUvnYsP5782Ctquv2P5PnG1n96wH8v2v+S+kr4T1b45/F3sP1vBxwZZ77h9nejj32zxG9Tfzz475MfefyJ+I+/v/SlL+2vxy/90i/1l/dUG38A8EjCx8sB4CTyP4jkX5D776X6jzcfayXfwVxuZJh/Xv/8/k7cr/zKr5zwub6d5/fRYv57qu973/sedJfbP4f/gbWj//joMN+IWx8x94/xTbePNvONynL5j7D7u5W+6XnjG9/Y/zf/3XDf8Pg78scuj//j3xzw37v1v7a+kvl7X/3qV/sxWMfyjaXfNoN8JHvY/D647777HthfR/Pb6sh6+F899469o+p/zXyQyLBhjn1/l9v/GJ4fJ97OnTv7EVx+HU+0r32D7j/S7fdXkd/+7d/uN+w///M/71bCjw3/Pfjbbrut/7eP7Dry0XLfBPtPbxy7XL6B9m8iLOdH/fz281/dOPqTCqfS+AOARxLudAPASeQ/8unvhPkGzr8g9s3DZZdd1m84/J1iX/N3oH1DsVz+bq1vKvyvOls/puabbd8U+B9m8j+O5n/kzD+X/3729ddf3/+Irr8zvJx8X9+c+TuLvgnwv3DtGwcfUVX0MWX/Q1C+ofHP4z8qW8Q3Pkd+9OxE/HdzfSPjt5n/6K6/e+uX23+X1i+Lz1f2fDPtm+qiX6H23w323331y310ZNZy5+/5H5Tz03/f931f/3vV/nvUfvv+5V/+ZX8bHsnHfjjwnzTwd4Z98+d/NM3f2fWNom+a/b/77w/7HyHz+9CPT/9dYv9Gjv8lev/dd9+0noj/cTb/Zon1Y2rDHPv+F8lf97rXuV/+5V/u/86A30d+PkX72v/qt7977fdVUWSZX4djP6FRxG+XI99B9/Fgftv4N3j8pwP8nXf/I2ae/8V2v62P/VG6I/x3vP3XNfz48m9y+PUq4vO//Xr4Twv8xm/8Rv9j8KfS+AOAR5QcAHDS3X333fnP/uzP5uecc05eq9Xyer2eX3DBBfmrXvWq/IYbbjjhNB/5yEf8rcD8c5/73IP+/QMf+ED/3/3fy/Xxj388f/7zn5+vWbMmL5VK+eTkZP7kJz85f+c735kfPnz4QY/dvHlz/oIXvOCE8/ngBz/Yf+53v/vd/b9/4zd+o/A5t2/f3n/M61//+v5/v+lNb+r/9/79+4977NVXX92vHfu8/t+O/InjuL/cl19+ef66170uv/XWWx/02Be+8IX9bbu4uFi4TK94xSvycrmcHzhw4Nuev3fTTTflv/zLv5xfccUV+fT0dH9bbtiwIf/BH/zB/Prrr8+Xw29fvy0sansdu+0uvvjiE9Y6nU7+9re/vV+vVqv51NRUfuWVV+Zvectb8tnZ2Qce12w289e+9rX5qlWr8pGRkf623LlzZ//5j11W/2/+OUOMfbXOfnknJib6z/2oRz0qP+OMM+TzPu1pT8vXrl2bd7vdfNu2bf35+vGuvPzlL++v/4nG5pE/o6Oj+bnnnpu/9KUvzT/96U8/6LFHjovPfOYzhc9x5Bj6p3/6J3P/XXPNNQ/aB8MYfwCA4Yv8/5zsxh8AAPz/f6naf0LBf58dAACc+vhONwAAAAAAgdB0AwAAAAAQCE03AAAAAACB8J1uAAAAAAAC4U43AAAAAACB0HQDAAAAABAITTcAAAAAAIGUlvvAz77pu2W91VrST1StFRcT3fvHsa5HUeQGkWWZrKdpuuJpXa6XvdVpy/rCwoKst3tdWa/X64W1yekpOe3Y6ISsVyrGzwF09LK7tHjdrZ8aiKJE1pNE1+N42UP/25b29JjoZamu93oDPb9at1KpNNixZtQ7qV72zK38JySsMTHoz1MMOv2g56FT9r1Z4xzn3HfqdtHXhlObcSzk1nrrc2BsjhnjuqpEetpowF+xsc8TxrVroHkbz2zsl9iYfRYV75e0Z+wz4/xnX1sGW/cs6w1w7ciCXpMjsV2t86u57Jne7nlunH+NMTM5Nl5YS+R62a9fW62WrFcqlYHGjLomNxoNOe3CwtyKX1svZ8yo/VqtVgaad7PZHGi7mn3Nw1m08mW3Xh9b16Wn/9YXzOfgTjcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIEsO6w4M7ICR8ZGZT0V8WZ5PGC+o5WPa2SCxkZwZxRnK86zK5ersr5q/fqBcuPmjRzE+fnFwlq3o7P+ZmYOy3rudEZ4zRhded5acfakNSaqpbKsJ0kaLHO5WhaZ9J6Zm+kGEkfFGz5yOr88st6Hs2LprZxZa9UHzKkdZN4hn/s7O7OZ7fadJzrJ79cPMP+Bh6ORMR4PluWqXm7kA2TMfovxWinLV77fEyun27i2mK+zBt1xxcuXZelJPYXp14l6n+dZMlDGuJnTbQy5JCl+LZUYs7ZevyaJNWaMDHLrcBHTW9f7NM0HyhgfRKfTkXWr77BeHz9Sc7gtuZFZPwzc6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAA4GTndOeJ8VAjjy/uFWc6m5HIVjyklf1rzd+KFs6TFecaH56fl/WW2C5epaIzn3tG/mSpXCms1eoNPa2xTyNjv1SrOiu7lxdnEea9fLCcwQGDN7O0t+Lnn5tdGGi7VsU+8yoVXS/FxeO119PrtdRsy3q3q+uN8bqsR1awpsqwtfapUTeT1wfMal15svt3uO/kGG8z9/g7UxQNdlEOe6wYz21GVevp40ifv3Onr8lq7pGx7OY50GDE98pM58TKJ4+N7W5teCN/11p3K9N5oIxwa9ky67lXnhc96D63tktkDAp1yU4q+jVetVofKEPcWnbr9Uiapit+LdRoNFY8729ZeZ60tWzWdimX9X6xM8rD51WvOId74Etu8fxz0et9qz749Z473QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAwMmODHNl/VPqzXZL1quVZMU/EW/9PL4ZCWb8dH9uxp/kK44MW7VmWtYXFpZkfX5xTtbLRiRDvVpbcSzAvBF3ttTRcWep9ZaOiBiJrdgXK9bFmN6K7aobcWcVEckwtWZSL5sVcWfFOXR7K94vaWoda3q9y9XSgJEKYeNRQs475LI9vA22T7+jI8OwsouyGduVn7x7CeayWWX9WklduqxTjJ2KZcTtGNFWav5WTJy1y+2YucGuDbkVRznAvAeNHLMjxQaI/DLrRpzvAK+1EiNKuFzOgkZTWdFaeV5cT41Y2smpcVlfWloaMJ6v+PmXlvR6xcbMrciwTqc4rvfUl618SiuKeAi40w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAwMnO6U6NLL+eEW9WUVncA+QrLiu8MjfeWzAyFiO17ka2ZFnlk/sdYOQeJxWdt1cy8vhmDh4qrH3zm9+U095553ZZP6RjvF1Pr7qLKsW1ipELX04Gy9keH9cZjGtXr5L16YniLO4tmzbJaavGPivFet3zVI/3VAak6+euVitGXW/3brt4vA0qdE72IzeHG/j2RFEy2PTmI8LlpdpZ19GA9eyk3QexzmHRAPO38qCXsVNPGjvjOx9o3QeZPhrw9auV2RzHg+V0J0myopqXZcZzG691siwNtl+t7d5qtQbK6a4ar0FLpdKK88sHzW638s0Honq95Rj4ZdjKZ5Dng2335eBONwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAJz2n28iNKxv5vmneKazFRq5aZmaj6dVIjRzDPNJ5evkAAZRZW9cPHFqQ9dlDOvd4bm5O1u/bubuwtm/frJx2eqIu65dedq6s751flPWWGFILC3ra/QcPyvr9B/R2dTt1fWyseLt5E6OjhbXrb7xFTrtp/XpZP/usLbK+ce1aWa/VRwpradaV0/Z6+jjvNZuyXo6tLFfjWFTnggGzVocQAHnS5MZ2O5miXOdy2ks+wLrlYcOBs2iwdbemH8jA677y7R4b620ZdMnzAdYrNdfbqA+a1ZoVL31sZHxb483KdI6Mc2Qi5h8lxljXS2Y/wjo9D5DZnLvB8p4tVta1uq5Z2enWqb9kDMckHmjIOPXyOCsZx1palfWesdN7xpjpGuueitcjeaJf9+8/MDNQTvf4+Lis18XL646Ro50lesVLrrjf8lqpfh1XLen9FlmDZgDxgNeWwXLCBzxHLcPD95UcAAAAAACnOJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAADgZOd0R5GRp5fpXLmSyJXrGmGm1ZEJWV/UkXRu7yGd+Ty1ZlrWx8cmC2v33XefnHbH1rtl/ZpPf1HWF3RUoDv3TP2+yWMvf3Rh7apLV8tpH/eYx8n6noN7ZX22pbOwK6ONwtpcS4+nA7Pzsr5jt162r113vazv3qMzGONyce7nvTv0et96q85ef56RKbr59HWyvrhQnL9eTvQhP7VKj4n9+3R+eWacURIjxzsTGbaRqHm1qs6WrNUqst5rt2R9fl6PuclV0yuettXR+emlSk3Wq3VdV9GXPSP/Vk7sp1/Q43mkpvdLuaSXvdst3u/Nlj75W/G7VWPMVI0xs9jS54lKrTgLdmFxTk47PaWPRSPK1e3fpy8eE1NrCmtRrsdE2tPbvVLV16VyordrbrwmSMWOTa2XNcZzZ5EeE/tn9LF82mmbZF2dx7ZvvUNOe97mjbLenT8s6+VM7zd1echjfTA1O/r8udRqDnQsjo6OyvpCs/i6u7ign7tS0WPCSmbvNPW6T00Wv4aslPX5b+6w3qeVit5uE2MiENpvG+O6t9Aqfj1Rr+vnnu/oY7GZFr8G9Hbu2ifrrbZ+rXXxBWcU1rJIHwudXF/XrvnCLfq5Lyx+bu+8884prMVlvc8qo3q7zrT1dquO6YzyuKfP3zN7i7fN2Ihe9slJnV/eXNL7NDKytK3XgepY7aVGMzkE3OkGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAAOCk53Qb2Wh5rkNDu93i3M+RiSk57byRxTqzqDNFJ9adLuvlhs4Bv/nuewtrN934DTntvnt3yLoRR+2uftq5sn7phefL+qMuvKiwlnb1k+/ff1DWDx3S+ZGr1+k8vlKjOJ+yVNGZoONTq2T93AsvlfUNp2+W9U9+8pOyftstxTmF0zpO1F10/oisbzlDj9fEWXnVxRmMPSPcd+6wzqZMkkTWR+o6dzNJ9LKn3XZhbWFB5+Pu379f1iMjj1ptN69U0eu+f4/Ihk/0+5uVms62jEv6VN1q6azVZqt4u0aJXu8RkTXtNeq6HmXFz+2lOqLclURu8tSYPsdkRsJup6OvLYvzOjO009XbPSmJ62Zm5J8bmaFxrPN9R0b0eWa0Xnyi6orj0Fts6nzytKmPtabTuclpzzhPRMXHYlzR6x2VjYzwij7WahM6P/362+6R9b3331dYy4zc90kjq7ru9HbPcj2mMnEe6RgZ32Uj075e1eM1LiUDHatpWjxmajV9XarV9fm3bJx/D/dmZH1psXi7dhN9ArRep2WxkYW9qMfUnJGLPLK++LVWYmQib925S9Y//983yPq+A3Oy/pgr9Ou8ymhxPnqveUBO+4+f+KqsGy9l3Ib1p8n65NSawtrMrJFPblwb8lgvXFTRr0emJqZlPc6Lr/ml2LqXG8lqr6evi3FknOOM4yXtdVa4ZM5FkbHTl4E73QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAAnOyc7tjId3SZkdMtMnJLke79UyPItVrV+ZDj4zrb8p4dVpbgNYW1Hdt3ymmbOvbYnXm6zqa86JLinG1vbFTnS7ZFjmxspNIttBdl/fTNG2X90OE9sl5NivP4lhZ1TuxCTy/76g16TJx/ztmyvvjkJ8n6ePUrhbWDe3T+46ONfbpxQ3F+o9fr6IxbpWtMu9DSuZijRk7s4YN63cvGGacmMnRHjSzrhpGlmuX6HFWrGHnUI/r5d993f2GtZOV0W9mWRoCkEfnsEjGD2Mh9T3Jj5rnOzex1jRxaK5czKZ5/OdUbxlr0jpFH3TaOl4kJnQldEdnFeW5l1uvtlhvXzZqRm6yGXM/acMZ4zo16kujrXjnWx2IeFR/rnVyfBzq5lROrx/Mtt90h6//+6f+S9ZYYUk99/AVy2saUzqWv53rMRB29XzoiX72S6H2SVPQ+tcZ7s6OPxcw4ySXi4lI18qRjYzwuNYtfR3kdY9nkay3j3B+V9XavGBnjjRF9jorrOsP8kMhuzyt6nx46PC/rd965X09vtB2XGtnt6vx96OBe/dzGa/dL9cs4N72qOCPc0unp7ZoZ12xX0uf+blePuR2HdE646xSP52pFn38j4/VCu63row29bpWaHu9JUrx8S0u653FG/vlycKcbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAA42ZFhiRG34yL9U+oV8TPti4sLctp6Xf/0/uS4jljasVdHA/znZz4j6zfeWBwLNqZ/nd7pH7937olPfKKsn3vhubLenteRDIvtpeJpRRSE12zqCI9kXkevlEpGTJIYfSONmpw2a+nxmBsxc42ajsk4c9PpxvNfXFiLL9Db9fzzzpL1aSPirtM2IsPS4m0T13R0SrWi40dG6nr6aETHm7SMY70rIkBKkX7uek2Pmbynx0TXiI+andH7dWqiOM6n2dbRJq2m3qdRrE/VSUVvmxGxbdJUx910Wnq7NCr6PJAYUW6lREeAZFnx9F0jVquXWdclPV6np+sDxVmmafEVIIr0dlsyzs8l65pb1eeRlpj/ghGdMj49JuupEQvjjNguK5Isy4sjnvLciGAScWNeuaTPIz0j4uk+nZroVOLNxjPP0MvW0C845mZ2y3pqvF5IeyJitKRjteKOPhZcrPd5ZhyrubHd1X7rGnFl+w/ofKixEX0s1cf0a9SSGO+9nn4t01nS14ZD88Wv8frzz/R2KxlRnFlWPGC7RqTXSGNK1qen9HUrO9Qe6DXmwkJxBGrbeB31jGeukvXT1q6X9cVFfQ5titewuTi/eXGsz1FRRW+XpSX9Osz19Dlyarx4v9aN14gt49qStvSgmp3X+21uTq9bu1VcX7d2tZw2Ms4jy8GdbgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAATnZOd9nISIwznfXqKsVZf4tGTuykkZG4YOSZfuUrX5H1m24qzuH2VDSbsVncah375i697DJZb4vcYi8ysjMb9eI86uqozmdcb2SCHj6kA0l3brtL1hcXizNDZ5d0dmVTZPd69XG94deu2yDrjYbO8d582qbC2sbVOrNzafagrM/Ozsp62Yi47bSKcwzL1cpA690zsqwbFZ17rKd2rivyrHvGsVBrG1nW5luMxjnMmrpXPH1iZDInkV44Kx4yNx4QxcXnidg4iWWp3u4Li3qv5iI33isb+6WkxpRYr/5zG9elzNruxvvSc/M6c1RFkJfL+ljJI71uSVkfy+WqPr+3W8XbJjWyrl1Z58R22jo7uN3WY8Z6OZGK3OO4rHNia+Pjsj46NS3r6zedJusVfQp16uVKnuhz2OyStV31sVot6fmPjxVnEy+K64pXMsbz6PjYQOeweSP3WO33clmvd6WtB1yrq+vG6d1l4tpl5XRXIn2cl4zzQBbremIM2KSTFtbKVX0sjY3rdcsjvV+6HSun23gdWC2ul6Z0X7Fm6iJZj41c+f27D8v65OS6wlp9VB8rC22dRZ1U9Pl5ZnZG1sdrer+2xOvv+UN62Q4f0q9/867u50ZrejyPj+jxPDVVnDGeZ/r8mUeDvUb0uNMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAMDJzumOjCBCK3O0JzJHRxo6L6/b0Xl413/jZlm/9tpvyrqVtb12TXH+4759OkfwiqdcKOur1hZn9Xnb77xd1qcn9LaLusU5iXNzOkew0tLrdvftetm+es0XZV3FLud6OLmmjn90B+d0fXxaP8Hll+r89LO3bC6sldavldN2Or2B8qJLdZ1T6EoipzY2NqxxMLREZqd3eOaQrE+M6ez31WsmCmt5ng6UIR47fR5JEiNs1aCyh2tG5n0j0ftlcUln5M4b9SwuHlP1ul628Qmdezkzo3M1s1Rv99jIka1Xi58/SnQWddbSuZutjl72jnGiGV+9RtZ7vW5hrVbTWapZU+/T3JVXfh7w19WoeN1qjUk57eyCkc1uXFOr9bGB8nfb4rq20NT7vLeot2uazOvnbhXv0+VkjKtNt2Ts89SY99hEcc52X1NfGHMRDb/pzHPktAtGjra1Xe7ft1vW77jjLlkviUzmM7ecLaetGtm/lYqRhZ3p80SrXXwOjEuJvmZO63OMddlqL+lj9fC8Pgfece99hbVNZ+prR6oGlL9uzesxoxOf7f1WLhc/f6+tX0+kPb3dKiV9/q5W9fl31arVhbUDs3rN775nl6xPbSh+HeXt2aPPA3//Od0zbd5UfKydcfp6Oe2ZZ5wm62efbRyreki5wwf2yfrsYvH5fbyh92kU6TGzHNzpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAADgZOd0JyWdCZqUdF7ewmJxtuX0pM4E3b57Rtav+9otsr5/ryy7qXX6vYdIbCaVzeudfdb5sl5KdC7c1LTOfJ6e1Dnd3/zmrYW1//rCf8tp9+zdL+sLs7LsLtyih9fmtcX5k2ecrbfbwXm93T/3hWtlfdt2nbV6aObrsv710esLa9/9nGfIac86fYOsT4zrfZpEOvd4VGTcNo3xemhhSdZdrvdp2cj3rU/oekWcZ+Znda58y8iVj2MdaJo7ncG4tKS3zZo1xeO5l+l9NjOjczPvvU9n2M4c1tumKjKh167VuZqTUzpTuTEyLetlI4e2UtXjPe0V77d9+/R6Hzigrx1JWZ/7p6f1eF03rY/lHfftKKztmzsop929e4+sd3QctZucLM6B9cpJ8XbfYJyjuh19/ixX9D4vGxm3ea6Pl8MHi/frvbvul9POG/m85ca4rEclnVs/rqOLXbLCTPr+c0d6vMbGbZSZOX2e6YlBVRmZktPeftc2WT+wXx+L9+7cLus33aTz02PxEvXss3fKaWsNnan82CuvkvW1q/Q5cGK6+BwaOx1gnhov07ffq7fbnd+8U9bnWzqne8+h4iztM/fqa269vk7W00wP2JqxbSLjtdDiUvGY6XX0eseZXrfpcZ2F3Wjo1xu9XvG6ffNWnUl/6913yPoT1jxR1uOyvuY+47sulfX77is+x37pWp0hft0Nuv7EK/X5+7xzNsv6ZF0fy6dtKj5W5w8dDH6fmjvdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAACc7JzuuGblR+pZtdrF+Y9xVJfTHjywIOt79+jsSSMm1qUd/d7D4cXifN6aji9342OrZH3/AR123WnqTNFdiwdk/XP/9aXC2pe+onO463qXu2c+Q2dpP/OJV8h6SWTk1sd1xuzOPXq9z7hP5/fmic49bop97u26vzin9sN//59y2pe95HmyXjIy7xcX9HgfGyvOBN1/QG+3u++9V9YnRnQ25VmbTpf1XXsOyfrB/cVjcs8evc9GGvo8cu7ZZ8n61FRxznZfpSnLebn4gNm5S2dT3nDTzbJ+8616+oXiKNW+6bXFtdNO0+NpYlJnem7ZonO+V03qMVPNdRbr7vv2Fda+8uXr5LTbtunczfUbdQb5pZdeJOuVcZ3jffNt9xTWdt6n9+kddxi59EZO95lbdN70GZvOKaxNrj9NTjs+oXOJ2y09IPfv1+eB/Xv2yvrttxdnD2/dpvOgOzra141N6/E+auSfd439UhEvN7KuPhY6TZ2PPj2lQ8KnJ/Q5ripe0GzdprOub/6mzhZeWNCv43qZXveqkX8uXmK67Tt0JvPeA7r+jW/8h6w/5clbZP3SSx5VWJsc08dpU2RNe3fcpXO6b/7mHlnv9mTZzYshNzN3g5x27Zric8y36CzrER257JaW9Ou0ubni19drVxkvcFv6tXe1qheu3db7bZfIur75Vp3DvW9Wn8TWrtOvw0an9XmgVtfX7HaveL/deYfuKyLjVu+IcW1pGK9Bu5k+lueXivPXc6ebRX2GWh7udAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAcLJzug8t6OyzqUmdR71mzabC2sKCzp6cOaSz+OZ1/KObGNV5elmk63URrBn19JOvmtQZti7Vu2B8XGfW3XSjzqm9VWQ0rta7zO3UEbcuKdUGygJcXCzOMexFOgB9ysghfNEPXSjr7/79P9LL1tLZxWWx6l0d7+g+/0W9z6In6XW/9DKdHfy1675eWPvGjToP+va7dD5uYmTeX/2EK2X9wvPPk/Ub79xWWLvphuKad0hH/7o3v0lvt/E1G2X9/vuLczW9v/6HTxTW0lQPihEjq3Vivc6DXrhfZzrvLY4rdbtn7pPTxsZbsxu23i3r3/XsZ8n6uNOZo/fuKc5dzis6Z/u+A/okNtPUWapzHX28HFzU18WlZvG1q1fSufITa3Uu/Nyu4rxRb9t9+hy2/b7rC2unb9GZ9pWSvi5NGlmqX//SV2V9650681m55Pzi1xrexKp1sv6JTxWfP73RKb1fUh1R7iri9N4o6dciuZHj3ZzTr5VKRg7tTdffUljbvm+fnPZLX9kt69/1XZfK+uWP0deOXbt0rv0t37y1sHbrrXrZSvqSa17TZ2b1mKjUis/vq9fr8fqWN71N1o1Lsrv8Mfq10KWXPVrPv1YckP7Jf79GTjt3WI/XyUl9Xdt1vz5/x7F+/XzxxRcX1rbeea2cdvGgzjcvG31D2TiWb7jx9sLavn36mji1Qa/3+o1nyPremQOyvm+ffj1x8SXFY2b92rVy2jM3nSbrO7dvlfVKVeerd9t6zM0tFF/za4nOjU8ivV+WgzvdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIMuODNs/o+NHanX90//lpPipMiP0YG5OZ3DM65QMt9qIDOv09M/EdzrFkWYNEafg9do6a2J8rc7tKsV6+lazI+uzYrede7aOrOmkOgbja9fdKOtnn6kjmDaeXhwd0E71z/4fntPLNjY9KutRuSLrmXForFlbPN4P798vp922Q0evnHmmju0681wdu1UfLY7zmVq7QU7bMSLDujohyd10m476GZvUMXK3bd1ZWNuv053cGp0E5NqZ3qe33r1d1m+8UY/3e3cX79crH/sYOe25F+hYl16qoypu/WZx/Ih3nVj2xVl9Dilbb80m+jyS1PSxODalz4FnX/CowtqadTqy8TZjnx40YhFnl/S2OfMcvd9cUnweGxnR1459+/XCff6/vizru3bp81ASF+ckTU7rqMtetyfrt9+qY+Suv/YOWV81rcfUM57xjMLaaafrOMnaqH6tsmHDFln/xKf+Q9b3ZbP6+cWlpyT2iVdO9HWrFOvXUnt2Fp9fve3b7y2s/fc39Hh61OX6en/RJZfJemJELK1dp+d/RaP4PLNj1/+T07a7+vw6rhMd3fkXFEdTebFYt+u/cdNAkbjPesYVsn7l5ZfLemTkgKoz4NOf/XQ57TWf0fGo3/imjodaM5GsuK/wtt9TfP5fNbVaTrthSu/09qKOOs70S1h3+FDxeWJRv7x13b36/PuFL3xB1usTOmqzUtbXpkqltuL46LkF3bDt3r17oAjokbIeExs2FPcdczP6tXkp0r3icnCnGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAk53T3e6lA2X9NdvFAb+Tq3Su5pq1OjN0tDgyrq9kZFv2Mr1unU5xUmFrUWeIj9QbA+UMulTn8a1bpbMGR8RuObBfhwGWdWSoO2Rk3M4v6oDJWr2+4ux2F+n6SGNsoHqzuUvWW/XiMTE2pnOJ9+zS26VjZODGJb1j1mwozjPNS/pYuPbGb8p665DOLY6N3PrTtpwt6yOT3yisnTXSltNeftmlsp6Xdb7jLbfeLOtfNnLpp6eL83/PPv8iOe1qIz+9amQ6R3U95u7edX9h7dC8HuvzRmboaZs3y/ro2ISsd7r6/FupFZ9D12/U867W9XZLc33+XrV6rayvNq5N3bT4eFm7Vs+7MT4l61+77hZZPzync5WrleKc2XJVX7fqJR1Cu2evvjjMHpZl96iLN8n6+edcUFhbaukBO3toTtbXG8fixg26vnWrzulWV65eV2/XPDNyYnN9H2XXLp2Bu2t38ZhZMM4Dz37Oc2X9tNOK83G9W27T155xIyz73POKx0RZ5Ap7caJXLjUyl1etXSfrmSveb3fefY+cNjJujZ2x5UxZb4zq7bZ9R3E2u1cbLX6ddt5FOnt9+1Z9DnJfuF6W41ivfGaEYc/PF5/fJ0f1taHZ0q83MiPbfePGM2T9nHMOFdZmFm+T0962XWeE/8snvyzrm87UY+L88y6R9VtuKb72bLvrLjnt4QOy7BpGP3fR+WfJeqtV3Gt6Bw4UL8BIVb9GjDO93ZeDO90AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAJzsnO7p6WlZbzR0rudBkdlcr+tstPMuOkfWN27Sy7br/hlZjxP9/NNTxZl2B4w80npF50kvzRVn9XmtRR1oesE5OrPusVcWZ2N+9tr75LSrdJSfe9rV58n6hefrerVSPPxmjVDQckkP3U5bT1+Jdd5p2Xg7qrkwv+KJy8ZRNzVZnPfsJUZwZ6NaHHS4bp3OE00SvV26RkzhxJRe9nOMMXHOuecX1hrGeeKKR+uc7q1bdX7kDTfqnO7798qye8YzryysbTpji5x2+86dA2Vdj09MrXi/xLv0eWDhkM4j3WTk71bK+hx4+JA+B2Z58fSTY6vltGlXZ606I3934wZ9vNRqOvd+306Ri5zrfPIs18di2rFyZGXZqctevaKPtV27dLbvHXdtk3UjNtltOfNsWc+j4m1z/159oEbG9X5iWuenb9xwuqx3stt1vVdcS/Wh5mLjumelyO49qF8LLYmI25HiuOa+irFTDx6a15nKc8WZyl5Sqq04C3vmkH49cEhHq7uaMV5dpM9xbZHpvGTkQRuzdgcO6vPnSENfO8oVfQ5bbBXvt1336+N8bFL3BWtX60G154Deby1j262aLs7Kvn+XzkevJfr8XK+O6mVr6mXbtGlTYW3DFv066bztW2U9reozwap1+hy3anqjrE9OFI+ptKWP41337tZjxjjWNqzV1+SFOd2T9TpiTFXLctpIXHeWizvdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAACc7JzuVRM6ky5LO7JeaRTnnx04tEdOOzWtcwYvuFBnet6z42uyvmhkQl+0tjgHPE6NbMoDu2R9y0adlzfbnJP1+hqdg3j1U64qrO0/+HE57Rojy+9/vOBZevrVer/t2bd3xRmz9aoOEd925zdl/eAenU08WtXhmCVXnLs5s7enc171ZnXrJvV2a87r7PZ2Vvz8UUnnEPbaOlvSiAZ2caSzLUfq+vnPOrM4u3LcOAdNG+eJz31W53LuuX9B1qf07N1Io/hc0BjR54ko0oHRWa4PiFJizD8vnn+W6XDginGVKMd62ZPMuDYYYyYRwfaNspGbqUKR+9ctN9Cy1xK97nlnqbAW9XRGbbWs83OrxnYf1XHUbvV48fNPjOtjbbaml33fYR183DV2WzfX9wMOiPk3xvWBOr16vaz3smigLGw94vw5uHjdRib0dW1kYlI/d1NnYS8Y2cEzYrcZlw536y23yfoZZ54p6xdeeLGsG6cp12kXb/nYuL1U0y+j3MjoyEAZ5eWkvKLl9g7pGG53++13yvrUxCpZP994/TzfK16A+SX92rnTE8Hv/lgV10yvFun5r16jM5vPOGNLYe2ued13dBf1a+/JEX2C3bbtXlmvjxT3Feddcq6ctrpKj8dkzDh/HpqR9ft26WW/+NziY3XTWj3e7t36N7I+e0CW3da79Xgfq+v9sl70JWmnM9DrtOXgTjcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAACc7pzsycrjnZxdlvSwy6fYc2CmnPeNsnRn6xCdfKev7ZnTQ4Re+cLesHzxQnOnc1TGCbte9et6PfdRZsh6nY7J+wMgB33xGcSj0D/3A8+W0UayHx3hD58jO7NVZ2N1mcYbt1LjOXyzVdL7jtdu3yfrenbMDvRtVEVGu64w858ddcYmsb1wzOdCxWE+Kl75rpMyOGFms40aeaZLr4OPFBZ19uWqqOKe2XNHZ6S0jo3b2sA6ANCJF3ZiOJnY333R9Ye2cc4vzQr31q3W2ZcnInlxY1CeixYXi8d4yzmGrp3S9WtK5xjXjYIpqetBlafH8W/M6bzQ19umIkRfd0EPOpU193WskxeHC6yb1dS110YozwL3ICIyuxuJY7elzzBln6czlyqhet5279H679oYbZH1JXHjPPVdn3MZG/vmMkWG7c7e+ruXGmIpU8H1JX3PTxJi5sW6VUf16Yr55cMXnv+uuKz7/eVmkl73e0BeX2Vl9zZ5fXCis6auSc8ZwdYuL+jhvtnT++cSa4hcFjYbOXO4a0cB33alfA46N3CLrzZ5ety0XFufa506v91136+z27Tv0a/PpcX3xaLf189+7vXjbjI3qC1tW1teltWs2yPqOHYdl/Z577ims7T6sx/q2fXqfX3LVhbKe5npQbVi7RtZHRb76wX36/Jga16X1xS1L38SYPoetN16wrF1VfCzuvFfnk9tnEht3ugEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAAONk53c7IBo4ynV/WS4szFOsNvRhZrrP4Vq0uzvb1nvXsq2W93dFBtdvuLM6dmzdyYO+8/UZZ3/uY82R9ygjHXJgz8tHj4qDZiy7QWautrg7UW5jbL+uHD+yV9bVriwP5slTv8/u363nv2Vmcgdin46rdiI4Bd9MiSvvyS3RG4tOf9HhZL6sQcOdcp6cXfnKyOPP5wJzOyTYil92ojot2lbg4l9g7uPd+PYOs+DwzO7OgJ23rY2V6Uuc7jjT2yPrB4gjbvl6veN2u++oX5bSbNm0eKJfeyuluLxXvdxF53FfXsfGuu6gzRV1Hz6BR0tnCC83iPOqZAzrntWes27QeEm6iobNa05Yek2VXfA6txjordW5R53A352YHyulO1LFmZNqnJT0ep9au09PfqbOwv/FNnfUaiRPVkpExXt+hM25HxvV43b1vn56+OAa2b75dvHw336FzjecW9XY770x9Hjn/URfL+u13by+sNY3XOlu36vG42PqCrN98i86THjPyeUfGisO2u8b1fkEfxs64rLm7775b1qcnisfU2WefLafdvUsfC7t3zsv6jTfqMXX7Vl0/4+7i3OPV63VW9Z69etn1qzznFpf0OfKLX/yyrO/dVXwwPvXxl8hpzzq9OJ/cSxJ9bZiaKn4d5t19T/G2uX3HTjnt3gV9MG48V+dsn3/BBbKeLer7sdu3Fb++vuZz/ymnXTKOtcc+QV87RhsNWW8u6p7ozoPFr/NqVb1Ph4E73QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAwMmODKtUdKxLOdH9e7NX/NP/G9YVR0d5c3OHZX3XHh1vsmXL+bL+zKc8Tta/FBXH/Wy/W0dX3Xabjhf5wheukfWnPuWJsj4+ofNJ0rx4u3c6OlOmZPx8/kikIzy6RsbImuniuImvf+MmOe2XvnKdrO/YqqNVpoyooLPO0Nv1kouKIxfO2LhRTrt5y2myfvedd8h6u6eDNtasL46LWJrXkWFL+lBzHZ1i5Nau0vF9paiz4oi8vKejIFYZ+U9PvOpKWe829bb5xjf2rjhV8eZv3CCnvWfrVlkvG+ffscniWBcvEgsX67RHlxpRQbu3F8cMeRvGdZTburX6eKglxeewXluPiZIR9WMkgpl5amMVfd2bd8XLnrb0wdSe1eewWB9KblVxglLfpIjqzDs616Uj4sa8q666TNazjo45uvN2Heu1e09x/e579LRGup57wpN0lFCnrbfNxLg+Vu/dUbztrrtORyDN7NPHylln6Kigiy+9SNbvuK04tmvHDh3/1DNeTywc1GNm//36tdITnqTP7xedWxyBOntIz/vWW3WUZWZEju28505ZP2/z6YW1szfrfdq64lGy/sXFL8m6kRLqcn0KdTd+vTiW8fIr9T6/5Pwtsr5mTEc+3n2HPpb33q/H1Gmri1/QNIzoqZkZff5tLenXYWvX68ixKx57eWHtjHn9Quzrt+rXE5//z2/I+hc+p+uTRmTuWG2ksNaa0yfYx1ymt/sTrireLl6tUhyD7OVGpG5jsvg1akfEk/ZFOsJuObjTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAADAyc7pnuvo7LNSorMpnSsORJ25b7+eNNKLeea6dbJeMTJFz1w7Keu9y4pzvtcZgXbXfPZeWb/upltlfamrswC3bNks6+MTxetWqhrZ62UdYlsq6f3S6eptc3C2OPNu6z175LRfu1ZnKI4Uxwj2nXGaXvfNW3TW9lWPK84SHBvVOYTbd2yT9fFpPR67XX0spp3i4OX5GZ3/mBoZtmNGfmNszKBR1dNnIhC1tTQrp12c1/mNiRFIPTqi99sZm4xM5vni8bxrp5zU1Rs6t/gxj9HH+TlnFmfUehNi3dLmTXLanffIsrvlBp2B+8wnPF7WK3Ek6/WJ4nzevYkO4k6MDPKykeNdN66QizMHZX1KnIjSpj5WmrN6vE8ZOdz379D11ePFKzdtzHuPkSE+XtH5vZdepMdz0tPhwnfdVVwv6dOAe/wVm2T9wrP1sj3GyLq+6ZabZX3E3VZYi4xz1LoJfU2eO6jzpuuRfi303c9/ZmHt+mu/JqfddvdWWT90SGfgnnO+3u5PfrI+j0xPTxfWyl19LNW6erxVKvr1whlnFOdwe1P14mvDRFUfKxedrfOeq+4yWd+6Ve+XAwd17vyZZxXnGl/1uMfIadev09vlm7cUHwteqa1fB5bP1gf7ZY+6sLA2Nqr3aa+tM5nrY/rFTMsYU1Orip9/dLJ4LHsTE1fK+p5998n6zCF93XIdPSbXrV5VWNv4RJ0rPzlefD3/Fn0O7CzpZc8yvd967XzFPc8wcKcbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAojzPjaTSb/n8n7xU1tPUmE23OIs16+lcNWsRS2aetJHHl+vnX1gszlNdaOrsya9de52s796jczXv36OXzRmZpGvWiCxWkbXnRZHOz201debnoT1GlutYcV7fgQM6t3h0VL9f9LSnPU3WT9+4dqAs7OlVU4W1WkWPx1kjf7dW03nRzYVFWR8dLd6u27dvl9N+5COflvWqkbP9gu99uqxvOUdnsWbd4ozGw3M693J6Uuebp8Z5ZN/9Om+61dXZlTffUJzPGyX6QN24boOsn33eubI+1tDB9Dt3F+d2bt+qx8TczAFZf9T5OiP8qit0juzevXtlvVQqPoctLOiM2c9//vOyXjUG9Omn65zZSy+9VNbTNF1xnuji4uJA223OOF6mporPYRs2nSGnLY/pIO9erAPOu622rO87qK8dM/uL65nT1631a3Tu8ep1+trQabZk/cAhvewLs4cLa6WqvnasW6Ov2aMTRsB6qsdcqynGXE+f/+JIn1/LsV63qKT3W2K82OnlxcuX9/SydTN9ve+19bqXqnq818q1wlpq5BJbz63WeznbvVzW273XLX79W28Ur5d1/lvO+dt6DTo6Orri3mF+fn6ga4O1btb5PY6LX8MmxusFi/X6Ne3o82/FePpGrbri7RYZ+9RadqtuzV9tW2velme+9b/Mx3CnGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAk53T/cX36Jxul+v+PcqKs9PiSGccWovY6Vm5bjrH0NoASaU4d65c0RngRlSf27NfZwPv2Fmcr+vtNaZvNpsrzifPjOz1PNd5eJvWnibrVZGvbmX9bTRytrds2SLrWdoZKOO2LIIME2NEdTqdgXK6l4yc7vHx4rzqpSWdMXvtV78m6/WGHu+XXnahrKdGpmgm8lSN4eZiI5/XOk/ExnuQtXpd1vfvLc6zLlf1dhsbGZf1ar224u3mLS4tFdY6bZ3ZGUc6j3TcyKgdMcbMtm3b9PzHi7fNpJHNPjs7K+uNhj7WrNzOWk3vl57INm61Wiue1qsb43FkRGe3q+efOXxITpsY2y2p6GzgSkmf312sg2JzkTedZvpYiKzXKokez6ONkcHOcWnxmErzdMXrvZznzjNj2US9ZJxfq0bGeDnR54FWp/i1irc4X3wO83oia7tR0/usPmIcx510oO2upu8ar0Ws8VoyQpWtdbf22+LSworznq3XOlYetXUOK4vXkFYW96FD+hy3Zs2agXK4rfO36musrOlSqTTQdjFzwI0xqZbd6tdSI9/cqg+Sw23V28ZrIcuzfutz5mO40w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA67O0ordmlYHmnFSPrumxk0lk5291Ob6As1lRkV6ZGnl2rpXPf1q7WObObN2+S9XZXP7/KIlwS2b1epWrkwNb1Po/TeMX7tdHQzx1Feq8fOrRX1ttLOuu6MaKfv1ErHrPNRb1dyyUjg9GIUIwjnQ8ZueLxOj01Jqe9zMjZHjXyopNEL9vsXHFuplctFc+/UtPZvrOHDst6y8gUtfJM2yJf19uwerqw1st09mS3rcfj7ILOmy4b2cJ1se1Gy/o47qV6u1WrxoA1xuvk1PiKs7C7Pb1stbq+tjSMfN6DB/V+abbSFWeCRrE+h2VG9q+17r1UZ7Wq5y+VjPfjM/3cqXHd60Q6k7lU0cd6pVS8bqVYL3vatV4P6PN31+nzQJQYOeDiXNDt6OduLTUHyouuGvnpKrPZyiVeWtTHijMyxiPjwjfa0Muex8XnwMx4nbRgZICXrdz42MgwLxePiVpdj3XjNOG6xrWltaTzqBfmeit+bR8Z54lKSV9Tc+Pa0DHOIwsLc7JeiovHzLp1Ooe73dRjJjZex1nX5MwV77esp3d6t6u3S9YzrkvGsreb+ng4WTnay5m/dZ5SOeCxce0YBu50AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAABwsnO6Szr6zOVG9mV7caGwlvb0YlQqtYFyMatlnesWWxmMucjMy3Vm50hNzzs18nnnOjqXMzYy7SZHi7fd1OTIQO/JxLmR9bekx0TabRVP29TTZplRNzJsJ8ZHBsr3XZgvzr4sGeMpSfR2S3s6IzHt6jGxMFc8JrPGqJy2XtX7PIl1/mPU09mW5VyfSMZqxeeCUkUvWyvW826M6ez1sRE9JhaMbPfm3MEVH6clI3uyVNbT1yr6HJqIc2CnVXwcek0jIzztGFmtRjZwo1FfcW7nwkLxdWU5uZuZkXEbG/m7JSNbWD1/uazXu17X171uV197mkbWqtqukxP6PNExsqytrFSXdQfKdM7F9HGix1vJ2KcV4/y8OHtA1qtG7nJNZJCPVfVx3Ej0vDv6suhykVHrRWK7q6xpLylb2914naU3u52vnhbXIyMPulzSucjVst4vPeM8kqnrYk+veGRst4pxbYiN/ZLnet1q1eLzVLPZHGifWVnXibVuxrEeu+Lpy0bG+GxTX+/LxrFaKeljNRIB7MYpyu81WTVaImec4tyI8VrIPL8PcE2OB8zK7vV6K152croBAAAAADiF0XQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAABwsiPDxmo6QqQl4p+88bHin6BP83TF8SBe5ozYAeNn4FV0Sv/5xa/zp8ZP969aNS3ri0s6emVhyYhk6Olt08uL1y1O9Xbp9bKBImumJ1bLeiajMvRzdzIdTVVJ9PRVnebgImeMuU7xeC+N6KgfY8i4yIjVqtWMGCQRC2PFkTUqOiotivTC58bK1YyIkEhE8HWW9Dkm6+l6rdKQdSOdxCXGeUrF1FWNeKhqtbLy2EK/X1N9nsiz4u2eRHo8TU7o7bawODfQOa5S1sdLIsaMNR7rDR0pUzZ2+oQRq7i0qMdcq1287qVE7/N6Q5+kSkYkzVJTx6l1VZxPpLdL1tHnXyt5JY71Sw9rvzonlt24bkVmpJgVz2fEGBnnbxX5qMb6t5ZNll1sPCAzpk/Fa60stc5BOqazY53DrBgiY3pVjY1IRmfVjTyzzHodJl4rWevdMw6mkhEJVi7pYy2OdP3woZkVbxcrgilKjUhdIxq2Y5yH1HWz3dbLNiZ6Fi+yxowhy4qXLTNea1jj2YqlLRtjxpKK15hWZFfPqA8aKVYyxruqq/UaFu50AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAABwsnO6yyLv2UuNrL+RkeJM0baR8d1u6/zHXtpbcUZtvx4bdZFFmBkZi9u3bpX1ck1nrVaNQOmSkcvZUzmHRiTdiLVs4zrH8MCBA7Iei3x1K2fQRUamp5HvuDCnx1ylosfz1FRjxdmRVhZgYuTOj9Z15nOnU7zuaarzRBMjv7HX0+uWxMZ+ifS6LzXnV5xVbc27I/JxvfkFIy/V2Ha1ujgWI73dukZ2pcUaU+o8ZeVkW+egtSOrZb0psqq/tWxuxWPOiFR2zeairM/PF4+35eR4W3nTuchb7Ypcdy9qGecJI2/aOb1hVRZsu63Ha6Oqn9vaL0msp4+MnHCnzpF5PNC8rfzdkRF93bPO/+128XkobelzTGy8VrFeDwySgWtdk1tdvd6ZyoXvP4FetkZVn6fisjgWU30sdIzXkNY1uWacI+NKcd14eWoueyoTyv1LJSvfXJ9ncpFBPjo6LqetG69VrMzmZlNfs/NkkPGszwO1mh5v1mt/M686F9ObEeD5QLnxHSvz3qin+cqnza3XcZkej6WSkUFujAm139V6DQt3ugEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACCTKzdA0AAAAAACwEtzpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAeJrZv3+6iKHLXXHONe7jzy3iqLCsAACcTTTcA4CG1bds295rXvMadd955rtFo9P9cdNFF7tWvfrW76aab+o952tOe1m/orD9vfvObl/28fp6XXHKJbHbf9a53HddUfvSjH33g3z74wQ/2/+3rX/964fOcaF4hHLuNpqen3WMf+1j3l3/5ly7LMvdw94pXvMKNjo4W1v06+XHy7e6j5cwLAICHUukhfTYAwCPav/zLv7gXv/jFrlQquZe85CXusssuc3Ecu9tvv9197GMfc3/6p3/ab8rf+MY3up/8yZ98YLqvfe1r7o/+6I/cr/3ar7kLL7zwgX+/9NJL3SPZ6aef7n7v936v///379/v/vqv/9r9xE/8hLvzzjvd2972tpO9eAAAgKYbAPBQ2bp1q/vhH/5ht3nzZveZz3zGbdiw4UH1t7/97e69731vvwl/9rOf/aBarVbrN93+3/0dXnzLxMSEe+lLX/rAf//Mz/yMO//889173vMe99a3vtWVy+XjpvF3wTudTn+bAgCA8Ph4OQDgIfGOd7zDLS4uug984APHNdyev/v92te+1m3atGnZ85ydne3fJfd/w/U/qv/4xz++v539ne+jP1r9t3/7t+7iiy921WrV/du//Vu/dt9997kf//Efd+vWrev/u6/7j6cfa9euXe57v/d73cjIiFu7dq17/etf79rt9nGPW1pa6u+PAwcOPARrCwDAqYE73QCAh+yj5eecc4573OMeN7R5fvzjH3evfOUr+428/46wJU3TEzaEhw4dct8p7rnnHpckiZucnHzg3z772c+6f/iHf+g336tXr3Zbtmxxe/fu7TfoR5ryNWvWuE996lP9j6fPzc25X/iFX+hP22w23TOf+Uy3Y8eO/psiGzdudB/60If68zzWtdde657+9Ke7N73pTcv+vj0NOgDgOx1NNwAgON/E7d69u3+39FiHDx92vV7vgf/2d1Pr9XqQ5fB3YX1z+Z3i6DcR/N/+O/HXX3+9e+ELX9i/633EHXfc4W6++eb+D9Yd4b8z76f3/75q1ar+v73qVa9yP/IjP9JvmP1H1f1++PM///P+d8R90/6DP/iD/cf91E/9VP/7+IPyd+S/k/YHAAAnQtMNAHhImm7vRL9W7b+jfeONNz7w3+985zvdL/3SLy1rvv7u9nLucB/h7/C+733vO+7f/V3fo78bfao49k0Ef9f6BS94wXEfEb/66qsf1HDnee7+8R//0f3QD/1Q//8ffbf5Oc95jvvwhz/cb96f9KQnuU9+8pP9rwP8wA/8wAOP8Q39T//0T7s3vOENx+1LP7/l8t8r/+d//ucT1o79Xj8AAKcqmm4AQHBjY2P9vxcWFo6r/dmf/Zmbn59/SBpffxf9Wc961nH/7uOoTkVH3kTwzbZvYM8999z+d66PdeaZZz7ov/33vf0nDPxdbP/nRPbt29f/+9577+1/LcA/x9H8D7YNyn8M/kT7AwCA7yQ03QCAh+RXtv3d0ltuueW42pHveJ+qje/JVPQmwrGO/bj+kRxv/ybHy1/+8hNO80iPYwMAYFhougEADwn/sef3v//9/R/buuqqq0724jyi+Y+k+08f+O90W027j3jzb5b4j40ffbfbf08cAADYiAwDADwk/Pd//XeBfUSV/yj5sb6d7wIfQWTYyj/W/aIXvaj/ve4TffrgSNyY9/znP7//I3gf/ehHHxQNdqKPpRMZBgDA8bjTDQB4SPjvG//d3/1d/9ex/feBX/KSl/R/Ads329u2bevX4jh2p59+erDIsGHxP1R2JOv6aK973ese+P+f+cxnXKvVOu4x/hfcL7nkEneyve1tb3Of+9zn+h/v979G7n9obWZmpv8Dav/5n//Z//+er73nPe9xL3vZy9x1113X/5qAjww7+tfRB4kMGwb/5oFv9o/lPzp/JPf961//uvvt3/7t4x7jf/ztyU9+8kOynACARyaabgDAQ+Z7vud7+hFV7373u92nP/3pfvPqP7LsP8LsP37uI6uGEUUVmo/mOpGjG3/flJ+oMfc/fvZwaLrXrVvXb5J/67d+y33sYx9z733ve/vRYRdffLF7+9vf/sDjfHPt30D4+Z//effHf/zH/f/2b5g873nPc8997nPdw4H/tfUT8Q31kab7q1/9av/Psd761rfSdAMAgorylXyeDwAADJ3/MTn/S+P+DrRvGAEAwKmP73QDAAAAABAITTcAAAAAAIHQdAMAAAAAEAjf6QYAAAAAIBDudAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQSCnUjIFHqh97xU/JetpJB3+SKNPlyJjceID1Uw9RrqePEvv9vE6vJ+ulkj49xbGuR7nezpGxDZcjG3AWUWT9pIbejqnehK5UqpjLkEV6JpGxK8tVvR96Xb2RKpX6wGMlqUSDLWOulzFL9fyTvCrr35qJLpfisqz3Ok1ZTzNdT4xjMikZJw2/Hct6GWNjvMWxXoZelg40vbmR/TyMc1+325X1Vqul528c0nmU6Af0t4Nej1JZj7fEqFfrNVnvdDr6+Uv2+b3VXJT1Sklvh3JiHLM9vYzWaE7TZVyH83ig8Zimxng0xkJsrEVStl/Cd9t6O6WZHu/v++M/MJ8DwPJwpxsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEDI6QaGrN3WuZfDYEfqDpbDbdXNEFQj99irVCoDZYm7VC9jZixDnAyel54bOa6p08tYMXJWzZxtHcHqqtWGnePa0/nOjVGd6ZtGejsmOtrZxbmdW1w2xkJcMcar8RRWHm7J2E+x9QR+rHSNbHtjFRpjo7Ke9fRYXGrO6+dfRn50boVQR/qY6xq5xVbucWRkjZux98tgLUOppMdClOUD5dZ/6zGdgfLSaxW9jB0ju9k6vWfLyLgux3o8JUky4L4uDXaNM64f33oSo26MlSQqDbQNrLFoZcovZ7w2qjrTHcDwcKcbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAyOkGhqxkvJVlpbSa+dT+MVaerTUPK9DWyum21sJeBRcnRm6x9Z6gGSWuM1DzOBp4PyRWFq3xHImRRZsamb+lss7QXmi1ZP1b89DL0OrpfZ2U9Tbo5T09vTEOvNwYr2luZWDrdYiM+edO5+HaycvOZcZz1Co6L7fV1nnqJWM/jNQm9PTL2A9WLrC1L628817PGO9JdaD9+K3H6GWsGJnv1YrOqM57ut5ut52lXq/LeqVS0c/R1GNlYmJc1rNMj+h0GTndqXGCzoxrTLfTG2j+Vga2dd5bzlgpWVnjxjXIWsZeRx9veaa3kVc1Mt27zSVzHgCGgzvdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgpVAzBh65clmN4kjXdXk4cv0kubEOzlrGZaxDu9OV9XK5KutxnMh6bqxCZj3AKH9rGfT7lkmpbMxAP0kv7ch6varnH6c9/fzOuVqtJuuR3syuXE4G2kYusS9DUaafI4oyWS+VrfeX9fR5nhrPbw/42NhO9VpD1kvGvk5TvYyJuYx6G/SXIRnw5GQcc9Y6xNY6JsZg7Y9HvQ6JcV6x9nVmHNNlvQr/v5noc2Mp0s/RSRdlPW/pfd3r6XpnGecVa1+Ua3VZz3v6mM0HvAilyxjvkTHecvMiY+xs67xljMXMmn9/O+nt2KhVzHkAGA7udAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAg53cCQpamRPx0b2c1W9mc/UTca7P00I6s2NrJoIyO8OVtO2LixDHlk5BJb01ub0ci6NTNY+/PQ2yEx39ccNHdY57yOjY0sYx105m7FWIZuuzNQLn3P2AbL2Q5WdrIRh2tm+kbGpTJaRqi7tYyHDx+W9WpV59ZnXb0dra1cNo43LzbGc9bTz1Ku6O3Ys3ZUrsdqZuQqe3Gkl6FrZLJnaVs/gZGdHCX6ePEW5w7JetvYTtNTY7KedWdlfXJyStY7mZ2H3mzp62Av1dshz/R+yjPj/G+cG7PMzulOYj3eS9b5uWRkwic6Izs3lrFUKg183pmf12MBwPBwpxsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEDI6QaGzMrOzIys2cjZGahmTLc1uRErHMVWLrKewTLWwJXKxqOMnOzUylk1tlFsZLAuR2xsR+sprAxraxVzK1PYjqJ1LtLz6BjZx1aeeZQbO8KOVnalks6QTozt3OsYmcDmfjT2k3VAeUYucaOmM3szY/qakYFdNo7pTqvlLHGuN3S1rNchNTKsKyU9/zwycsIzez+kqT4ocmNfWvnOJeOkUDL2g3feOZv1PIxjds9998r6qlWrZH312glZv3fHfc5iXcd6Hb0O1XpN1lMjp9sndcvpl5HpbtPP0TNy67tdnWXe7uhjsl7V28hLjWVMylVzHgCGgzvdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQk43MGRJYuThGvmkxuR9JSMAOjOyOa0Q69zIsnWRlZFqB0RHmc4rt+Js4ygZaAmifDkh1tYyRANl9hqxwz5cWZaTpDRQhrZXKpcHyrO1smKjbPC83Jo1GIxs5WpJ50dbm6lnZDPbx5s9oEu5zuztpU1ZLxsnjl5TZ/42jJxvr5To7Vg15jE7r9ch61kHvZXT7UzmYW8ck1Gs1zGOjCdYxnlnx45dsp5kel8+6arLZf3JT36irH/2mv+S9UMHDzpLGtf1A0ojstyZm5P1yDj3VSqVwV/8JvFA52/jtOQy4zprXeN61nW6f8wax0ysz/8Ahoc73QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABEJONzBksZElWzZyka26VzJzh3VAaGYE2tr5zjqfOsqX8X6ekUtcMdYxKlcGyhLvuc7A+dFlI4e7WtbL0Ono3OJGrSrr7VSvQ61qZOU655aWlmQ9Kel1aC/Oy3rW1XnstaodTN/uzg8UG1+p6O0YxzprPO3q7dxbRvZyraGP61FjX8cN45jr6ezm2BiLF5x/lrPct2uvrO/Zr/Obk0iPx46RO2xlDtvB9871rPBkYx4lIw89SvR5I++29fP7477RkPWRiq4vzs/KerWs1/G0DetlPYpulvX+c9R1DnfT2Ay50/spNo652Jg+N84ZyxlOsTUWjJztrKRfgidl6zpsizI9j1KVNgB4qHCnGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQAjoA4YsjvVhlXZ1vmhi5Hx7nfairEfOyLs18kGtjNS8p+uVipGh3c8N1o/pplZOq5HDnel86MTIqjXKfZVYL0PW0/upauSw5j2doV12Ovs57i0jPzrW26luZFyvX71W1rNUB/LuP7DLWeJEb6fx8VFZLxvrcGhWb4M01Znyo2OTztI1crTnZxdkvV7Wyzha1dvo2c96uqyvWTXtLPfc/U+ynnX0MiR1vR9KsR6vuXFes8693oiRhx6VdbZyL9eZ7ZFRd5k+Zpdz7jFWwZVKeh0WjBzvSl2fm9Ncj0UvMTKqY2MZy2W9LzPj1NYz8qkTI2N7Oc+RZt2BcrqtrPByWY+VxSV9TvEqxjyynl4HAMPDnW4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIJBSqBkDj1S9XirrURTJepp2zeeolPU8ojyX9XJZzz/tGcuQZLJc0ov3refI9DJWqjVZj8v6PcMo1SvZGNGnv6oxf2/+4EFZz7KWrE+Pjct6N9XbqJslst4zpveSWO+sxYU5Wd/T1es4PlqV9QvP2+wsz3/Bs2W9m3Zk/f1/+SFZX1zS27E+ukrW5xeXnCVJ9Haul/UyLC7My3rNOOZ279gm6xvWTOoZLOO4ziJ9zDQXm7KeGMdcHOttFDl97vV6vZ71CFnNIv0cZWM/R2X7Zdfi7Kx+QFcvw9ppfczVGrpeqej9MDox6iwz83q8VkZXy3pm3BPSVyDnGvWGrMexfX5vNvV4jZJ4oHqvq6+zmdNjKTZeS/TnkektlRt1AMPDnW4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAIhpxsYsqhkZM0a0cndrs4G9UqlZKCc1c6SzoEdaVRkvdfVuciNhl4+b3pKZx+3ezqDdNf9M7JeNXJap0Z1RnbN2Ibe+nGdd7tu7ZR+jobOEr/hxltlfW5OZ2T3cr0fvSTR+yqP9GVioamzZtttnWHdaun96HW7T5X1xFjNprEMna7OhM9jnYtcqtXtXPquzn9ud/QxVUmM/TCv89QrFb2RGmMjst5/jJXPPKvHY7mit3OatWU9ToxM4XwZmcOZ3s5pT5+g80g/R9eITo5zfbx4jVF97mrNH9T1jt6O+2cOyHqlrsdK3ThvefmilUGtt3NknHfKI7WBMrJbbb2NvLis17NU1ufONNXnjUZD7+duR58zkvLgOd2pPQsAQ8KdbgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAiGnGxiy2HgrK8p1PmmpYmdcT03p/OeLzjtL1lvNw7KednSu8epVY7Le7dpZtF/7+k2ynkc6h7Va1cuQpno779mzR9ZdT2cOe2um9DJe+ehzZX1sXE9//Q3fkPVOS2cOG1HnfZHT461U1gO6bGTFRpnOmnVOZ9l6t9x6g6xffsWlsr5+3WpZ7xrZzXlJr2Mv02OtP49EPyaO9X6olHQW+JiRGTxzWOd49zJ7P0ytmpb1dMd9sh4Z6+iMzPikZLxkye2XNMbp12VGDreLrINKP0GW2wdlZATPW8OtZ6zCzMyMrG86a4usl6z90H+MXsiFhQU9fVXPv14qD5RP3VvGNapW0+fnPNXPsTA3L+tlIwd8GCJrvOb2cQ9gOLjTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIOR0A0PW7rYGysVs1Ozszvt23S/rh2f2yfrUqM4fXb26IetPftIzZP26b1zvLM1uW9ajil6G1Ih/jhK9HcuRfs8xy5v6CXxWeF3n6e4/sFvW164/T9YbdX2KrpZ1Tmzi7LGUR0Y2cmSEAuc64zpO9DJWqnYufS/VY6Va01m0tbp+jjzTx2zsdKavFT/tJUZOd8XI7I2N/OeFJb0Oza4+79RHxp2lWtfHZG6EYFtZ4Hbeua4nRjbzcnLpIyMrPC4lA93KyDM7p7vV1uM9y3WIdckIubbOnZWynr5e15nxXmxc5yplfQ0am5iQ9a4RRh4b+6FcsV/+pj193JdL+knGx0ZkPTYWstXSx3Sa2hnbsXFyKhvXSQDDw51uAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACIacbGLKSEXvZbupc415mh/6OjutM3cTISF1a1Pmf3dFkoNzkyckxZxkZ1xmmC02dyVs2MoPbrd5Amb+jI3r5vIqR9RrF+jmqFf2+Z6WsM317uZWBrbNwva4RjVxK9DLGRrZy2lmS9SzXWedeuaT3dZTp/TBS0bnCSX5I1ns9fcwmy8gaj41c+HZXH5PG5G6kPirrc7NLA+Uee1u2nCXr//3lO2U9dUa2spGRnRsbIVvGfYSKlcNdNsa7cUw6I7u5ZwVI+/NCVR8TS8YxVanWB8p/rhvPPz2pM7S9Xffr5xgb0/NodnRGtnXElVw0WN56/9ylr3OpsYyxsQxdI2e7XtHn747Tz+/luXGCB/CQ4U43AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABBIKdSMgUeqTtqW9Uq9LOtpmprP0ct1PY8T/RxdPf3YxCpZ379/v6zHSeZs+jFJkgy0nRqNUT19rynrzfYhZ1lq9mT94MFI1jN3gayXqxVZ73TnZT2p2e+rtrt6MGWxXoeRal0vQ6Ln3+vo+Xt7dy3p53jsmKyvHt8o6xV3WNYbVT3/2W5H1vvPUdPjOU70dnQ9fV5xuZ7//IIe77HT0/el+pitlPV4KxnHdK+kX5LEsT4eSqm9DrVyTT+HsQ4u1tsgNepRzx7vldi4RlR1vdNpyXq3p6cvG68Mp6dG9AP6Y0HXE6cvQnVjBnGs91O3pY+XPNPn7v5junoZ80zv67hclfWecSHuGaeVNDVeCPjtkOr1jBJjRwEYGu50AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCDndwJCVyzr3MjdCtitlnUXrpT0jL9fI3uwa2cz3790j64dnN8j65OS4szSqOi83M05PmRFRGuc6A7Xb09nPtZKdp5taOa65XsjxyWlZX79hk6zHt+ks8czIkfXqIzpzNzWyZJeai7LesDKFjW3oRUY+c57pPNyRxtRA7z/3ejrrtlq286FbbZ0bXDJmUY70vsxynVtvLWKnY2eNr1+/VtbjSI/3pDLYds7M+wT2foiMZSzFut7N9HbKjPznzFhHr21lWI/oTPf9M3tlfc1afTwdnjso67WaPt6Wo9vR49VKoG4b29Eai41lrEPbOH9WKnoeYxOTsr53zz69AIm+BlWsQHV/TLX1doiMvHMAw8PRBgAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIOR0A0MWZfq9rCTSucVRZudD+yRXJct1lmypbGSFV3X+aLuj13F69WnOUi03ZH1hfn6g/Odu2pT10arOea2U7YzrKNXz6LR03m6c6Kzy+ojOO08SvR/iyM7A7nYWB8oaj2M9XrtGDnd9GVeh+aXDsr7U0cfD1JrVsh4bkb0tYyylzs78jYzM3aikc4vjSNd7bb0fJ0b08TZzwMgM9sf12vWyXirp8Zjl+pgqGeM5sTKF9Sbq6xhjJTJOv63mgqznsV7Hal1nbC8nr7yV6XVIYj19M9PjOTOuD2tP2yDr/Xm4e2Q9ikZkvVTR5/dS1djZxnknc/b5vVzVJ6fUyGRPjbz1UkPP//D8kqzHJb2NvPLIyEB56QCGhzvdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQk43MGRZqt/LMqKdXblWMZ/DiHd2pUQ/SWZkmM4t6nzQTldnnI6OTDjL+nWrZD3t6azx8TGddzs+qZehPqI3YqVsvye5akTnM4+Njcp6qaT3db02JutRqvdD2tbbsC/R5XLVGI/GMpRK+jKTZfYyLi4uDlSvGxnVI6NGdnJHr+PcMrJu40Rvh8TYEWlb50PXje1cqej9uHfvfmcZm1yj6+M6E3jfjN5PpZrOpW81ewPlhHtxpvdVvmTkeOf63Dk1MSXr9++zt/PomD53xbHOZ16a0zncbWMdk1yPxbGanQ/dXpiT9ak1+vzfMc4LcaSzxEtG1nia2uedTkdfB9etW6dnEOvQ906nJevlij6m48TeD5F1gl9GXjmA4eBONwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgZDTDQxZlOvDqlrTWbYdI5/a6xn5nrWKzgctGzmvs4d1PunhQ7petbKdnXOXXHyWrF9wwUZZrzcmZX3ezBrXebvzc4edZcfO3bJ+4MABWb/k0Y+R9U0bTpf1kjHWomWc4nuZfu81a+tc43pVTx/nevoost/7bdR1fnOvp59jfFxPXy3rzPb5JZ0vXS0ZOd/Oua6OmHalis58d4k+pl1mZP6W9DZoN+283tFRPY+pcZ0vff9efUxFFWMs5jp7Oct03UtKOre4HOl6q6nPKxOjY7I+OzPrLFWnz5+lXG+nak3nqTf36xzvbF6PpZKR4+2NlfT5tT27S9aThj4mu6k+oHpGHnu2jOvsSL0h62ecprfz1m07Zb27pLdzY3xa1qPEOGf4PPJM76tO194OAIaDO90AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCClUDMGHqki472sVqsl60kSmc9RqzZkvVzR0ye5rnfbbVmfm1uU9VKpqp/AObdv3x5Zv+nmG2S9l+uVbHcyWY/KZVlvLup19EZi/RwjIyOyPjNzWNarlbqsjzb0/OeXjB3tnEudXodqXW/nXleP50Zdb+eesZ+8tjEeFxYWZH3N6lWyPlLT4/X+PftlPa7b471a0Y/pNlM9vXFMpVlP1hcX9H667777nWV+To/Xit7VrlzS47Hb0vux3hiX9TzV26D/mEyPpdR1ZD1JurLeqCV6AZaxjL1WU9YrFX1Mxk7viMR46ddt6bE4NqavP96GdWOyvn9Wn1/rDX3uG59cLetrVul6qWRcJJ1zWzadIetTq9bK+u233SPrYyN6PC8s6mM2Ktn3zfJM78tezz7/AhgO7nQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAIOd3AsEU6i7Zq5LhmTudqer1MZ8W2l3S9bmSUTk2u0wuQ61NH5Ozc4tGx9bJ+YEavQ6U2Ies9YxnbRhZtpaxzYr0s0pm/M7NLsr577wFZP+/cS2S9WtXbecnIt+5L9HjspDq3eHREL0PPyEXOl/HW70JzTtY7bb2dYyNPffVqnZe7c3eknz/VebpeZsThxpGRvRzp/VRK9PRpps9LS8Y29KJYZ0yvXaNz4++5V4+linHM5pHOCW93dPazV6vqATc5VpP16Ul93pqaMvLYu/POYmVIt7p6MKUtfe5sGrn2e/br7bxg5Ih7Vz7+clmfXr9B1jPj5WnPuEzu2a1z55sL9rnxS1/6iqzPLejtkOtDzsWxzlOv1/Q1qJPaJ8+ukdOd5/rcBmB4uNMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAg5HQDQxbnOkO13dZZskYcb19tVGfJ1pzO/8zaehk7HZ2nu7RgrIORCeytWa1zWvNcr2Ono98zjMsNvQCJzifNomXkpRv1iSmdWzxz+JCsp07vh9VrpmR9/6F7nSUp6wFXr+nc4Y6RGV8ycsBL8XJyYvW+np3XOd71hl6HtWtXy3qno3O4yw1jrPVz4/UxV6nqYzYxtlPXOKaTnh5LG6ZWOTfgiI+M3Po81+eNsjFWNp62TtZrjY3OMjaqs4+nxkb1c5T1eenA/llZzzLrrOGz6/X5szaqzyttY/qxxqSsf/XG22W9m9o53UtGdv3ktD7mslSP94UFnSvfNbLK88y+0JZinZeelPRYynJ9TJfKut7Tq+B6beMB/txlBJrnVpg4gKHhTjcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIGQ0w0MWbmi8z8rJX3Ytbs639TrdnXmbpTo54iNt9vy1MjjzXUeb6s5r5/AOdcY0Rmo4+M6L3dhSW/nONHZzKVYT592dQ6st9jRebWTU2Oyvn3XXbL+6MsvlvVKTee0dlOdX+3Fsd5O3Z4eC+Wazi3OUiMvN7JzYueW9DExc0ivZ5brfT02NiHrsXXAOJ2R7Y2M6CzvnpF3buV8p5neRkuZHs95NO4sM7P7ZH3DRp29/KLve4Gsr113tqx3jFjiQ7M6I9ubNR5zcO8BWb9n6y2yPjerzwmr1m5yloWmPuYOL1o52Tr/Oc71udc4/btepufvZRV97tuxT4/HRlUfL3lHr0PNyCLvGfnV/edI9Xmj2dXnrm6qj9lOV19H45I+t3azZWRsR/rcFSc6Dx3A8HCnGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQMjpBoYsz3Tu5dKCzlit1OwM1Lik3y/LUp1BGhvxnrnTgbilsp7B7KE9+gl8kqyOQHVJpNehp6PKXVzWGalZuoyMU0PZyPruNHUWbbdtZY3rbTC9SmfhlhM7P9rlel9nuR7PbR016yJjsJWsgdDPy9X1iakpWZ85pLOZW229Dc4+W+dHb79vv7PMz+vs+lpD5xKnRubv+tPPkPVyrNfx8Lydcf3xT/y7rKddnRU+NqazwOuN22Q9N16yZMbx6B3Yf1DWu20dUl2v62POJaOyPDdvHDBeuSrLpXp9oHVY7FrZzPr5e6md7Zwbj8kTvS/nFvR4jZxx7jWuga2mcQHxW6Gqt7PxFC41zp2xsQ0Wm/p46nSN4Hov0a8V8nzw6yCA5eFONwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgZDTDQxZZuR0V6sjst7p2jmulZLOKI2M6M0o0jmu7aU5WT94aFHWD8zcpxfAZx+feY6sr5+alPXDM4dkvbOkc5GzpCLrSckIYfV5tca+WlrU23njBr0NWot6P0yP67G0alrnInuzLb2MSbkm6yUjazYz8tbHJ/U6eOVYL8Mtd+h85y9/9cuy3lxsyvq69afL+mmnb3GWvQf1eO1lejvGJX1eOXhQr0O3uaDrbZ0p70WZzhIvJbqeZTr/eaGlT1ztTnOg7Oe+xDgmjFnML+m89HpFZzsvtfV5yeu09L4YiZLBMrJzfcw7Y/rU2TndPeO8Ehn7KjUiqDsdnbNdqxr51JEei56xCs6KuI4jvQztrl6HcrUs66m1H/vLqMeri4w6gKHhTjcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEEgp1IyBR6peryfrJeOwi3L7vbAoTfQD8kyWe5mup8b0WZbL+uHDh2W9/xzdjqyPj47JehLP6CeI9TJWa3o/dLpdPX/n3Gh9VNY3bJiQ9YU5/Rz/+snPyPrBffOyPjG+2lmm1k3J+oHZBVnvdPQ6JLEez72eHmvewtKSrOepPuaiXB8vUWlE1g/OLsr6oaW9ztLVi+h6mfEAYzsmUSTrWUdPn6d1/fz959DzSK1zV6rrWcc4N5b0MnaMceA1m/q8k0TGWMmN7WzMv9XSda/r9HpU2lVZL5d1vVLR9V47lfV4Gdeo2BivWa7Pz+WaPrfGsd6OpZIeS3mu1/Fbj9HLGEe6bkmcHktJYmyjZbyCT1N9fo2M8waA4eFONwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgZDTDQxZkuj3sqxYzCQxMrh9Hq2R9Vpv1GS9Z2Rkl4083FVr18j67r1GhrZz7q47PyHrTSMr9ozNG2X9cFNn3W7dcb+sj4zo7GZv9YYNsj4zo7fDnVt3yXqno9chyfV+rtXLzpKVjUx2I2+3Wq3Ieq+jx1q71Zb1bz3HyEDHVJ7psZQbebvtrpHHa2ThenGpMlhOd6afIzc2Qm5M74zMYC92+tyUGRnWLtXTdzO9ndOezoRvG3XPiphuVHWGdafVkvVeqsfz2Iief/85MuMaYuR497rGvjYyqq1jdjnXqJIRIm3nR+tlTIyhFrt84FtOvV460HbIjGPOXARj+pJxbu4/h7Gd44h7b8BDhaMNAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAyOkGhiyK04GyO5PYPiyjkn5MZOTdrlun86VrOv7Z7Z/ZK+v33afrXru5JOvj45Oy3k0XdD3SGdUj4zr7udXUWbXeXdt2yHpsRMWWKnodJ0b0jkhinafe7uixuJwc7ooxGCKnnyOysp0zOx+61bTXQykZx4tVz7s6/7nTNTK2nXNlYzvFxn6wzht5ng84vSwfmYuuWrvJyD22lKulgdbRa3e6Ay1jych/jtLewJsgNbZzr2c8R66nr9UaA92NsTK2vU5H55l3jHnUqnoZU2s8G7n3kRX07a8hXesaoK8xWdodbLwa22g5490ZrwWyyM5cBzAc3OkGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQcrqBIYt1dKfrtXV2Z1JaRm5mbmT6GhGkBw4eMpZBZ3vWyjoferFlZOE65xp1nVHdM3K205JeyZ4RGtwx6lHF2JF+Oxk5rUmk91Oe6FNws2eMhVxv59yOonVJRS9DLzfydtuLA2WVV6s6L305+c+RsZ1douu9yMiyNTJ9Y+v5++PRyA021tHK4baWwZp+OTndZn60kVFtZQaXjHWwjuk8XkYIthGUvdRqy3rVGEtlaywsY6xYj8niwbLI41jvp2q1Kuu5dYHxY8UYT7VqTdZLZb2SkfEE1nknM475vtw6KI0dYeWZGznbpVif/3vG8eQlMS/zgYcL7nQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAIAX7AkKVGfmi5bORPp3bWbGRkxbZaS7Jea9QHyok9MLsg6xOjq/T8nXNdI8O029H1zOnt1HM6SzYqVfT07Y6zLLabsl6t6O2cGuHMmVGvJHodssjIifXbuaVzuMtGXm61pDN9s1RnBreW9Db0YiNrNq7o7dDt6n3Z6epljCIjp9vI8fZyY7zHxnhNjPfII2M/xUN4i92KZ46MdYiN4Pi0rcd7u2OMlYoei16tpvOhu22d023FO+fGIReX9Pnfa3WMY7Ku16FsvLJLjHNfHOkZ9HIjj71/3BuDpaQzqCMjw9rKwM6NY7bXszOujVh4lxqDIbfua+WDxXx3u/Zrha5xnbSOSQDDw51uAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACCQUqgZA49UeZboeqTf68qzzH6SOJfl+khD1tu9tqx3Oh1ZL1fqst7q2euQp7reqNZkPc26ut7T65CmPVlvt/X8vbhlrGeq51GqWe97RrKaGRsxKduneGu4ZXqouG6qx2K3o7dzY1SPVa/Vbcl6luntUKlVZb1Urch6mur5Z6k93pNYnxdiYztG1nvkmR4rqTF/ax293DjvWKIkHmg/J4lxbl3GbYSljnFc9/Q6Vmv63Je39HZstvTx4MUVfe5zsXFc5/o5Wq0lXc/1Nkjisn5+v4iRPuZ6xjWi12kPdP4vJfqY7i7jOpul+pjq6bKLotJA16Ak0U+QO3089B9j7Ms8MlYCwNBwpxsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEDI6QaGzMp3bhuZvsvJy41KOp+zXC0NlLcbJzqH1Yj+dM22Ee7s825LOsd1sWXNQ2/HuKTfU+x0dUZqaRmnx5Gx2kB551ZUbKWi90Ovp9cha+vn90plvR/iWG/HxMqdN6Jk5+cX9AOcczUjyzux8p+XsR0GOd4i64DoR7b3Bsr0zXI9WDpGHnqrpTOwOz37mLUyfY0oclc2cuPLFV1vGxnbrQV7P1tZ34mxH6xceiNa2TVGR/QDnHMLTZ2jHef6vFAq6XoU6Xra09uxWtHnjL68OtB4rJb1MR0b+6liLGNk7Mc+M45cL2NkHC8dYxF61nlrOcH0mZELT0w38JDhTjcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIFEeb6MgFEAAAAAAPBt4043AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAA8TH/zgB10URe5U8OY3v/mUWVYAAE4mmm4AwMPKtm3b3Gte8xp33nnnuUaj0f9z0UUXuVe/+tXupptu6j/maU97Wr/hs/74xnC5/DwvueSSE9a2b9/en9+73vWuB/7tmmuu6f/bRz/60f5/L2d5/B8/3TAcPc84jt3GjRvdd33Xdw1t/qFt2bKlv+w///M/f1zt2G179BsSX//61x/YH8v54x8LAMDJVDqpzw4AwFH+5V/+xb34xS92pVLJveQlL3GXXXZZv6G8/fbb3cc+9jH3p3/6p/2m/I1vfKP7yZ/8yQem+9rXvub+6I/+yP3ar/2au/DCCx/490svvfQhW/YPfehDD/rvv/7rv3b/8R//cdy/H718g3r2s5/tXvayl7k8z/vb5b3vfa97xjOe4f71X//VPe95z3Ongve9733uV3/1V/tvGizXmjVrjtuu7373u92uXbvc7//+7x/3WAAATiaabgDAw8LWrVvdD//wD7vNmze7z3zmM27Dhg0Pqr/97W/vN5W+CffN5tFqtVq/6fb/7u9YnwwvfelLH/TfX/nKV/pN97H/Pkz+0wBHz//7vu/7+m80/MEf/EFh091qtVylUulvx5Pt4osvdnfccYd729ve1t9/yzUyMnLcdv3whz/sDh06FHR7AwCwEif/igsAgHPuHe94h1tcXHQf+MAHjmu4PX/3+7Wvfa3btGnTsuc5Ozvbv0vu/34keNSjHuVWr17dv+t99Me0fUP667/+6+60007rf1x/bm6uX//qV7/qnvvc57qJiYn+v1999dXui1/84nHz/cIXvuAe+9jH9t/cOPvss92f/dmfnfD5Dxw40N/eS0tLy/6Iub9T7+927969e6B1BwDg4YqmGwDwsPlo+TnnnOMe97jHDW2eH//4x/sf5/Z/L0eapv3G8dg//g7qqcAvp/+zatWqB/37W9/61v5Hzn/pl37J/e7v/m7/TvdnP/tZ99SnPrXfgL/pTW/q//vhw4f7H0+/9tprH5j25ptv7n9XfN++ff3vyL/yla/sP/5E2/Q973lPf3sfPb3Ff1Wg1+v173YDAPCdiI+XAwBOOt/4+Tud3/u933tczTeCvik7+qPF9Xo9yHL4u7Sn0neA/UfF/ZsCR77T7b/T7t84+MEf/MHjHud/gOzIdvOPf9WrXuWe/vSnu0996lMP/Ar5z/zMz/Q/8u3vin/605/u/9tv/uZv9h//3//93+6MM87o/9uLXvSi/l31YTjrrLPcj/3Yjz3w3e4TfcoBAIBTGXe6AQAn3ZGPO4+Ojh5X89/R9o3wkT9/8id/suz5vuIVr+g3jP7v5X7c2X8P+9g/f/M3f+Mejv7iL/6iv03Wrl3b/4SA/2j4L/7iL7pf+IVfeNDjXv7ylz/ojYobbrjB3XXXXe5Hf/RH3cGDBx+4o+8/3v/MZz7Tff7zn3dZlvUb+H//93/vvxlypOH2/N3s5zznOcctj78T7rf3t/u9et/kc7cbAPCdijvdAICTbmxsrP/3wsLCcTX//eH5+Xm3d+/e4D+S5e+iP+tZzzru3x+usVPf8z3f049X83eq/Tb0d6n9OhzrzDPPfNB/+4b7SDNexH8Pvt1uu2az6c4999zj6ueff7775Cc/OdS73X/+53/ufuVXfmUo8wQA4OGCphsAcNL5H/LyHyu+5ZZbjqsd+Y73w7XxPZlOP/30E75JcKxjP47v72J773znO92jH/3oE07jP3Xgm+6Hiv9ut48B879Sf6KvGQAAcKqi6QYAPCy84AUvcO9///v7P8J11VVXnezF+Y7mf4HcGx8fl027/+i6b9iP3Bk/mo/6GvYy+U8y+E82DPPH9AAAONn4TjcA4GHhDW94Qz+26sd//Mf7HyU/lv+u8LfrkRYZtlxXXnllv8l917vedcKP9O/fv7//d5Ik/e9u/7//9//cjh07Hqjfdttt/e96DxoZdqLvdne73X58HAAA3ylougEADwv+e8N/93d/5+65557+94Vf/epX97/j6+98+u/5+gzpOI77H6kOFRn2SOG3o/9Uwc6dO/vfA/c/gOZ/Pdz/7bezf+PjiLe85S39v5/ylKf0P/r9O7/zO/1fPffTDSMy7ER3u/0PvQEA8J2Cj5cDAB5WPwzmc6Hf/e539yOr/vIv/7L/I2GbN2/uf/zcx1xddtllJ3sxvyP4Xxj/8pe/3M/w9s2yv+O9fv36/ke7fXTYEZdeemn/rrb/VXQfH+bf9PCN+P333+9uuummoS+Xv9vtfy3e/3I6AADfCaJ8JZ/XAwAAQ/fBD37QvfKVr1zRR+kBAMDDEx8vBwAAAAAgEJpuAAAAAAACoekGAAAAACAQvtMNAAAAAEAg3OkGAAAAACAQmm4AAAAAAAKh6QYAAAAAIJBSqBkDwKlmfsjz6w35JzNKUTTc+aXpUOcXu+H/REieDPcyNdwt6Fw65H3cGercnKsPeX7NIc+vkmVDnV8aD/9eQjzk4+7hrjTkMV19hG0/ADgR7nQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACBlELNGABONfmQ51cd8vzaQ55flCQP6+3ndYc8vygf7lKWouhhPWY6D/MXDb0hj8EQL2rSIc+vPOQxmA97fjH3YwBg2DizAgAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQSCnUjAHgVJMNeX7lbneo8+tUKkOdX/XNbx7q/JJV027Y2q957VDnV+71hjq/TpIMdX5JPNz3wkt5PtT5tdPhHiUjX/7SUOfXveIKN2xpozHU+ZWiaKjzy4c8P5emw53fkI8RADgVcacbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBASqFmDACP9Hch22k61PkNd27ORV/84lDnF69a5YYt+dmfG+r8snJ5qPNLhryPo153qPPrlYa7vrXbvjnU+SUveelQ55f9yXvc0L3whUOdXTRzYKjzSw8vDnV+pc2bhzo/AAB3ugEAAAAACIamGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQEqhZgwAp5rekOcXPcznl5eGewnI165xQzfkZSzNzAx1fun09FDnl7lkqPNL8nyo80vP2jLU+R1+318MdX7VR13ghi3pdoc6v+pvvXWo88t2Hxjq/Lp/9f6hzq9Wrw91fgBwKuJONwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgZRCzRgATjXRsOcXDXeO+VDn5lwUD/l916Wl4c7POZf+n78Y6vzGP/xXQ51f+9wLhzq/1i//r6HOLzrvrKHOLz84M9T5TX79q0Od39IVV7hhG/vEJ4Y7ww/9zVBnV9+wfqjzi175iqHOz33474c7PwA4BXGnGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQGi6AQAAAAAIhKYbAAAAAIBAaLoBAAAAAAiEphsAAAAAgEBougEAAAAACISmGwAAAACAQEqhZgwAp5p82DOMIvewtmrVUGcXfehv3bDVT9881Pl1fvCHhzq/6u//76HOr7zt7qHOb/afPjHU+TXu3zPU+UVvetNQ55c+6/lu2A5v2jTU+ZUvvmio82t0e0OdX/vJTxnq/KpDnRsAnJq40w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCClUDMGgFNNNOz55flQ5zfcuTnn5maHOrv86qe6YZt/828+rN9prmzZPNT5jb3kJUOdX3nnzqHOL5+aGur83KpVQ53dhGu7YTt41eOHOr/Soy8b6vyi++4f6vy6r3nNUOdXHercAODUxJ1uAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACKYWaMQA84uX5cGc31Lk55xabQ51ddM7Zbtiq7dZQ59fNhrsV09JwL6P51PRQ55d1O0OdX1StDXd+zeGOwb3JcJfPm+h2hzq/2v37hjo/N+TlSxcWhjo/Nzo63PkBwCmIO90AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARSCjVjADjV5MOeYRwNdXbRkBcwn54a7vw6HTdsaaky1PlFyXDfay6XykOdn4uGO2bSh/n8XHm422887bphmxvyMi6uXzPU+U3v3jPU+eWjo0OdHwCAO90AAAAAAARD0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARC0w0AAAAAQCA03QAAAAAABELTDQAAAABAIDTdAAAAAAAEQtMNAAAAAEAgNN0AAAAAAARSCjVjADjVRPmQ55cNd4ZZNNTZufjgwaHOL6rV3LB1h7zOpSHv4yxLhzq/qNMe6vyydmeo83NJMtTZ5a3hru9iPuQB4/fJkOeXVYZ7nERbtw51fm7/vuHOb83a4c4PAE5B3OkGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekGAAAAACAQmm4AAAAAAAIphZoxAJxyouHOLo+GO8NePtTZudb69UOdX3nV6qHOrz/PIa9zNuS3mtv1+lDnN3L66UOdX9RoDHV+aWW46xttGu76jpWHfy+hM+T5RS944VDnl370o0OdX/1JTx3q/Nydtw93fgBwCuJONwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgdB0AwAAAAAQCE03AAAAAACB0HQDAAAAABAITTcAAAAAAIHQdAMAAAAAEAhNNwAAAAAAgUR5nuehZg4Ap5J96XDnFyfZUOdXyYb7PuniwX1DnV/WG/77uI11q4Y6vziKhjq/1sLiUOdXO7R/qPOLpjcMdX7tpDzU+Y3suGuo85vftMkNW7nRGOr8hn6U3Ld7qLPr3Xz7UOe3+rnPGOr8AOBUxJ1uAAAAAAACoekGAAAAACAQmm4AAAAAAAKh6QYAAAAAIBCabgAAAAAAAqHpBgAAAAAgEJpuAAAAAAACoekG/r/27gTakquu+35VnTrjnW/PnfSQdDqdOSEQkjDPMigIiKgoOA8gOLw+vu+jPo6PS1RUBAV9RFFRQREFRARkHsOQkImMne4MPfftO98zV9W79uG5WZ0+v3/o2nSF4X4/a/WC/O/ZVXV27apz9qlzfgUAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFCTMsiwrauEAAAAAAKxlXOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAgG8Sf/u3fxuEYRh8K/jN3/zNb5ltBQDgG4lJNwCsUfv37w9+9md/Njj//PODRqMx+HfRRRcFr3rVq4Kbb7558JinPOUpg4nV1/rnJmCnO6H8Wv927tz5kEndzMyMXJ573Hd+53c+pPZwy/3pn/7pBx/3wz/8ww/5W7VaHfTDr//6rwftdntoXae7XOc//uM/gic/+cnBxo0bB3167rnnBt/7vd8bfOADHwjOpJO3IYqiYOvWrcGznvWs4OMf/3jwzWx1v36tf27sre6r0dFRc3nusW4cr7r33nsfdrmvfe1rH3zsqeO7Xq8Hl112WfD6178+SNP0IevJs1zX9u///u+Dq6++Opieng7GxsYG4+vlL395cN11153hHgUAfLOLv9EbAAB45L3vfe8LXvrSlwZxHAcve9nLgssvv3wwcbvjjjuCf/u3fwve/OY3Dyblv/qrvxr8+I//+IPtvvjFLwZveMMbgl/5lV8JLrzwwgfrbqLytTzpSU8K3va2tz2k5pb92Mc+NvjJn/zJB2sPN8E6Hc985jMHk5tTuUnPydxE+y1vecvg/y8sLATvec97gt/5nd8J7rnnnuAf//EfvZb7ute9Lvgf/+N/DCbd//N//s/BpHvv3r3Bhz/84eAd73hH8OxnP/vrem7WNmVZNthfb3rTm4KnPe1pwX/+538Gz3nOc4JvRi960YuC884778H/Xl5eDn7mZ34meOELXzj426pNmzZ9Xev5/u///uC5z33uUP1Rj3rUQ/777LPPDn7v935v8P/dBzz/9E//FPzCL/xCcPz48eB3f/d3vZb7mte8JvjzP//z4AUveMHg+HLH2Z133hn813/91+BDmGuuuebrem4AgG8xGQBgTdm7d282MjKSXXjhhdmhQ4eG/t7r9bI//dM/ze6///6hv73zne/M3EvHxz72sTOyLW47XvGKV8i//cZv/MZgXcePH5d/37FjR/a85z3vITX3+Fe96lVfc71unW7dJ0vTNLvmmmuyMAyzI0eO5F6u67fx8fHsmc98pvz70aNHv+Z2vfWtbx2s63Sobbr55psH9Wc961lmu1arlSVJkn29VvfP18vtX7cct7zT3VcP1w/79+8f1P7wD//wa677yU9+cnbxxRcP9Y8bW2NjY1m/38+9XDd23Bj6iZ/4iaG/uTF2OuMAAPDtha+XA8Aa8wd/8AfByspK8Na3vjXYsmXL0N/dVTl3pW7btm2nvUx3pdhdJXf/+63KfUX4CU94wuCq8b59+3K3d1dJFxcXg8c//vHy7+7r5kW79NJLg/Xr1w+uejvuq+buebmr7L/2a78WnHXWWYOr7247nc9//vODq+8TExODurtC/5nPfGZouZ/+9KeDq666KqjVasGuXbuCv/zLvzT7wI2DZrMZfKtyz9E916WlpeDYsWO527u+d2NIjQO3Lx6JcQAA+ObC18sBYA1+tdx9vdf93vRM+fd///fgR37kRwYTefcb3DNpdnZW1k/9ze0q95ts9Tvw8fHxoFKpPOy63O92nampqdzLdZMp95tg95vuV7/61YPf8j7S5ubmBv9O/vq2474277bxl37pl4JOpzP4/x/96EcHX0F/9KMfHfzGb/zG4OcFbv+5r6d/6lOfGnzt37nlllsGvxXfsGHD4PfY/X5/8Hj19e8/+7M/C37rt34r+NjHPvbgb7LPFOu3/RY38VdtJicnBx8sPZzV32+7x+Zd7o4dOwb//c53vjN4yUteMvgwAwCwtjHpBoA1xF3hPHToUPDd3/3dQ3+bn58fTKhWjYyMDCaR32h79uwx/6Z+S/7Xf/3Xg3+nevvb3x583/d930Nqq5Mnd4X+3e9+d/Cud70ruOSSS+Q6v9Zy3aTV/Z77t3/7t4Pt27cPfsPurpy7K8lXXnllUITVDwJWf9PtfmufJMlgsnfq4770pS89uD/d410A3FOf+tTB74xXU8h/6qd+Krj44osHV8U/9KEPDWouXM493k3E3fNyXvziFw+uqj9S3Dcz3KQ/D/fBgPt3qs997nMP+U2166/VcXDixInBPnZ99bznPU+O/6+1XPftEfc7exek5n4v7j58cFe93fIuuOCCXM8BAPDtgUk3AKwhq18rVmFlbnJw0003Pfjff/iHfzi4Mno63NXtM32Fe5WbCLuryaf6wR/8Qfl4F151cpr1qlMniWoi5ybJf/d3fydvhXU6y3VXed3EygWaffCDHxxMaF0YnQvZcuFsJ4fPnQmnfhDgvhr9i7/4i8HP//zPP+Rxr3jFKx4ygbzxxhuDu+++ezC5dhPNkz396U8fBN65bxK4ybZ7Hu5DmtUJt+Oex3d8x3cE73//+x/S1l0JP50k+7zc83LfILDC5BQXznfqhw+OS+g/mfs6/Knj4PnPf778gOV0l+u+MeC+KfA3f/M3g2+BuH/uWHLfInCTcfc1fwDA2sGkGwDWEHfrotXE6FO53+m637EePXrUnNB+I7grxu53ymoipriri894xjNyTeQOHDgw+K27+w2vdXX/dJfr0q3dP/cBh/vNtLtVmkvE/q7v+q7g1ltvNbfbx+oHAe5DArdv3VVq9w2FU51zzjkP+W834V6djFvc1X/3VfRWqxXs3r176O/u2wCnTrqLUiqVTqvvT+a2+XTauFvP/dVf/dXgQwaXXO8Sy11yubWfTme57lsP7tZ77p/7UMP9Tv4v/uIvBh/CuG9FuG8NAADWDibdALCGuMAs9/VXN/k71epvvFd/1/zt7tSJnLty665Su69Yv/e97/26l++uzrursO5fuVweXEF3k3AXVnamnO4HAad+kLD6e3j3bYYrrrhCtnHfhnCT7m937kOKk/vQfRXc/RzAfVXf3R7v67Vu3brBlXP3z32b5BOf+ERw3333PfjbbwDAtz/SywFgjXG/LXX3jv7CF77wjd6Ubyruwwh3f2Z39fu66647o8t+zGMeM/jfw4cPB98MXAL56gcDbsKp/rkPCtzXrt2EffXK+Mncfae/HbmcAPdND/fNj/vvv//behwAAB4ZTLoBYI355V/+5UGi8o/+6I8Ovkp+qq/e+jifb4dbhjkuddz1zWtf+9rcbV2qtQvTUtzXir9WKNwjySWWu4n36173OvlTA/f16tVvA7hvALiQuZMnoLfffvvgt97fjrcMWz1Ger1e8Md//Me52x45ciS47bbbhurdbjf4yEc+Mvjq+anp8gCAb298vRwA1hj3m1T3G2P3u2M3CXzZy14WXH755Q8mYLu/uYmB++ryN8Mtw/K66667gn/4h38YqrtbXFmhWyd/Fdg9DxeE5iaWJweffa3luonm4x73uEGCtUssd/c5d4nwbsLqfsPrwshcoNo3A7d/3/KWtwxuGeZ+B+6eswv3Onjw4OB2X+4K+Orv3V043Ac+8IHgiU98YvDKV75ykHD/xje+cdDu5ptvfsRuGZbXDTfcIPeX+7Dh2muvfdi2LhTtuc997qCP/tf/+l+DcXG6y3X5AC5EzYWmuVC6zZs3D7ICXMq9Cyp0IXcqowAA8O2LSTcArEEugMvdf/mP/uiPBreGcinLLozL/c7Uff3c3U7KTcS/Ff33f//34N+p3G+pv9ak23Hp3y706vd///cHIWinu1x3n2YXyPWf//mfgw8f3BVPd6XYfbDhfjv9mte8Jvhm4ibF7sq8u4e3myy7K95uguh+2+9+137y163dVW3XL+72Ye7DGDexdl+RPnXS/c3ETXLdv1O58LivNel23O3f3L50HzCcnMj+tZbr9vfrX//6Qcic+/DGfZvEhbK5W9G58fFjP/ZjZ+DZAQC+lYSZz/cIAQDAGecm+e6qMy/NAAB8++A33QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQfhNNwAAAAAABeFKNwAAAAAABWHSDQAAAABAQZh0AwAAAABQkLioBQO/93Mv8Wo3MlL3aldvVHO3qVQqXuuamq55tauNNLzajY3nbzc2NuK1rtExv/6vVkte7Uplv8/+4jj/6Sub8DvlhWHo1a5UKj1i7SK/VQVR4BnrEfq1izw/6u32O7nblEK//V2J/I6BO+8+6tXuLX/9vtxtTpzw2+FZ6nfuqsaeYznyHF8Vj+M77Xut6tLLrvRqt2XnBV7t5o8d9mr39CdN524TN270WlfW9evLJEm82tVq+cdlv999xF47nDDwe78QRxNe7aIw/2t4Gj+yr3Gh5/nc5zXOdxvTNPVqNzH+A17tAAtXugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBhlmVZUQvH2tZdepdXuyx65IZkkvq1i+K2V7swDL3aJUH+PkmSxGtdWeDXLkk9O9NTVPJo1PZpFAS+p8k0Cx/RcfKICtNH9viO84/LOKh4rarXanq1azRGvdp95bajudv893/d6bWuTsvvGGjU/fpyvrfi1a6djORuU/E4TzoT4377bfPWR3m1C3s9r3bPfvqG3G3iyfu91hW0ul7NyuWyV7tOp5W7TVTqe60rLvuNk0635be+Us2rXSnyaJf4Hae+osjv2l0Ylh6x18UojL3aNaZ/yKsdYOFKNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABYmLWjCw1D3i1S6K/IZlkoW522RZ5rWuoNf1ahZFvp9zRY9YP4a+7UK/dlmaf785SZK/TTluea7LY2WDhn7NUo/9HYZ+/ejZ/YOt9OF5xAXllXLuNmFc8lqX77HTDpa82m0+b0PuNsvBF73WNTvvdwykx5e92s0nNa92J5Y7udtsHJn0WtfoCb9xsu/gPq92F+w416tdL3l07jb95LDXukYqm73aZZ7Xcuq18dxt0qzvta5er+nVrl6reLXrp37nyl7Sy90mLfsd30EWPaLvaVKPPkl9t9HzvUnDqxVg40o3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUJC5qwUAYpF7t+t2eV7tyXHvEPnfqlLte7Urdsle7tJO/zZJn/88srXi1q3TrXu2CZb/1lYOl3G1KG8a81jU+WfVqN9H365OV0kzuNqW477Wudr/k1W7Eq5V7bqNe7foezy+MDnitq972O3aSbNKrXdjJf857zFmbvNa1MLns1W79tN9+m+/7nJeD4Nbr9+du89LvucJrXesmM692UTX0apemfn1SCg7nbhMv+u3vINrn1ez4yHqvdqNR/tfGat/vvFyt+J29+kn+1xynVPY7dqrZYu42K5nfukZTv/ddS6nfPiiH+dvVQr/+X8r83q8BZxpXugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKEiYZVlW1MKxts0c/1OvdnGp4tUuTfO3OXz4qNe65t/X9Wo3uVzyaldayf/kwqTuta64NebVrrfc9mq33J33apeE/dxtjk4e9lrXlb9+jVe78jq/zzXDJP9zq3djr3V1SmWvdmG26NUu6Pe8mtX6teCR0vfrkmAlTLzahUn+43si8ju+g4rfOOkGfsd3reN3Pl9sLeVuU5nyO96SxG9MRolfXwap39uubr+Tu02/7Hecxk2/gyDpN7zaVSr5j4G+52Wjbpb//OpUK37PLY42+a2vl/980mqEXuuqeXZmXJn0arfSXc7dph/Oea2r7tclwcT0z/k1BAxc6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBxUQsG0rTv1S4rhX4rDPO3S7O216pWDnS82qWLfs+tXq7mbjPVbnita6w74tXu4OScV7vscr8+GZlYn7vNFbvyt3GWTvg9t+ab573adWpjudts/K5pr3WN7vYby6X2kle7RuS3nSfGe7nbjDT9xlaW+vVJEJf82lXKuZuseJ670uWuV7so8Htu7XL+/eZUR/O/PemtJF7ralXzn1+dNPN7jWtU8+/vgX7+viylfsdbOuZ3DMRNv+fWjJbzr6uXv41T8jy+s4rfa+NC6nfMlev5r4vVPM+vK4FfnzR6fsd3XK3nbpOVJ7zWFS0serUDzjSudAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUJC4qAUDYZp5tet3ul7tSnH+z5A2bZj2Wtem/9n2ahe0Kl7NFufyH6oL71jwWlfntqZfu6v9+mT3j5zt1W48HMndplP1e243vP9mr3aX3b7dq91iL3+bylNCr3XV+6lXuySqebVb7PptZzXNf+wkgd+5pBZ2vNolSd2rXeqxC3rtFa91NUbyHzdOkvitbzzxahYcSvJ3ymjg99xG+j2/vvRqFQTNjt+5MivnP3bqPb/X4Xbi9/awXPI7L3Q9Vjce+g2usD7m1e74Cb9xsnRiyatdKc6/7zrdkte6alv9zpULlWNe7cLmeO421VG/92tB7LffgDONK90AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFCQuKgFA2EYerXr9/te7bIgf7tyuey1rrCcebUrj/mtr9to5G6zEq54ravS8XtuYaXt1S6rzXi1Wzx+MHebifLZXuuqLOXvfyetVb3aNbfO5W6z+/xJr3Ulmd/x1u6UvNrVyxWvdkmUv0/albrXumrJtFe7cn/Ur13Uyd+m3vRa1/0zXa926YLfZ/RHS4lXu8rO/G9Pov6C17pmm37n5Qf2+vVlUB3xahZP5D83b1zv99z6kd/rdxD5jZM4yn8+iUt+x/fRFb/z8v3v83utqt3gecwt5293f8XvdX/iaTWvdjte4HeuLGXLuduES37nknbc82o35tUKsHGlGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgsRFLRiIgrJXu1Lot74wSHO36XW6fiur+B06aXfJq91Ku5e7TSfpeK1rrD7u1a6UVbzaBSsNr2ZxOJK7zVx4wmtdE3Mlr3atY02vdrXLpnK3icf8+rGVJV7tsppXsyBenvVq1wzyb+fB/zrqta677vE7TuN1W73aTV6U/1y57dH5x4hz0z/f6tVu6werXu2OhQte7S74f3fnbtN9tN8x8JX3HPJqN/0ev3PecvOAV7uVifzn9MXvucBrXec/0++cF3VbXu1Kcf5rQN2K3zYePeTXLri17tVsdMnvWJ3ZlP89zeXHJ73WtffLB73aRc/d4NUuDvd7tMm81pWEfucu4EzjSjcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAWJi1owEEV+w6tUKnm1y4Je7jbdbuq1rqTd9mpXrfu1K0f5Px8Lux2vdQXRil+zkYpXuzCqerVrh838jeLMa139cujVrtTw+1xz5e653G0+8bv7vNY1fu5mr3ZnP23aq13lLL/j+9j15dxtyv+WeK1rfMXzGCjPeLU78dH8+3vytZd5rWt8fNSr3dkdv/091vdb3973HsrdZmt9g9e6Rm/MP7ac7d1xr3Z7L/E7D40d6OZu0/rAEa91dZ+5yatdGOXfRqeV5e+Tfuq33xb2Hvdqt+6Q3+tAutOrWRA9P/8xt/Q3s17rKmV9r3a1qt/rd9DP/36hlfmtqxz7jRPgTONKNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABYmLWjBQispe7cIo82rX7ye520SR3+dO1dK4V7uVpOPVrtep5G5Ta9W81hW2Sl7tgtivXTn2285qqZm7TSeY8lpXP8m/Lqecf7cNJN3846Q2O+K1rvodLa92e790k1e79Nee6tWufeOXc7dZ35z2WteJp633alfxO7yDxmfzn/M6h/Of75wwWvFql8V+bxeSyK9TNn2mmrvNl2+70WtdF5Yu8Gp3aMNxr3aNV2z1ahf+6QO525S7o37rWhjzajdS73m1S+Pl3G2qQddvXcdnvNoFS36vVceDvle7xlmbc7fZO3rAa11bzpr0aheV/fZBKczfl0tR3Wtd5Wrbqx1wpnGlGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgsRFLRjoJ/Ne7eJw1KtdVqrlbhN5tHG6waJXu2q/5NVuJpjJ3SbKQq91pWHbq11l1G+/pZ2DXu3CZDp3mzj2O+VNxvnX5Sx0j3q1a7ws//o2Xjvita69f7TPq926e/z65OiX/NbXu7Wfu83B+LjXujY/e6tXu9anlrzaVRbz9+VIe5PXutLeXq92J7KWV7vZs/PvN2f+ktncbUav9zufj+33O58vvDDxajeyOfVbX5j/Okkc+r0ONKNJr3bB6F1ezdK0mrtNd9RvG0vbT3i1a0/6ja/RfT2vdt1PH87dZvsP7fBa18YLx7zarXT9zrG1oJK7TTX268cs83tuwJnGlW4AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAAChIXNSCgWZrzqtdpZJ4tYvLDY82Va91ZYlfu2plo1e7IFnK3aQbtLxWVS759X+zE3q1C0t1r3adbCV3m27a81pXttj1ajdVn/Rql24ez92mX/Z7btWk4tUuDvIfb86RpeNe7c4+nn87O1W/Pmkt+B0DnRMdr3ZxfT53m1Zvymtd0eKoZ7u2V7vK072aBd/501flbnPjn9/hta7smN/+bt/q9xYq3X/Yq926+/Ovb2lj/tcOZ2tp1qvdPfdmXu32viP/MbD+ipLXui594m6vdp+/2m98TX6h7NWu8o78+2Bha99rXXNPG/Nqt/O7/NqlWf7X1FLH77l12n7Hd+B3qgRMXOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgcVELBtLUr1231/RqlwXd/G3Ckte6onDKq11QGvVqVourudssZ4e91tWPel7twrjm1a6bhF7t0rSdu03UbnitqzO76NUu6fmdYhuls3K3Cfsdr3VVsopXuyD2PHbSEa92rUqWu0192a//7/6T673arVuZ8GoXr89/fFcn/I7T0nL+fnTCuO7VrtEpe7VbbuV/ATnS8zuXnN/3G5Plll9fztVPeLXrXpV/H5yYSLzWdVbs15czNz3g1e7cGzbnbnPfDce91vXFBb83J+lI/vcYzv3X+F3fqu3P/xp35Z35Xzucm6/3G5N7nrPBq123v5S7TRz79WNU8ztXAmcaV7oBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAAChIXNSCgUp5yqtdmnW82vX6S4/YuqpxxatdWsm82vVX2rnbjJdqXusKYs/+r5S82vXTnle7OMzfl/PdRa91pf2WV7tmEHq1S4K53G3qkd+YDKupV7tO3XO/TfuNy37n3txt5qt+nyuf9+MXerWLSn774LZP35e7zcFbb/da18Sxvle7Rna2V7v7y4e82gWNidxNkjG/Vc115r3ahVvGvdrtevlur3ZH//Ou3G2izy17reszk/u82k3snfRqV43r+dtcMe21rsN7859LnI1fqHq1izf6jZPRdv73NPdsOui1rqldnttY9Xvdb0b5+zIq+b1/SkK/9zTAmcaVbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAAChIXtWAgrlS92qWJ3/qSpJO7TZb1/dbVPubVLmhMeDU7fuBE7jbnR7u91pXWQq92rajs1a5S8fvsL2y18zfq+A2u6uKIV7uw2fNql/Zbudv0W35jOTjh+dlrx29/n7Vp0qvdA+OHc7cZOV7zWtfKF5pe7bLFo17tNnwl/zg5sMNvbM10S17tNiR+42usNe7VLljJ//y2P8Hv3HXsunmvdhtv8zsv3PfrX/JqF7bzv2UbvWir17qSTV7NgtqHUq92y2H+fbDtJXWvde2qbPRq13relFe79uKyV7vRaDR/o81+x8D6rWNe7ZZ6s17twlL+94dZ4jdliRO/96LAmcaVbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAAChIXtWAgKCV+zULfYTmSu0WS+q0rC054tet2F7zaLS/mX9/elVGvdZWytle7ifo2v/UFZa92YdzN3aYepF7rui2936tdpZF/G53zS+O525Q7XqsKjlcOeLUrrZv0anf51Fle7Vo/kH88z7zvmNe60r1+42Qk63u12/r0Dbnb7HrOFq91HbzvPq92nf3HvdptvWKzV7skyXK3OWd8wmtd6391q1e7hXv9xsmWxK/deHVj7jZTlzS81nX9J+/yaje+2PNqt/T4/K/f67f5HW/Vjt/1puaWqle7sdRvfSuVWu42lWTFa11J2vRqF/ZKXu3KUf52vcTvPWVcqni1A840rnQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBB4qIWDHgr+Q3LKMtyt8k82jj9rObVLuz2vNptu2A8d5vOct9rXeWqX5+s2132ahf2E6923bCeu020fsFrXee8cptXu/qKX182tuVvFzaaXus6/+fO8mrXjape7cKJ417t9ly1Lneb7ZdMeq0rLfsd30Gr5dUsa+Q/VqPeste6rpzY4tVu5ol+567Sot9n+800/3lhpDLita7xEb/9Nj0x5tWuVt3p1W4hyd8n3dJhr3W19vmdl6uer3HTF47mbpNlfueSpcDv+O42/c6xSbLi1S7yuC4WtvzOy1mQ//XUqdb9XuOWWu3cbSp1v/eGnczvXAmcaVzpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIHFRCwbSrOLVLsoyr3ZZKc2/rrDvta4g3ODXLD3m1W7jufn7JNla9lpXtZG/Hwfry2a92qXdhle7rDyeu81I0PVaV7wr/7qcSstvfGWd47nbdJt+z626vu7VLi15vnwsNr2a9dKR3G2q5ZrXumbTxKvdxMSoV7ve4lzuNqWy37raqd+YjOZ7Xu3qqd8+iNOl3G1ajfxjxBntzHi165T8zrFzbb/XuDjKPy43hRNe67r0qX7j6674Pq922y8by92m2/A7L090/a43tct+567Q8zU1bOXvk/aE33EaZ5Ne7XpZ/nOXU5/Kv75y5NePQc/zfR5whnGlGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgoRZlmVFLRwAAAAAgLWMK90AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMA8E3k3nvvDcIwDD7+8Y8H3+zcNn6rbCsAAN8oTLoBAN72798f/OzP/mxw/vnnB41GY/DvoosuCl71qlcFN9988+AxT3nKUwYTs6/17zd/8zdPe72ry/yu7/ouc9L6ute9bmhyaP17xzveMXic2/bLL798aJn//u//Pnjck5/85KG//c3f/M3gbx/60IcerN1yyy3B93zP9wQ7duwIarVacNZZZwXPfOYzgze+8Y3BmXRq305PTwdXXXXVYJvSNA2+2f3wD//wQ7a/Wq0OxtKv//qvB+12e+jxD7cPf/qnf3rwmFe+8pVBFEXB7OzsQ9q6/3Z1t45Tl71v377BMn7lV37lIWPI+vfa17620H4BAHx7ib/RGwAA+Nb0vve9L3jpS18axHEcvOxlLxtMVt2k5o477gj+7d/+LXjzm988mJT/6q/+avDjP/7jD7b74he/GLzhDW8YTHAuvPDCB+uXXXaZ1zZcf/31waMf/ejTevxrXvOawaT0VNdee+3gf5/whCcEf/3Xfx0sLCwEExMTD/79M5/5zOB5um3v9XpBuVx+yN9KpdKDy/jsZz8bPPWpTw22b98e/MRP/ESwefPm4IEHHgiuu+664E//9E+DV7/61cGZdPbZZwe/93u/N/j/x48fD/7+7/8++LEf+7Hgrrvu+paYHLpJ8Fve8pbB/3f9/p73vCf4nd/5neCee+4J/vEf/3Ho8e7Di5e//OVDdTdZX92Hbuy5/XLyhzJuv7jx6fbfl770pcHjVrnHrrY92fd///cHz33uc4fW9ahHPerres4AgLWFSTcAIDc3Ifq+7/u+wZXcj3zkI8GWLVse8vff//3fD970pjcNJjluknQyd+XXTbpd3V2p9eUmtUtLS8Fv/dZvBe9973tPq80Tn/jEwRVoi5t0/dVf/dVggvac5zznIZOy7/3e7w3+6Z/+aTDJv+aaax7826c//enBBwZjY2OD//7d3/3dwYTdTdAnJycfsvxjx44FZ5pb1w/+4A8++N8/9VM/FezZsyf4sz/7s8Hk9eQPCFa5q+DdbnewL77R3IcZJ2+/u1L9uMc9Lnj7298e/PEf/3GwadOmocn1yY8/1erE2e2Xkyfdbh+6/dRqtQZ/O3mC7f7bjVW33pNdeeWVD7suAABOB18vBwDk9gd/8AfByspK8Na3vnVowr06kXJXlbdt23bay3RXOd1Vcve/p8NNcn/hF34h+I//+I/ghhtuCM6E1YnY6pVPx30V2S3/RS96UXDuuec+5G/uyrK7onzyBM59IHHxxRcPTbidjRs3BkVzX/F3Hwq4/eO2z3FfiXY/A3BXjt22uavLH/jABwZ/O3jwYPCjP/qjg8mtq7u/u6+nn+rAgQPBd3/3dwcjIyOD5+H6vtPpDD2u2WwO9uPMzIzX9rttdf2ZZdnga98+H8a4cXfyfnLcfz/+8Y8fTKzV36x9BgDA14tJNwDA62vd5513XnD11VefsWW63027r5u7/z1dP/dzPxdMTU2d9u/B3ZVxNxk89Z+b4DluUr1169bBlc9V7oq1uyrsJmunTtjcFXHn5Em3u/rvrobfeuutwTeKm6y6r7yfPIn86Ec/Opgou58EuK+579y5Mzh69Ohggv7hD394MCl3dbdf3dfTX//61z/Y1l0dfvrTnx588IMfHDzO/WTgU5/6VPDLv/zLQ+v+whe+MNiP7kq7L/ebasft21O5D0HUPnT7aJXbH+4r5KsfCri/uf24ug/dflvd53Nzc8Ftt9029NXy1Q8Q1Lr6/b73cwMArD18vRwAkMvi4mJw6NChwVXPU83Pzz9kQuKuitbr9cK2ZXx8PPj5n//54Dd+4zcGV6Pd14Efjruiqxw+fHjw22vHXQ11Hyqs/nbbTbLPOeecwRV9N2E7eYK/Ojk/ecL2S7/0S4Ovpl9xxRXBYx/72MFX2t2E1f3OW33V++uVJMmDV5Xd/7rfM7u+cF+tdle9V915552DgDcXFrfK/dbetXf1devWDWoukMz9ltk9T/dVdbf//s//+T+DK/r/8i//ErzkJS8ZPM79Xl2FzvlY3X73LYd3v/vdwbve9a7gkksuGXxN/lTuN/fu36nc19HdTx5W94f7bzfRdv/f9YebrLt9674B4ELVbr/99kFfrE7A1aTbjSv371Sf+9znHvITAwAAHg6TbgBA7km3Mzo6OvQ39xvtm2666cH//sM//MPBJPR0k6zdv7zc1W53Vdb9ttuFcD0cl4rtJsGncqnfq9zk653vfOeDv912k+7V3/q6SZv7Xfbdd98d7N69+8EJubs6vsr9Vt1Nyly4mbsy7P6/+zr+hg0bBoFhz3/+84MzyX2V2y375K9nP+95zxv6irhLXj95wu0mmm5y636r7v7/yV8H/47v+I5BorubrLrn/P73v3/wocPJv4d3E/qf/MmfHLra7cbA6lXk0+EmwSdv/+o++Lu/+7vBcznVC17wgsHV9lNdeumlD2nvrP522+0nlyDvvnruts3tb1dz/WGFqDnu+a1+yHCyk/sRAICvhUk3ACCX1cCw5eXlob/95V/+5eAr3O5ry49UAJULElu92v3lL39ZfiX55InZM57xjNP+Xbf7+ry7Evq///f/HtTc1Vd3dd39zf1u2E3M3de1T+US0l2Cu/tas/sQwn1l/k/+5E8Gk9Ybb7zxjE7a3NfEXfibm6C6YDT3YYD67bj7cOBk7vfe7psJ7iq2+6esBr/dd999g6+dnzoJVlei83Lb7H6Xv/q7cfcBhVuv9Q0Jl9b+tfah20/uq/WrE+rV33M77jm4pHlXc1frV/elm5CfyvXl11oXAABfC5NuAEDuSa676ql+s7z6G+/V3+Q+UtzVbjepdVe7T/4tsg/3lWn3wYK7SupuF+W+irx6pdslXLvn6P62a9euwaRaXSFdValUBhNw98+lbv/Ij/zI4Cq6+sqyL/cV/tOZGJ46iV29j7f7cOQVr3iFbONzG7e83G/PT95+d5X9ggsuGHy1/XRT6U/l9pObWK9+ddxNrFfvwe24/em+CbD6W2/1UwkAAM4UgtQAALm5ry/v3bt3EJr1zWD1arf7erm72v31TgJXv1buJtfuyvbJX11eDVN7uK8lK495zGMe/P34NwP3lW734YL7Tbeb9Kp/q1fMXTicS2U/9Wvj7nfiZ5r7QGc1ld7d29yX2y/uAxM3cXdXzlevdK/uQ/d83NfmXUjc6e5DAAB8MOkGAOTmfsfrftPrgsncV8lPlec3vb63DDuVm3S7rxT/9m//dvD1cpMw9/Vrd0s0d2XbXTk9ecLmJptugu/Cx1xS98k+9rGPyefvJnhn6ivZZ4L7cOHFL37x4Hfd6lsLq7cbc9wVfxee96//+q8PSfZWX0v/em8Z5rz61a8ejK/Xvva13stYnUi7e8a7Zblgu1Uu4M7d1s59lf3kxwIAUAS+Xg4AyM391vWf/umfBinXbhL5spe9bPC1bDfZ3L9//+BvbqLqfn97utzvnt3Xr91E1ydQzV3tdl8zd18xt7jbXLkUa/U16pO/Sr06CXMhaKfejsxdBXe/C3ZXYV1C+Km/c3YTRjfxfOELXzj4mrT7CrP7mvM///M/D35/7Z7jNws3qXUfErgPFtzvm91vzd3VYReg5m4j5v6/4/7mbgH28pe/fPA7dnc1+m1ve9tD0tFXuW8/uKR29xX6072V26nchxmun970pjcNUsZP/mDDpaj/wz/8w1Abd59xF2J38sTafb3f7UMX7uYm2avcdrvx6v7mPqhxvwFXXD+odbmfFrivrwMAcDqYdAMAvLgUaXerqT/6oz8KPvShDw1+I+smoO6ryO7r5+7WU2fqllJ5rna733RbV8vf8IY3yLqbIJ486XYTazdJc7c/W/099yr3dXM3Sbv55pvlFdLXve51g99tuyvb7kqwm3S7kK5XvvKVwa/92q895N7Z32huouomye7bAS74zU1y3YT34osvHlwhPnmS+pGPfGTwgcIb3/jGwX+7D1rcrdGe/exnF7Jtv/iLvxj8xV/8xWA7/vZv//bB+n//938P/p3KpbOfPOl2AW2PfvSjBxPrU/eh475u7j5AcJPnk7/JcDJ32zH371TuN/BMugEApyvMfL4DCAAACuFC6FzSuLsC7a7QAgCAb238phsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAi/6QYAAAAAoCBc6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAAChIXtWB887vl0C2yHoahbpAV+xmNFS+QJImsp1n/tLfdfE6GpG9sS9qT9bikl5P1k1zP1drOvH1jLSeO9SHfanVkPYr0Pi+VSrnqanusbYxCvYwsCnP1QWTslNTYJ0mm65W4rLfHGFLW8oPI2If9vl6OFbdhbKe1PWGYfd37NgqM/Rrovoki/fhyKco1vns9fbz1jT4zx72xPZnRlyWjb6rVqqynaZpreyZGx2T9nnvulvX/+q8PDtX+5V/+VT5206ZNsv7MZz5d1l/4PS+W9R07dsj60WPHZP3EiROyXq5Wcu3z1Hr5MfZVkum+zxtXY752GGM/jiunPfatY83sA2N8W6zlm8d4zteZ0OjjvMx9Yr2+GcdVqRznfJ2PTvu4tY7lMzWerNdIa/lW1+d9r5P7vUWQ5Xp8PzXeixjdFhmv56FYfpL0ci47yvVewWRsoyU03rsExvY8+tyr8m0PvuVxpRsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACkKQ2hpmBcTkDUyzA0aiXKEsRg6HGSQSZMPbn4VGoFHeQAwjRCQKjHAU47lGFd3HeYN1LFboixU0ZQVT1Wq1XNtj9UMUnn6gjx3ski+grGQEplkhPKmxb+OcgWmdTidXWEu9Xpf1atUIfMsZOmQFTVmsgDUdemeM177u4zjSLy39rh5/7XY7V/DfuBFEZo3LVqsl61lqjAUj8C1I9POdHB+X9fn5eVl/85vfLOvv/vd3yXpTbP/LX/7D8rFPedozZP2SSy/S27iwIOu33X57rn1SH2nkOk4Co4+tc5oVdGg93mIFMlrhebERjtTtdvVyxLFSKlnBX3obzZBJ/fAgMkPgglyvA4FxPLizZh55g0vDnHU7rNIKEQu+bnlfm82QvJzj1XoByt3HOYPULFbQYWq9gTPGlPmuMcsRVGksJIzOTN9br3u5Q+xyrRXfzrjSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBSC9f08ws1HzJpvnCL+0g1MRIUzbSO2VSsZVmmS/YOSgZKahZpJNs+0YabGYk4iZGJ1jJ11baebmsE7fLVvqyET1aMp6XlfaZGSnO1g6IRH+GRqZnYqSgpkk/3yg2+sBMJDXGjpXIXi7pvo+NvqwY6eiJlSRspYubx2HejFQrVTY77VsIVOKqrPdbnVzjeHpsUi8n0X2zNKdTwUsl3fdjYzrtfGJsfa4U2iNHjsj6P/z922T9Hf/8T7J+9OhRWf+u5z5P1r/vZT8wVNt53m752JaRBH/f/ffnOsattH0rIdq6Y4KZRp72zkiashUNbO1DlS7+cMn3WaDPR42KPp71tkRf9x0EBnXzhgbWHUF0vWQtv2SdQ4w7nRhypzvnrPeMuybkuXuGJTOWkRp3Rsmfah4U2jfW9pjbaS1HPzro9oy7dli3mTFer6y7c6jgcevOHFZqvxVSbt3pJH8SvK5nxvu6vKc0fPviSjcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABSG9HKefNmlEQkZGarKVqGrFYqrUyq8KTz/l0lqCuWy9jWGaMzE0NNJdzVBMI1nXWIyVvmo9LyuNvFTRy8mM1HQryTUzUm6t7YzFcsx0YeOjQCtxt22kJleNZHdrHPeMFPHQ2Ck1I408tdLXu8byZTUISpmxz634/5wpt3kSia1jM+vq51qygnKNc0u/rRNxrSN6pFKT9WpVp6k3qjqJ+8B9OtH7E5/4hKy/593vlvXjMzOy/rSnPlnWX/yS75H1C/dcIOupON5OLCzqxxpRudY4sI6HtpGCnhr7pFYx+r7RkPWlpYVcicTWucVMpjbvXpAv9TkwkrLDspEY3h8+JpK0k2tb8qRtD9ZpBUcbfyhZZx2jz6zHZ0aSvSV/SrSxXuNuJImxD61+yPPYvCnl9vGWnZGxkDelPO/zsp5taryvs1L+zbtkWPtEPDw03xxax7LVl9Zy8saLW6+d1jjj+ia+ipEAAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEFIL1/LrAhQKxzZSrm0kr5zppSr1NfBcowGKgk5MjY+S3TCtbWRZmqqXooZQW2laJZK+tArxXGu5NF+p5crkdRMQTeeWd6EcWs5geh/O2le/6VkfEY4VtHJ1KGRvN63+sZIoC+XdEp5aBwnPSvtOGdyr6WUI3X84eTZHjv9X6vWR2S902nJenO5Kevj4+OyPjk5KeuHDh+U9es+81lZ/5d/eYesf+X222T9yisul/Vf+ZX/L1caeWgcQCvLOpH8yPHhdPTK2Jh8bGSk7feNlP9utyvrsXEuqtf18ZYm+ly0uDAn62Xj7gKWyLqLhXl3CytyP1+6s1UPE+P4Ef1s3h1CLyEIYuPuE8a+tc6N1vk46et9FWV6+VFczXWOzZ0Qb8iM5cRG/+Rdr67ne+3P2weh9cJhSfON48B4zxEY79MyY/mZsZ1ZZNVP/33aV7fHeqOZ47UqZ4J7r2e8x8x55w9rH1rndeuODFh7uNINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEFIL1/DwtRIVD39UMn/W7cSG/MtxwjWfphkbbGMIO/C01yJm30rDdZIvu4bSaJWknCvZyRrGwGgVihmJS7nSr/tddp6vcZOLFlRrlaaaJJ+3Um2VmKotZhOq58rjbha1Qm9ZWMfdjodvWLjuKoZqc+dtu57q3+Mew7kTOi1x1SehN6KkWq8NDcv641GTdbP3b5DL2dpSdY/+J/vl/X3vOfdsn79l2+Q9SuvvELW3/Rnfy7rV131aFlfXl7OdZy3l/U+b7b0cqYnJ4ZqS8adC5rGuLTuXFCv6X2SGInby/MLsh7HevnrxidzHT9mirixPdZdL7KckftmSrmV7mzU0xzp5al1zPZ1X2ZRvjsX9I0X85JxR4a8KctWcrzVN+aLv7V8cxcafZ83HT3H48/U3Ses4zBver697WfmWpq1XmvxoXG8WW+9MuM1Re3b0Bg4Yc5tt/re6ku7761zkd6eMNR3iMDaw5VuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAApCevkaZqWCm+mU5sON1EojW7JkpIDW6g1jvUYCq0icNAJ0zW2x0svTvk4GtjIoF9r68eWKTqy20mzb7aasxyV9qI6Ojsp6ydhZzaZe/qYNG4M8ImP5VsJrFBhJqHlSWTO9c5eWVmR9amw81zZaac0lI2V1vFbPlfQdx8bptpEvnTZv3WIdz6p/rD7Lcj7XJNHHyc1fvlHW3/72t8v6F77wBVm//IrLZP31f/wnsv6Upz5J1vvGWGitrOQ6nqvlcq56raL7rS3ORzUjdbwel3KN715bp4hnxr5q1Ku57ozQ7+nl1MuVfInBUc4UZyNh2OqHrnlHCX3W73dasp6q9GXrWCtZL7ZGSrlxJwVLZPRN2bgrhfVaHhp92TP6rN/Lt69C6/ka22m+zhjp1HnqmdHH9vk1X928K4U5/vTxY4miOFefWXcF6Ad6e6Jyzn1irde4y4dKKrfSy617eWRm3Uq9T3ONY+sckli3/gH+L650AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQ0svXMCsINTLCOEs5B5FZN5b/uQ/+t16vkXJZSYaTIqtGUmY1ttIvddpkJ9WJtc1MJ3qec8nlst5Yp1PBw7JOAE6Mz8GsZFArVXbxxJysH9y3V9ZnKzpJODSShKM0y5VarepWgm6QGCmuxjhYMepbt+/IlS5+5OAhWe939fKnx6ZkPTaSVleWlmW9Vi3nSrlNjWRWS2gcuSUr0TeMTzv9t9vVx8ncwrysf/ijH5T1d7/vPXpbKrpvvu8HXirrz3/BC2V906ZNsr50SO/z40ePybp1FhkfGZH1tpVyaySDV41k4Lo4VpKqTi9vG8esdQ6plY20YyNhPUr0uKwbvdOo1E777hOD5RsvQJHeHHNsWinozU5b1ue7OsV9OdXno8RIU6+Kc12jpPtgpKLv2NEwkunLZX0sG10QhEZfpn0r3VkvqGe8Zjf7ug+WjOTrrvG6YaeR67FZN84LZeP1p17Sj6+I1HTr/G2Np56RWK1S7J1mVx/7nUjXe8brlSU2nqt5nFjJ9KnenuVE3wElNO5eYOzah/mDeKi1T4w7mliJ7O1WO9e+tVLK7SR7Yxwbd5TA2sOVbgAAAAAACsKkGwAAAACAgjDpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKQnr5GmaE0AaZkdhYTo30WyPJsWZ8plPu68fv/fTnZP3YvntlfUs8nPBa6ek05VJPp1b2kpasp6M6bbJZ1du+dP9dsv78H/phWZ9v6+0sG0nTmXGoWsnAi3Mzsv7Rt/29rI/1ddrsdK0u6/PHdTr6+PikrPd6Igm1r9OCx6s6SX0l0ftwoaHH2Ste9UpZj43E4A+87W9lvb2g1zsW6b4PW3rfTlR0Yn1kpMR2e7p/orpOp+0Zqe+BSOh1qkaCfndleHuOHz8uH3vvvfrYnF3S6eXxiN5Xl21bL+tnn7dN1qO5+2T9r/7gV2X9NT///8h6JdDj+19e/0ZZ3zy2TtZbiwunnTruRIEeI0lPJwNPbhzunxN9I+24oZPUX/ij+lxUKU/IejXU4+PuO26T9Zs/8RlZbxgvNLWSrleNoN+S8W4lMxK6+8YlhV5J/2HRSJt+7NOfLOu1KX0eqUbDT2Du7gfkY2/88h2yPjlqpOG3lmR9fFyfM7O+fn2rGX0zNqnvyHDUOKc1tpwt65suvlTWUyMpX9yIZGBkRB+fNSuJe0H3z6G9+jz1wK23D9VWZudz3ekkNu5cML5J37lk066dsr5uyxZZr1b1cWjctCM4duyIrI+NjMr60sqirDemx2U9NPq+1THOaUZIecV4nY9E+rqVIm4lsterxvuWeb1v+139Gp8YKf/WXVpKZeMuMISX4//iSjcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABSG9fA2LIr37jUDIIDRSyo3Q1yBI9R9CI3YzNFJuJ8s6JXqjSKEezXRM5KiRKtnPdBL0cqyTo5fLehuPf+UWWb/9s5+U9Uuf8hxZjxLd+ctG30RG51vpt1lbp9nWjfjYkUxHj1aNOM5aTydoJ2IsxFYccWtZluOy3pZDx47Ken9RJ5Vu3blB1sdjPUZGY93HE0a6cyU0ElWN1P44049f6up06n6mU2Ir43qf943PVu/Zt0/W9921f3gZHb1fpyZ0Wv32c8+V9U2b9eNHpnXabL+izxXzBw/IemtGp/a3jTFy9q5LZD1c1iniHR1yG0zG+jwSrejlRJleUKWij8PmA/cP1XrGeXe+pI/NrpHcOzqqU4316A6CWleveOYunRA9GerHL3Z031QCPdZS404TcUWfR7qxcS41koSXq3ofXvuka2V9pKqTvqvB8HKOzOmE6Ds+f4OsV4zzbmT0QaOuj5Osq8+loXHniMQYOzPG61Jjpz7Ov/fc3bI+OjIt68sr+nlVjPTo+QN6rH3+gx+S9SNfuVvWs9nh/TJivLYlfX3e7Rhp+KGRvF4a08fbzvPPl/VHXfkYWe929fZ8/OMfkfWlph6DXeN4u/ppT5L1C6++StZT43UmMd5QRiLl/+Hqcp3Ge8zUeF80NqIT2UuhPof0jUT2UkWfK8plXc+Mu1hg7eFKNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFIb18DQvDMFfSos4RDYK+8dFNx0hgTYwgx05DD8fyep14HImk8vmFOb0MI2G0HBsbY6Sm1hKdrHl8QScDf+W6L8j6rkseLesjG7fK+rHFWVmPjaDPxEgjr0/qxN32jO63vpGsXTKSx6tGinNQGu7nlpFePNdckfXy1JisdzO9LT0jcbdU1emxrWWd6Ns+sSTro6N6vamROp4Yqf2Vmu6zdevWyfq9czqJ+5bbb5f1g3N6bB6f1/0ciOPqrO16XI5Or5f1qfHhOws4I2XjGG/rvpko6wTgRkUnzTdjnQreX9EptGlPHycNI2E41IsJQiOht1zV21k1Hl83zoFhb/hYWWnp8dpt6z5oLekk68zogzTV9arR92mS5OqDibEJWa9kOk056uvlRFXdZws943g2krvbPX2cZ8ZdOyYaeow0KsPnqflJfSzHkT72x0pGXdyxwymHOv07MtKUy5Het1FDL7+/oMdOs6XPdSNlI/veOM7XG3eC6B/WdyP46N/9g6wfueMOWd82rvt/TNzloxbo9wTdtpHObby36Pd1H5+4V9954dDBY7LeeOCIrLf7uu8X7rtT1itTerzONfV7i7lD22U9DPR7F4t1HrHuhqPel5aMVH2XUy6XEejHV6pGMr1x7ioZYyE03nhZ6eUp4eX4v7jSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFIUhtLTNCstLQCAYxwle6RiiQkeUVlEIdQvG0736+rI93dbBO+dCJodqX3/d++djOnA4Kq1bKuQKTYiMcZSLSIT+z9+vQlBs/9SlZv+YFL5D1qREdKFVpGMFlkzp87rkv+h69nBkdthWdWJD1Gz/5SVlvLq+cdthRK9GdfNae3bK+55qrZH3rkt7Gdk332ZIRkve4Zz1L1ifDmqzHCzqw6iuf/KysV4zgm1ZXhxQdP6wDdG66b6+sL5d1Wsv6C8+T9cddfqWsbzprx1CtURsOHHJKHR1KNRXpY3y8rR9/63W6z5onFvV663rc95v6XNFc0kFToRH899gnPVHWU2Of33fLLbLeblknEj0Gl44ZgYljwwFXm84+Wz62bARhRjU9jsOq7gMjXy0oG+eirTt18NKEEVLUOa6Do+JMr7hsBHOGxob227rv123SoVrdSB+fs8t6DG4IdBBUXwTZrfT0sqOaDhAbaejQyKiznOu5To3qfT5z4kiuEMgTSzr0sj6hgzlbLX28TVR1UFvY0dv/kX97r6wnRujYBRM62HH9lH493H/P3UO1Y8Z5emRUB5HVRsdlvdvT56LpdXpb2kv6tXNm3z69XmPs7JrSAYWlcf345pIeC1FHBwumXX0O7BpJu10jMC0yjvOquBYYxXqqEoX5pjDGKcQOajPWa4UNp8Y5IU11HWsPV7oBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAjp5RiSGAmPVjJjElox5bocBjrm8qxLzpf1s6s6NTTaf3iodusndCp4Z+ZErtTKipG+nKa6c8aNhOu0oxNMb/v8F2R9+4V7ZH3L5VfI+nKq+75nJMrvvlQvZ6qiU2WDfftl+fMf+7islxKd/FoV/bm4ohNxLz5fj4OLv+OZsr7eSEFvV4x90tBps5c/++myPpnq1NfUSJr+3Gf0GDyyX6fQZomRgn7fXbLen9Db8+If+zFZv/SpT5P1TefqlPhAJJU3l3UacdnY31N62Af1jj72jxwaPpadw3feKetjozqhtzyv90nHuFNDuG2brF/7dN1ntUzfpeCjRjr1vi/fIOtjYzqduhnq/umJtNzv/N6XyMe2x/X5cslIKT/R0fu2b9RrE3r53/my75P10ZYe3//8f/5S1ktdfU6rGK9Lva4+/sOS3uePvlrfBSHeo9PXazt1Svxh464Jo/XhdOpNF+hj7YU//EOyfpZxt4DrP/5RWb/7xutkfX5Zp50HRpJ9r6bHyBOf/GxZn9p9fq6k/IqRuL3vVn2c3/SZz8n6YzboBPqRVB8/Bx+4T9aXxfF22bOeIB+764pLZL0yqc9FvZ5xh4Ij+r3IspHIvvfzX5b1pK9PsuNV3ccrRjp6sKjHcbakx05o3JYmMQK6e0aKe2Ckl2fiWmDZeE5hSY/XyAoLN96/BcYddSLjjiCZkcieZnqfp8b7NKw9XOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCCkl2NIYqVKWnUjmNF+vK4fO3RQ1ivrNsr6pEhNbxlpyi0jiTdJ6rret56UTrMMjdTUsZpOOz52QieV3vTpT8j6xu06WbdrpDK3+ka9q7e/3jDSnUt6+5c6HVlf19CPHxkbTjxePHpAPna2rVPNm0Za60qkT2NzTb2cmQWd4nrWxAa9fCN9NenrPjhw7Iisf+WOW2R959mbZT2q6H1y/qUXyvoznqsThstnG+nLRv90u8PHUBrocROXdIprJ9F9dk5dpxrPZPrxB1d0gm5nVC9n2RgLh5cWZb3f0mPheFefL7ZtmJL1qe1bZH3hc7qPG8a+bdT1+ejw8eF09zv36zsLnPeka2W919fnxraRRpwa6d+Bcf7etd0YZ7focd/q6dTxcmolA+vN6RnJ8YkxNktVfY7auec8WT9q3J2jY5zz++L4GTPOo5t27ZD1sUinNfeMdOSKcVxNT+oU9Nv23yHraazv7HDts/WdI6J1+pz5wCGd0L3eSIm+7+69sr5xXG9P3bg1Sqelj9sF4zzy6O946lDtyS97kXxsMqH78nhXp/PXjH2+eY/e9njBeI/S1uNv3w061XxmTp/Txib1GJkcn5b1xbklvZ2xPsfGxvFZMg7cvvFeqt8XYzzSx7h14xxLZLwuWYysczONPDFex1Ij7RxrD1e6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgI6eVrWM9IckyDfOnlQaQzHnXO6mBBshzXdGJrVtLDtDYynCY6MjEpH9uv6lTgLNJJokY4ZRAaH1OVjdTkyMi/bOjFBIfuuF3W939ZJ5XufNRj9PJHJ2R9fkGnKa8YCaNxpSzrPSN9udnXicSjYjRksV7GkpEK3gt1X0ZGgvtETfdBXNPL6a7o9Nj3f+Cjsv6VT3xK1m/68g2yvnOzTinftWObrI9M6TFbrRrJxis6tTZd1MndvY4+QtvB8D7PqnoczPR1cu9YrB/fiPWBFW7TfdM6cL+sH4r0cvpT+vg/lupz3ZKR0L1S0mNkOdVjc9vFOlF+2ti3J+6/T9YbEzqtOUiGt/MmIxV8w5WXyXrPSO1uGunflbJOa+4ZdxGIRvRZbWZuXta7xj6JjbFTtq4RpPp4WE712PyKcY6tXbFb1meNOzIs9Iwk8fLwa0HW1+Np/aix7UZy9D333Svr7Vndx6XQSnbXfbzubD1eezWdfH10RqeUd42U9dA4hx89cEjWVxZ0P/Q36nNjx3hN6ZT1Ptxx6SVDtXRcn0NuP6q3cd54ruNjOhU8ndXP6bwJfZeWK56q70rR7+r3Ljde91lZD43jpBuPyfrKkn4tD4xU9rKxb5NMb2dmHP8q1LxvvCELjbTwzLrLjHVXmpzvVTMrjdzog9C6EwTWHEYCAAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAUhvXwN6yU6gTENraRIIz3aSC+3kkqNxQSRkTDa6Rhp1r3stJctHjrQ7Se5Hl8VybTO9Lr1sj7f1MnR5Y5Oyl44PiPrN37yk7K+6zydmpxFOpE07Rsp5RM6eThMjYTeSZ2y3FuY1dsj9kujodeZGZ8FVip6fEyV9HJOLOk07w9/5OO6/r73yfrC/Qdl/epd58v6k5/yRFmvtPX2TI3r7e+FOn25aaVHd3VS8Wis+225q4+rVCS8Nhqj8rH1WCdcd9t6fDeN4/Oip+o+u+Taq2W9ZKSLzzZ1H2c1ndZ8ZHlB1juxXv4h4/Hbtm6Q9a3n75L1z999p6xv6Ov+3LBuONn4vvsekI9tzulzTv2sLbLeNt4FNCo6IbrXWpb1vnG8Hbxfb2fa0unIpVE9XiMjebhq3N2iaqQmH96nk+NX5vS+nVp3jqz3Y+PcWB9Ovw6X9LFpWVnSCdetJd3343V9Dhlt6NeBE/feI+tPuPxyWS819FhYmNGp6ZvX6bsRlIwX6Im6HvdzxnuCvnG5KIn1Pm8bCeOVxvB6O10jEdtIfK/V9V0ymsbtW0bGdDr6vpk5WT9vWr+3WH/BxbLeMu5qcNTYniPGe4KRqvG6ZNwtxLqzS6mk90kp1OtNcqSRWyHi1p129Ja4u9UYd+yxbmNjKMX6XBQbdaw9XOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBE6mFIFIS5kh+jnk54TK2UciNxshRXZb0W6QTTamN4+I6P62TQB9o6Cbq8YUrWO8s6wXRsVD/+UY9/nKx/7kvXyfryXp2IXY7152CH77hb1g/eplOQ9zzubFlvGQngzZ5Osh4fqcl6VtX7qm0kkoYihbZvpHDPzuoE9PaCTvT9xOc+Ievved/7Zf3+A4dk/fKLL5H1V//gD8v64y+7VNb/8y1/Lev33Hi9rFerOhU3DIzjqtOU9ayt6yMlPaZGEiPRtzy8z5NlnUwdGeO1ZsTKdjI9ziaMZO3A6IPMWP42I714saW3v2Oci1qpkWBsRPTO9PT5ZfulF8n6Fz+hx2zTuJvCSH04PTo9ro+TfdffKOvXbtsu671EJ2uHKzqBfqykzyGx7rLg2N37ZL1mPL5m3Doibeo+nprUCd2tnk5HP3hY3yHi8K13yfq1e/Q+TFO9ncuLw/1WTfRbrXVb1sn6LZ/UrxvJoj7GN26clvWFEzqRvSqOcWfTxq1BnrjwUeOuBr1lfZwbLz/B+hF9N4zDxrmrH+rjpBsZCfdVveID+4fH5taL98jH1o3xujir7xYQ1XTi+1LHeG8xotPCF43073irTojfds1Vsr7vPv0eYmSLTl8vjeg+O76kn29jQr/3Co07TXQ7+niOREp82bj7RN+4o0G328015Yki3cdlIz0/iowEd2O8mm96seZwpRsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACsKkGwAAAACAgpBevoaFxt4vpfqzmHKmkyIj47ObzEh3zZuabgSSDvKdhxjpkdYikkBvYz/U27JoJOJOnrNT1vckOqFz/5H7ZX1qQieJHjh0VNY/9t73yXptQqdBb73kMlmfC3SiajvRKbTVsRFZn39AP372xPzwNhrJoLfdfJOs/9xrXi3rD8zohN5Lr3ysrP9/P6bTyC++UKcUT1Z0UnuQGamsJT2muqlOie72dZ/VSvoAHRnXSb+jRqJ30NX7dnNVp+UGleF6mlnHle6DmRW9T/pGIm7b6IMjC8PjxrHOLA2R8v1wabahkUIbGGm2PeOODL2yXs7UNuM4PH+3rLfuvk/Wx8WdHarGPjl0l04LT5d18vVYrPfJipFAPTaiU4qTmRN6OUbK+mhFJ2iXrXN4phOrF5f0XQ1qdf28KqHet8f3PSDrvTm9/JERfa4ORML9aF2fL7N5Pb4P36u3pVbW54SSkXC9uKiTpqc36eTrdZt1fdk4fkqRTpWulRu50v/XbV4v60lZH3BN43xRr+n+qeubFwT7brp5qDa1QW/Lut27dH3jRllfMm7f0q7os1drWb9XmO3q142RzfpOKk94wbNl/ZrwmbK+0F3W613Ux225rvdtr2e8vhnvG8vl8mmnl1vLzozX4NFRnapfMu7BY773NOpBpOup8cqUWK/NWHO40g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQUgvX8MiI7lX5zsGQdmIAI/MZEad8JgZj7fqVsCwSpC00oiT0EopN+rGOhdaRgxqXSfxnmOkhYef/JisH5vTqc+1kk7Q7szrZN19twynsjoTmzfIenm9TiTudXSiqqXd0im38/3hlN599++Xj70v0Wmqj3vRC2T9VS/7QVkf33qWrE9u0Gmzi3M6SXhm7pis7xjV6cX9Eb2v2saY6qU6lTlL9QHXW9CJxNd96KOyXvr0F/V2hno7I5E83Ovpbex3dX0h0Wmzlz3nKbK++dILZH2xprcxM5JvreMkNJK+k0RvfxQb6eWJHt+dvl7O+nXTsr7jEp2Uf91te2V9RKRHrzNS7I/cq4+rmSOHZX3DrvNkfXlRn+vGR3QS9+E79XqX5+dkfZOREm31sZVk3TLuvFAuGenlxrn6wD06OX7mPt1vZz1Kn0e6ndZQbbSqt2XJON/fu/8eWY8r+i1bN9Tjb66rE+u37bxQ1se36vTyB+b0OadvHG9d434hJ5r69Wrq3LNlfXq73p6F+3X/jNbGZH3jqE7cnhfHyhfe+1752O2XXKKXfYG+E0F5nX6t3bxN3+mkNaHvvJAm+j1Ku6cT3Jvdll5OVZ8Dq+v18bxpve6zZEUfn5mR1m69ryvFeiyXK+JYSfR4yozrhrWq7susb94KRzKyy23GHRasPsDaw5VuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAApCevkaFgVGqqRRtz6hMQLAgyxn9GNqpDX3QyNxUiw/qupU48TYlr7xXK0I96WWTvOeb+mU2MkdO2T90mseJ+u3fF4nTTd0GHTQ6+nk3i9/5lOyvmPPubK+c8uVsj63rJNKu8v6+d67VycYr0TDp5qljk4p375nm6z/zCt/Qtbb63VK7JwxbA4vHJf1bld38khNpx03JnXi+8SkTqyuVPRyRio6JbZW19vT7+oxePi2u2V9tqnTbLNIb08s0su7Rupr2UgRP7Ssk+C3XnyOrG/Yo+t9kdr91e3R4z4J+3lCZYOakQadpno5kXFiaBop/x0jRXfHbp14fP2kTiSfWxo+3nau0489fOKorN/1la/I+sZzdN/XK3rfBmXdB/fu08nrqZHsHld1388c1cfn9AadTD0+ptOXHzh0SNYbRh/PHJ+R9f233iHrux/9GFkvpcPntXKoX4BOGMnuJ2b1Plxn9NliT4+/VqSP2/Xn6nNsv6ZT1lvGa3NqvDb3jNT0jrGczWfpc/j5T9B9fP37dP8cM/pzuqrPsevqw2OnfXxWPvaOj+g7jtz4iU/LemOLTl7fsFO/Bm/YrvfJ9l36XDG1Tr/+VGv6ndp8otPOlxZ1onzbOMfWQt2X1Yo+11msIPEwHD6/NBr6tapnLKTT068bJSNh/Uyll6dGaj/p5VjFlW4AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACkJ6+RpWsgIVcyYtZkZ8eWokgyfG43uhTrltZTqJMo2G03VLRrpraqSRJ8Y2xrFeTq+jk6CXOjpperKi02DPvfRRsn77HToB+MAdd8n62Zs2yXp/RicAf/GTH5b1uw7o5Ou77tZp5Nd/XCe2Rk2d3nnehecN1errdNrp0pTeWStLOmU1GR2V9Y6RVNozxrcRfB0kHf2cuks6DTZoG8fPih7f/VSPqdFQj8Ew1P3WCY309XGdNhuWdSJsSaTQ9oz05aW+0QcdnQCcRroPRuo6KXs009veTPQ+CTP9OXJspKynfX1u6Rmp6WXjeI76ep8vGyn/W7aeLevn7rlI1vd+8QvDRaMPRmv6ud5y642yfsETrpb1Deu3ynpg9Nm9++6R9WpV91lmpKAfWdFj56xH7ZL1nefr+vXv0ufSrY31sl6vVGX9rltvk/XHz+qE/pHS8POtRnpcHjl0UNZbrRVZj8f0ti8s6z7r1fU5ZOsFus+Od/W5qBvrfZWVdD00UqVDox9a43qMbLnyElm/oq3T2m/7xCdlffaY7p8JcV6bLunzaynSx/ic8bpUSU/I+vF9h/U2jugU/gMbdLL7lt36rgPbL79Q1reeu0XWmw19jl02zi+tln4tiErGey9jLKTGLSXUezLr6qCVCm7d1aVUMu7IcIaE5pYaUe1Yc7jSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBSC9fw8JUp0eGRiKklRSZZjqZsW+klPeNxPCe8RFQO9FJlD2x/HhEJzKHJb3wJNF9UDJSWa0+6BjLaXeNhOHNZ8n6Nc94lqx/dGZO1mfndL3bXpT1/3rXv8j6CSNd82lP/Q5Zf9JjrpL15iGd6Lt9enqodryl011XlvS2j47odOGVQKepVozPFDMjTTUykq9HyjpZd6Q+LuvjNV3vNnW6eH1Sp9aWEp0SPTN7VNbbmV5+dXpS1o8tHJP1TjrcD+1Q91lpXCf9Llf0MTvTMsbxsh43mZE228/08lOx7U63bxzn1h0WUv1868Z5oWwE1reWdQp1ecOErO/cs0fW993w5aHa8oLus3VGCvJXjun9fWC/vkPBeefobekfW5D1uaN6XE6M6HTkxDivt/t6HG86d7us737UpbJe/8hHZb3X08ufqOt+mzlwRNb336JTzS96zGOGi8Y558BenbBeMsZrvazfss0YqeONcf2cNu/YJutHF/WYSlPjTgrGXUEqFePOCCXdD3M9fReEkVE9dvY84XGy3qiNyfrR2+6Q9cO33zlUOzKrz1HTDf2c6sbxVq7qc9eWml7OSkef72fv0ncWOXi3rt9+s75LwZaLzpX1c67SCfFnXXSxrM8bd5poGYPBSimvRKXTfu/V7+tzRZLoE2+1qt8rhMbjTbnfC+d7j4y1hyvdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSE9PI1LDKCHK1kxsRI+k0jncyoM2KDINFh00Hf2KBWqpfUFx8ZxTWdWhlE+vMlKxWzZCRih6Gup5lRj3XydamuU58vvebxsv6V674g6+9/+z/IetTRabB9o++vuGKXrP/MD/2QrB+8+X5Z/4+3vUtvjwj0nmjolO8Hujo9dn52RtZHGlOyPhqUc417OaBcualTZYO6HpflSI/BclUn6/aNMZIYx9UJIwF46/l6H175rCfL+owxRpKq2J663sZ2olOTFzpNWV+3bYesN2P9nJb7uu+7xjiOK3o7+z19nIeh3udxqJeTWSnoxnKSrn58t62f1/YdO2V948aNw9uydzh52ZlYN6rrRrr93TfdLOtPeeIzZX35iE5Bzxb1Pp8e1eO+tbAs61XjbgGbNm+W9WjrFlnfs/t8Wb/7Rp06viHW56N2Tz+vO6+7QdYvuVCkqTf1OJjZq5PjJ4zU/tBIuE6abVnfdYFOoJ4a1+n5B47qc2zZOB6su3xUjSTryDhuV4zzRWKko2ehPsee/ZirZX3rrgtkfeqcm4Zq99/+FfnYEzM6xX7+hO6z9sKsXqeRsD49qsffxlE9vjvG+Xt5Vt/9Y/9n9HhdPKSf14kH9PPafNU1sh5MTuRKEu8aqebW+08lMt7XWe/Tsrwp4nnTy427XmTGe2esPVzpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgpJevYVbCY2Akd2ehkdiY6c9u0kAnOSZGkGMpNIZjqpOHM5GQGpd1KnhqpFwmRphlHOjY1DjT9X5Tp6821ulk7es+91lZ/9hHPyLrt3/qM7Iejuik4k3TOkl0/ZR+fGbEms/uu0fWl4/qBOPpcZ1UWq8O9//sik5ZTQOdahr09MCpG+nfS209bnpdvdPrRqJ8XDKOh7KRMGykL1fqNWM79diJAiOd1kjznzr3bFnf+YynyfrGpk6P7ojnlRqp4Fmk+7hvpLiu9PS+bfX149OuXn5knLvKxj5JEr3e1FhOHOvltFd0SnQltu6aoM9pfeMcu9FId9927u6h2r47dXp5lOpz1HRtRNaPfOUuWQ9mdHpx6/BhWQ/bK3p7xhqyvmgc/6W67suqkY4cVPVxu/08nV5+02dv1Osd1ftkuq7PmUfvvltvz+z8cM1IO+6eMBKujT7oGOeKlpFkffbOc2S9HBvnOuNcmsVxrvcQoZXibLzgGiHlQWCkoLf7uj/LsX6drxmvh+c/8QlDtd1XP0o+9ujBB2T93r16HMwcOiTrh+65V9abTX0niGhlXoeFl/XryaSRgl7v6uUv7z8g67ct6uOzum69rE/t2a0fP6HfA3Xaesx2k+FzfrmuzyGRcR5tGu/H6rHus9B4r5qXlaaenaHl41sfV7oBAAAAACgIk24AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAjp5WtYEugE0FZbJ/RWjQTj2EgMrhiBjfVQf9YTGRmm3VZXL39sOOF1dESnlLZ14HOQjeiU2KSpG9SNZODbv6DTyG+882ZZ/8cPf0jWd+7Wibsv/ckfk/Xyok6gvvVDOgW90dX7diTWY+GWj3xM1kdrOv02zHQab1ck09fG9eknOaRTTUO96WaStRE0H3TqRuJppPd5yfhscp2R3B2P6YTULEzznYQzvT3dnk5mXW7petDS/XnwmE7FTSrDac392EiCj/S46Rtp4VUjabrTN/o+071TLut62jEO9NS480LJSFM3brEwYiRxN1u6f8am9PlooWec08p6e3ZfOZyofNvH9LF5YkUvu5rp8/TIgk4dP/7Z62R97qBOL5+s6eOqH+kDdz7R43Vql05wX7d7j6ynyzqVedclOoV6+259Tl7Yq5/XhtExWT9yZJ+sz+6/+7RTvrPmnKwHDT3OOkYKelLTr807LrxQ1heN5OjUeG0uVYy7eVjvCVL9fNvL+vVqtKTPmfWSkZquVxtkxp0dmqk+rnrR8PPqG+s867IrZX1ylzEu2/o4XDp2QtZP3KfT0Y/cpe8ucOQunZreXtbn9RHj9eesCX1Xg/tnjsj6Z9/5j7L+lO97qaxvvvqxsr6c6fNCNxweCx3rfGmMm7JxN4ksyZk6bvRZYBzP1h17SC/HKq50AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQ0svXsG6iE3enpqZk3UpgnZ3VidUdI0l4rK7TYCtGtGTNSKKs1IaTK8Oqfmwz1emRS0YqZrCik3VvuOUWWb/ukE4SvfAZT5D1F/3Q98r6057zXFk/Z9tuWV+8Y7+sH71DJ+su3Km3c3xEp0q35xZlvRfoBNb1RhJqszOcWpsYCdQNIxl9ZWFJ1s82PjvMEj3+IjFuBo+P9PZEoU7uzUp6vbWG3n4jIDUohTqR2FKt6zFequh06sQ4boOaTkiujE6dfhS8kQocG2n4sXHHhEZFP6eySBd2Rkf1tneNxPd2qs91S12d3N03tjPp6edbN/Z5mOjtb3X1+aVljIVN55w7VNtyvk5NPnzbrbI+lRrn0Y4+N975+S/qbWzqBOq+0Zfjdf16stjWy9m+cZ2sB2UdWb1i3JGhNj4u61t2nifrN91xQNanjURvI0A7uOOG64dqcWwsw0hHHjHOx0ePH5X1sfXrZX3T2ToJvmU9p7I+N64Yx3m7qc/JtUj3TqOql79udPiOCc7cUf18N63TY6pS0efwmVn9epWKc2NS0sfsAyf0Mqo1ve2NaT3+Nm3YIuvbLrxI1ruPuUrWT9yjX8vv+by+k8o9Ylw6nb7et9N1/bz2HdP75NDt+ryz8wL93qVhvFdT78j6Vrp4pl/bwsy6Q44WGXcFyKyUcuOuHaHxuhEa24m1hyvdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSE9PI1rFLSu39hYUE3MJJ1R43k0akpnabcaenE8H5XJw9XR3TKZS8YTiTuxjqluB3rZd9xSKd8jxip5p2GTqF84vc+T9a/86d+RNbLZ22V9ROLOtX4y3ffLuvnr9NJqNd8x7Nk/YMHDsn6QlsnADf6ep+3jHT3TZt0Mn271xqqlUq6L8tlY3939L4tl3XSbzyv05FrkV5vy0jPj1P92WRY1snD9Vgn9LZWdB+nE3r700BvZ7Orx+axJZ00v5wYx1tVp/Q20+HHLzc78rFVI9l93Eigbxif85aMBN3777pT1iuxkaZe1stPSnoclyf1uWt640ZZP9HSac0lI8q6Z9whomecS1eMfbVuamKodsljdarx/tv0HRZaPb0tYxOTsn7k0BFZr8Z6XBoB3cHKsh6XnUSPqZ3n7ZJ1KwB4qTN8bnFGxnSi93mXXiLrN37sc7LeNpK7JyenZf3eu+8ZqtWMVO2KkcpcNjpzbkHfKeTCyy+W9YYYN87Msh7HsXHu1a/kjpHWbKQ+l4xk8NjYuXfeqMfyxw/ou3ZMGc+309Fj7UIxFvZcerl87H2H9WvnsjH+mtZdKYw7MlhjYXxCn6POfdy1+vGT+nh+4NBhWV+Y1cfn9KS+i0A6d1zW77pNv0e5/OnPkPWRs/S+mm/1Tvv8GvT1Oc14axFERt9Hxr5KjbsLWFcrjRD0IDJey7H2cKUbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIKQXr6GVco6k7Re0wmmi4s65XJ+YU4vv6IjJ0tG4vFoVac+T4/qROwDD9w7VPvSF6+Tj73vvuFEWafa0YnSO6Z0Auj4hnFZH9ugk2zHN2+Q9fuXjYT4qk59DkZ1fTnQ6ZrnXnmFrG+97iJZ3/+ZT8n6rin9vOqZ/ryubaSvq7HWMxI9W8Y4y/o6gT5o62TaqGuk7Tf0uE+6Ogk1NFL1K8au2mikGo/VG8Z6dWJ1Y1yvIIriXInylVg/fizWx+es6Ld6Qx+bgZHOHYo7CzhVY9vbJ+Zl/ePvfresL83pFOdyTe/b2aY+3p73Ay+V9Z3bz5b1ZkuP2X7XSKZP9HES13R/rhhJ2fPt4YTkHRddIB87tkEnry/NHJP17eumZD1a1vskMiJ6R4zxPbNs3EWgrsf3uXt0evlKXx/nS0Z9PNNjcMuuc2V907k7Zf3EPfoOF2dPGHdqmB/uZyPMO6gYr4UrLX0s94zz/TkX7pH1xErJF+PJiYz08thIHQ+NlPUo0U84Ne5SEOldFRzcq1+3b7ruM7I+Yhz/1l0EwpXhfrhk1/nysVP1EVlfMJadGcf4gvF6tdzTrzOLie6zalmfvzfv0Un2O3ZfKuu3ffyTsm4cnsH6SeM9zUF9t4NeU58bx4y752TiTidxzTjvpnp8x8brTGQ83gqaLxkHbmKcA62U8syKNceaw5VuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAApCevka1mrpBNMg0EmLiZGiOTGqUz3HxnS6a5To5c8c0umX//2v75T1Wz77uaFaz0joHa/oVNNdZ2/V9U3rZf2eI/fLetlIpw2MRNLW/JKsl8aM1M1Q16qvxnIAACCiSURBVB84MSPrY9ObZf3SJz5B1o/u1wm9i0t6O89q6BT31twJWa+IlNtyX4+DXkunnbaWdApy0NeJvuXQSNY1kk2TSO/Ebk9vT9TW+7Ya6KTfij58gk5TH4cb1+k+rhnb313Q/ZMZ/RkZycn95eE09fqkTvNPynoZbeM5ler6nDBhJMpnRsp/aUnXJyt6Ow8ePizrK4cPyfq4kcp81DjOe0bqeGYk9JaMO0S0l3Sy8ZzYhzvX63PUrssul/UvfeS/9To7Oj2/apynm0bydWNCvw6kHZ3EvWGTTlnfZJyTj3T0erOq7uP5ru7Ls4z05XOv0P121623yvrGhh77JXHeqRrHWmakI88s6vFdNxLTz7lQJ263Q+PcaNy5pJ/ox6fG3SoyI/bZGDpB30gvnzCSwTfV9TlwW1Un5U/UdaJ3u6PP1Qt3Db/uzdxxl3zs1I4dst7VXRa0e0bqeFUf+yUj7TxZ1uP+0HF915gNG88y6ttlvd/VYyFt67E5ZtxNpnnoXr0c4zgsZelpv2ey3l9ZqeChMe5D8/FG6rgVa26FkVvLNx6OtYcr3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQZh0AwAAAABQECbdAAAAAAAUhPTyNaxU0omqVjTj2KhODK2U9Gc399+tE7Fv/fINsv7Zj35Cb2dHp1++4ElPHapduEWndn/s3e+S9eYRnWo8VdYpqJORkfh8WKeIT3R1X166VSeJ7j8xL+tZpLenPDkh620jfXnLRbtlfc/Vj5X1G977H7LeKOkE1vGyHiOldDi/s2x85jdW1ymurZZOQQ6Mcdlf0Y9vt1ZkvWEka9eM5N6akUBtRZVWYiOh20i+7rb1uA87fVmPezqxtZHq7a8YdxcI0+H1lqt6nyz3jETpku6E6dh4ySnpvtlS0X28ZJy7NjVGZf2wcY6KmnqMhH2d6J0ZSfZZpLe/VNbr7SR6H3atsSBS0PtG35xz2SWy/uXrPivrCyv6OZ3d0InSfeOODH3j7hYz8zplefejLpL1sK6P5+aSPjeWrMe39PY0A32cbLv4AlmPx/VxstLV/TYhxmZq3GGhaqRtL83ru3CM7tgi6+PG3TZaid5XpbiSK73cuLFDkBjvFUrW62S/l+v6z4YRnV5+94o+76wf0WOhbZx2Zo8O9/PBO+6Uj91ytk4FnzCS4FeWFmU9MdLL20ZKeWSMnWnjPUpg3D2jb7xuGIdDUKvVZX2+rZP1u13jnGnctcMKBo9Ekri6I8DDbbyVap5ZKehG2nlqbGNiJa8b642stHOsOVzpBgAAAACgIEy6AQAAAAAoCJNuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgpJevYeWKTr8cHdFJxTNG0venP/4xWb/+c1+U9eWjx2X9O5/2TFnfbKToPu7CS4eX/cAD8rHVJZ0EXc/0ITDW1emUI8s6oXNnpvvsxOdvkfVZI/m6smGjrE/v0PUjszrR98CSTuje0tBJvHsed5Ws773lJlmfu/t+WR8xEk9TkcpcMqJEK6FOX+309T5MSnpfLZV1ZGg7M8ZCY1rWZ4/rZPr+zKysj83q1NqJ9TqJt9XTj+90Wrn6eMRIBl+476CsB+N6vSWRTN9dXJaPrdf0Osupke66eEKWkyM6rXldosdI30gvzox9ks3rxN2wpZcTGEm8kfG8kkCnMkdG7HO3a6Q4G2M/Fem9i6lexsZzz5H1TeecK+szt31F1ktGYn0p0wnx7USnec8bCfHb95wv66kR9dsTqcZf3SDdZ+1M988J47ia3KrTqbdfpFPNF2/Sr28bRIJ2f0H3QXlEHz/Nvk6a3rVN3/WiNKZfI+eXlmQ9Cq3zdJbrzguJkVgflfXzSjM9RoKWrpetfW7c0SQzltNe0f3QF8f53cZr3tQW/Rp81kUX6/q4vrPIkpH4vtTS55xypPty25R+vVox3qfdfYc+zitVvW97gd63K8ZdNSoN4+42xnud0DjXhWKfp8Z5N031cZIZy86sW4sY6eKp8YfU6BsrvdxKasfaw5VuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAApCevkatrCok6//6/2fkvXPfeqTsr44o9Odn3z1tbL+/T//i7K+saaTtf/z7f8s62943x8O1UZ7OrF2c81I4o11gvvsMZ2wvm2TTrg9fPvdsn7wqO6b64/qROnHvehFsv70bTtlPcx0qmd9TKfTLvd1/5y9Wy//4muvlvWbDhyV9UWRUu5Us+R0gz6DxZZOmm13depwy0gjrjV0amrZSDZNWjrx/RMf/pCsH7vlTlnfVdZ93z6gk/XXxTpRNezpvlxX0Smx84d1Mvh73vo2WT+8pNPLa2PDqbgdI8W1Z6Qsjxppx422fvy0kWSfnJjTyzfCY+vGmBqL9VjoNvU+t0JuKxX9kpkZ+8qiEnqduKz7rdsfXv58R69zx/oNsr79/N2yfuiO22R9xkh8rmZ6H6Zl/ZzKE/q8vnHbNllvJ3on9o2d0hd943SMc+NCV6cvT05Myvr2i/bI+qe/8FlZ3zk9PNZ6oZG2L6tB0DbG3/ptW2W9Oqb7eHlWp/lP1fSdFEIj8d3SNVL+61V9vEWxPvcGkb7+M1rX57rYOF/US3o5iXGnlrHJ4YTxQ/fpO3N8/H3vk/VrjfccOy+6RNY3jOh9tW16vawH1rllQR+fn/2ovpvMPTd9WdbP27xZr7anx2zfGLWNSX38VEf03Q6ykpVwP7z8XmKcc4xU85JxRwMrjTwLjXR04zhMjJT/yDyiga/iSjcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABSG9fA373d/537LeNlKcH3f1Y2X9ec98pqzv2aETsWMjdbfc0gmVR48elvWl5eWh2pbNm+Rjm0ZSc2CkeU+LVFOn1dbJt30jiru33JT1spG+OnvkaK7trFV0Sufiit6H46Mjst6O9Pace/lFsr60915ZP3CDTkgN68PpsY31OtW0v6QTnFd6uu/rRiJueWlZ1zN92ts4qdNjKwt6Ob2jOuE+G52S9a1jOt21YiRox6k+HhqRTgYeMZKvl2f02N8Q6s9c++LxVSMNNjHiXctGCvKo8fiSkU47GunU4XLDSOfv6uVMTOjjudnUx2dmpLL3jXT+ONZjqt02lp/qfdXr6H6rhMOPXzGSdU8Y5+89j7pc1r/4sY/Ierenk6kbRt8fmzeOhxF9zhnZsFHWZ4xzZljWY6Hd1X2WRXqfJCXdcQfm9PbvuESfA2/col9rDi8Np0pvnNLnhNmWPjbTmj7Gt+0+Ty9HvBY6VqZ+x3g9CY1Eaev6TMl4vHWclI3U/sRImh8b0ynraddYvvWW1ki4X54bvjtC1Xgtbx/XdyL5yD+/U9bXbdR3e9m86WxZn54cvmuE02vq4/CeO26X9ZkH9J1RNleN18lEH2+RPkUFd969X9af8/Lvl/WN27bL+u2HD8l6vzS8YuOQDTrGOWq0WsmVtm+llKfG+6Is0uM1NO6MElq3w8Caw5VuAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIQWpr2M7tO2T9+S94rqxfdtllst5Z0SEuR2Z0ME1DBGU407EO6Kmv0wEjaaM2VDuwtCAf21sYDkxxyokO1pgPddDMihHINj2tt7Hd1+Ffc/22Xn6q19tO9HJKRpBa1tVhMAvG9kdGGNGms7bK+nmPuULWZ47o0LsH7h8OX6kv67CqJf2UgrlE91nHCOGqVHSYSpDpzxqjQPdZparHa9+IKTo0d0zWZ1f09o+FOpSlHOh6Funt7BsfobaN5fcC3dFpJBYkgrwGjzX6smoEyqRGSNFyT4+/oKvHfWSEAi1HejkHOnrcb2y1ZP3I/Lysd4xEn6ZxPPdSfX4ZaehgrcgY+6EYm5kRbtcyxuXIhA6l2niOfh04cdfdsl6K9T5fNEKHtl5wgaxXJnWw4LKROdTtG8eDMY4DI7Srp4dgUK7pMd6oGMGIF+hQs7s/87nhZYzr53pwZjh0bfD4DTrUceu5u2R9vmMcJ3ElVyhdoE+lpkidKwZdr/u+b5xje8a5Lq3qt6hlYywvG4FpWa0h62Mjw/2wrj78vsLZf/99st5c1O854p7elpUD+vVhrxEwmxrhitWK7ptt6/VYK9f04+cWdEDcvUf1du6+XL8PPOeSS2T94Jw+l8ajOkg1Eqlmi019nq7V9XvGOWOflI2w0cx4jUx0OQiMc11ijG/j1IU1iCvdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSE9PI17Hd/53dkfW55Vtbve+B+WS8Zib71mk5IXWzpRN/USJU969KLZH1EpJpXjBTU0EgRNwI9g0pZJ+I2l3XabLut04uzqu6DuKe3Z8tunU67XNLPq2PEa1opnUbgcbBspINu3qiTUDecv1vWpy/cK+sHO8MJps2G7vw00mnBC0Zi6IyREL0S6ijeXkfXU2NfrTt/p6xfaIydRl8P5FKrqRt09NiJIyu9XC9GhL4OdEO9PamRMJyEwysIjfTyrB/mSi+v6k0Msq7eh/32in680feRkey+saf7fuLc7bI+m+ok4Y5x54VM9NnD7ZRSpuuZkXhcFuejJaNvQuPYHxvRqcybjHPObbfdLOuzc3q8HpzTd4hYd+Wlst6t6O1pGYnyna7um9BIcQ6NVPN+Xx///Vj3WziqE5LPuvBCWb/x+uuHavtXdHr+AeO5XrDjYlkf3bhJ1ueWdN9nxls867W2Z8U1W6ng1nUb4y4FSaaXv2zctaNv9P2SkWo+N6fvmJIY59JzNw2P/cuvvVo+dvyAvpPHA3v36W05rFPBR2Q1COpVfXYcnZiQdSM4Pphd0cndJ46fkPWwqs9FIzs2y/rjvut5sn7WBfp92r3z+v1kKdbPqynuZJGEen+XYl2PjbqVqm/d6cAa3tZyjMMqyIz3Llh7uNINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEFIL1/DHnjgAVnvBzqpNK7pdM1+XycPtxKdSFo2UqJTI832wmuukvVaNDx8a7H+HCkzEq6zzEjEzXTCbberk3trNb3t8YjOKj3e0cnDHSMVMxXP1VmZ1Uml1cxIL4/1PoyMBNO5jk7XrU2Oy/oFT32irG++9PyhWnlUj4MHFnUCbWPTcFq9kzR0wm3a0fu829NJtqkxvi+6Vo+/upFyO2JExGcdnVg/WjNSk60sVCOh20ov7xvLSYx6KpK4jdDhIMv0cy0FemPKRmp3apwr+kayexjr9SZGOv9iTy/n6IJOfe4YCeBtox+qlYaux0Y/GGOknejtLIvk8dDYlp5x/k6NxOcLHnOlrDeNdPTUODfuMB5/0WMeJeuLRsp/y6iHRoJxJdLnkdBI0G9neqx1jdeIpVSvd+ocfVeDi540fA5szuoE55Hzdsj6nquukPWZpWVZD4zzequrXx/qxrjsJ7rvs5KxT0o5r9sYj28by8+mR2X9su98lqx3FofvkuHMLev6+q0bh2uX61T6dRfplP/pc/U4uPe2O2X90N33yfryoj5+Ds3rfZ4Yx3kv0sdnbbO+K8h5l+/R9Sv0cbt1t+6fpdR6XTUS6I07pvSj4XPv6Kg+v64s6btSjDb0+67EeJ1JjZNpaNw1xkopD427WEQp1zfxVYwEAAAAAAAKwqQbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAAoSZllmBfHh29wte78o6z0jnTYo68TTUs1Ij0310Op1jOX39OOrJZ0eG4sk5DAwIn0Dvc6ekWaZhEbyupFq3O3q5NuwqlNllxIjTd14rlbqcyXTn5uNBHqflIy06b4RT93v6X6ox3r5NSNVuio2sz6uE9+PL+tE6bCu1zlrJJgaIchBv6X7fnx0TNbLRmpyrVTOmcSt08ujyPjs04qnNsa4lcBqSY0xpcp9I408KeUbT1aqeWCcK7JUH7dxbNx4I9LLb7V1Kni9rpN1F5Z1knBgpDvXqzpdNwp0/1SM47yzrLdzZHx4bLaN9OJOSy9D328gCKZH9LYHbePcaKy3a7w+ZMY5c25ZH7dGCHIQG+cucywYesZrQT8yxqxxeG6q6fNFKvq/1Ldef3RfVif1Ppk3Ep/jUZ3WvLSi+zgO9bm009bbGZb144NyKdcdFpKeXv5EXT/fXktvf0Ok+TtjY3o5s3Mzst7vDi8/S/Q2NmLjLhM9vQ8jox4bb39mj+q7dszPL+rlGwn01Qk9Firj+lxXGdP7tlzTyfHdjl7v8fklWW9s0HcdmV027owiksdj4zV4ZVan0o+PjeY6lq3Xzp6VXm68jAXG+zHr3PXYXdcaC8K3K650AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFCQfLGf+LZipSbHVipzSSeS9o1k1n7PSFk26mO1kVyPXxFp0FYQdLWmU8TTWDdIrZRvI4k3CvXyk0gvJzVSn0tlfUiGHZ2EWon09iRNY58kui+7RhqnlU7dK+t+m7USunvDieG1vn5st28ku1up90Z6cdVI3K2XR3MlaHe6us86QSdXEmpopN/2rSR7Y3us9HLreI6sxHAj6Vvd0CINjHRx4+YXHSPlOzCWE4Z6W1Ljc+HYqIdGrGzHSJVNreOqbCT9VoxzoxG5nRjp1H3rRFXTx1s7HR4jmXHuio36SkunBYdNvU9qNZ0QvWAkYldGJ2T9xIJOXy7H+pxZNu6AUDYO/7Bvpfnr52W8vAWpEcTdNZL4j7WMhPv+8D6fntDZ8W0jnbtjJF8vG3fJSE7oc1FoPNmSkTRvxTtnoVE3xn3f2P4k1DtxsavHZmYkyh9f0KnVY6lO3F82xmxVHCst484FFeN8WQl1H4+PGAnaxh0NRiemZH3KeM8RVvTrXj/Ufd/q6bsarHT082239VirhPrcGI3o9289I3m8KlLKB3XxPnBlQSejT4yP55rYRMbrVWK8/mTG60lipZcbyzFe9rAGcaUbAAAAAICCMOkGAAAAAKAgTLoBAAAAACgIk24AAAAAAArCpBsAAAAAgIKQXo4hvZ5OAE2NNOXISNwuW6mbRsBjmOo/xEaSeBQNp+uGVmJtoj9fSoxk2sRKI27r9NXYSEeOKrpeNpKsSyXdl6mRBl0yklOzVCeYVo1E4paVGF7Xj+9VdDK4ztB1z3f4ecVGgnOlqlONYyMxfaGpk2/7Pb2varHe9n5XPz7p9nKN+5KRcF0y+iwxOs1KBi8Z9cBIWg2N5OGS8Xi13iwxxn1fL6NmJU0bydSJMRZ6Rvp3aJ4TjOeqD+egZJxz6lU97jMjOb6b6TGSRXpftVPdQamRrJ1l3dPve2v8ifOl0zXOdV1jYC4a215OdB+ERqqxCPn+6uONeiXL97oRGfsqM+4uYCVlG4sPesZdE5baw/3WWdEJ0d2OPneNVI07L4yN6W007iBSivS5tNUxxqt5owMjxdl4nbHuaJKV9U5sGWPHOp5r5Yas95b1eiMjob8xNtzPG6bPlo/tiztwOKmR1N5cMdK/jX3bMu7S0mkaY8foY+uuFOWycceUku7LWlmfL0Ijfb1e1cfDsvHeomQsvysS+kvGHThGqzpJPTD6JrXOUcbyM+vNqsl6Dc67HHy74ko3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAUhvXwNsxJGazWdeGoEgAftjk65TY0k8YaROLm82Jb1elk/viGSuBMj7bhvpXMbSZ8VI724ZKSpVnv6UOobfbDcXcoV3RtlVhy0TkLtGdHAUWwkxFd0wnCzq5fT7evU3Tg0kk3rw2MqNVLBW0Y9TqNc46ljjMsotNL2jX0bV/MlcRv7qte2tsdITpVVl0yf5UoeTowlJYmR1izKYV8/NjZWWrZSkI2XnNT6/NdIss6M9cbGPikbfVyu6MenxvHTNFKoYyMxvNrQ55FOVy8nNc4vUTS8nVFfP7ZvHLNWGr4Vhh+W9HNqjOn6SqefK7W/ZKTqG4Hv5gERx9a1A123Fm+lppeN5WfG40dHRMK49Tpj3OGjY6SL95r6HFIr6z7utjq50pQrkT4+Q2P7jSB4871CZIyFrpE0XzLGoHWbjGpJL3/EGLO91vBx2DHuCtBq6de8sYlJWS9Hxr41+iw00vDjknU3jyDfcW6cTI0buJg7t2fcvcBarzXWSiV97m2Kfh6r69f4Fes11XgfWC7pgWmE8NuvwWYYue4z420g1iCudAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABWHSDQAAAABAQZh0AwAAAABQENLL1zArFTNr63TK0IiVrcZGgraRlN000p1Lozqhcqmpk8dVImxkJGj2jKjZzEjoTIy4ycRIxC6VRmV9YWFO1sMRo49LxvZ3dLJ7s6uXMzY2LeuLC3r765MTsh73jbRmI+q3FOrU3aw3vN6qkZLfFknnTrOt+6Dc1eOjbiTQ94zHp8bYiY2U1aCtl9PvGdtjPK/ESMqeX9RjZ2rjellfaer+6fT08uOy7p9KRdwVIAhzJdm2jPTlyEhej4yU4l6vlyt1PDI+R+6lep90V5JcabNlI3G7ZKw37OnzS8VINm4bCcP9ZHiDIiOlfKzakPVusynrZSP5OhXrHKzXSvk2tqdqZgBbkcHGWLOSgY07JpSMemrsk9iI3K5GtVxjMxNp530jlb5j9HGS6n1iDL8gTfVzjUtGKriR7lwx+r5rnHtj4/UzNvZt0jVinyv6iXWNPh6pjMv6yoK+K8hYVfdnJO5M0U71OmvGa+S88Z4gM5KyS8bxFho3KCn19RgpGang6k4Hg+0xxmDPeq9jvE/LrDtBGEn8SaL7MxHvCZxKZXha0jaWERrR65Wq3pbMeHyYGan9xrkrzplGbr3PxNrDlW4AAAAAAArCpBsAAAAAgIIw6QYAAAAAoCBMugEAAAAAKAiTbgAAAAAACkJ6+RrW7+t0yig1En1zBjBaKZdWMnCv15L1sapONR/NhlM6R6o6ubM6ohN9UyMRN9VB00HTSIiudPV6p6bXyXo8pde70JuX9cRIa+6s6H3Y6+u05rEJnfq6uKxTyssVnQLaqOlTR6+1KOtZfzh9tGUkso+u0+nc1areKeNG4m5sjNe0rLe9ZKSUx0YarHX89KxUWaMeiLRWpzw+prfTWExSNtKXjeMn6RvJr6Xh7ekYib5WQvzIuB5nxk0EzPTv1Ghg7ZNuTyd0Z8Z5oRzr80LNGGv9jj55tY1053aix0gcGanPVnq06IfMSOdOrITenOO4bOyT0Zre5+uNcdYRx/7DrTcxEu6tc3Vg3HUgyPT214zjMEuMA2vZuOODPjUGHZHc3zf2SWScc8riGHRKxj40X1QN3Y5+fcisO5TU9T5PjTESBUZ6tJFwvWIkgFt3Qciq3VxvaDPjjhKRuPtCaIyzxHhNDY2EeHV3FSftG3dAMdLLw75xpwbjXGG9UUuNh/eMO6Zkxh1KIqN/op5eTmjc5UOl/Ft3z4iM9xuRcQ4MUr2N/a6R5q+XEoRWH5us45Drm/gqRgIAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBAm3QAAAAAAFIRJNwAAAAAABSG9fA1LjdRKKw02NpIZK0bCoxHQG4RGOu14fVQvv6sff+jW28VKjdhxI/K53TJSu+vGoWElA1f0tidGH7eqer0zzRlZ37Rzm6w36jrh+sT8sqz3+kZSaaRTZXsi3dUxQmXNeOqp0ZGhWrlkJFYbqdrHHzgi62MTk3pbjKR5K/m6FFipxrrPeomR6F3Wy2+Hui9nWguyPrpRJ9+3Wjrlv9/Wqbg1I7k76ul+nhJp6s1EP7Yzp9P2R42U/LRlJPcaKbQV43irGinIQXNJlmsNfV5oLejjMK3px8dGInbdSMTu9vQYLFf046vV4ePEme8Mp7JXx3Va+ILxnCrG+XjDqD6HxF19LJe6Oo14vKG3p9nT474mzglOGBuJ3sY+TxK9nd0VPdZi4zivhRVZTzO9/amRWt0VCeB9I8laj44gWDb6eLmt0/kTI7G6VNHPKRjV29MyEuW7Jb09SdfoG/3woNLS2zNd0XcRqNTH86WsT+mU9di4I0ssXp8T4zWs1dd7KzHeW3SsxHQrsd5I866m5VyB9anxPq1npJEbweBBZrxO1o33e6PG3QtWjKT5Zrdz2nd8WF4x3qcZd8+w7pxTsu4gUvDVSuMtL9YgrnQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAUBDSy9ewsJQvddz6hKZkpGKGRlK28fAgTnQ66NG998n6u/7yr4Zqm40U1JqRoNtbXtTbaHRCvaETd5daOqGzayTctss6jbNT09v5qCc9Ttaf8vzvlvVwTKepz7T08usj+nm1Ovp59RIjzbajU9OzeHisjZX0Oj/78U/K+s2fuU7WK8s60Tdc0dvSiHWCbmKMkcwYx6mRSBxP6BTXuVSntVa26pTyZ734hbI+snljrtTaaqxP830jPbYs+qF57IR87Nv//q2yPmokUMcrRvJ1RaeFJ0ZqbaOiE327xrisNHTKbbmu17tojJ3QSNCujemU5QUj4f7siy+Q9Uc/7Yl6O4Phk2aro1Pv62U9viequl7r6RPyJ/7jv2T9+H33y/q0kYIeGQn0ra5O4U+Nc2/VSEevVvU+NG4WENRivQ9Hy/q1IzCSwcfX633ejob7s2KcX6e2btH19etlfdLYh71Mj7NuoDthqW+koMtqEKwYd0ao1/Xzqhrn2P6Cfo2vG+eoFeO8c+TgvbI+NaaP87TTOu07O4RGLHirb5wvR/S47Pd1bzaqepxlxtvxMMiXXm7+wXgDF5WtPxive8ZdLzrGcdg2+i2u635bNzkxVEs6etyExjaWjGO81+t9Q64/cnUTqxgLAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABQkzLLMykAEAAAAAABfB650AwAAAABQECbdAAAAAAAUhEk3AAAAAAAFYdINAAAAAEBBmHQDAAAAAFAQJt0AAAAAABSESTcAAAAAAAVh0g0AAAAAQEGYdAMAAAAAEBTj/wexbX3wXgIxYwAAAABJRU5ErkJggg==",
                        "text/plain": [
                            "<Figure size 1000x1500 with 5 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Visualise on Test Data\n",
                "test_df = pd.read_csv(config['data']['test_csv'])\n",
                "test_dir = config['data']['test_dir']\n",
                "\n",
                "num_samples = 5\n",
                "\n",
                "fig, axes = plt.subplots(num_samples, 1, figsize=(10, 3*num_samples))\n",
                "\n",
                "for i in range(num_samples):\n",
                "    idx = torch.randint(0, len(test_df), (1,)).item()\n",
                "    row = test_df.iloc[idx]\n",
                "    img_name = row[0]\n",
                "    gt = row[1]\n",
                "    \n",
                "    img_path = os.path.join(test_dir, img_name)\n",
                "    \n",
                "    try:\n",
                "        pred, image = predict(model, img_path, converter, DEVICE)\n",
                "        \n",
                "        axes[i].imshow(image)\n",
                "        axes[i].set_title(f\"GT: {gt} | Pred: {pred}\")\n",
                "        axes[i].axis('off')\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {img_name}: {e}\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/jt/_vhzybnn7dg3qwfs2vtf58q40000gn/T/ipykernel_11003/3680753580.py:10: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
                        "  torch.onnx.export(\n",
                        "W1229 15:49:14.819000 11003 site-packages/torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 14 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[torch.onnx] Obtain model graph for `OCRModel([...]` with `torch.export.export(..., strict=False)`...\n",
                        "[torch.onnx] Obtain model graph for `OCRModel([...]` with `torch.export.export(..., strict=False)`... \n",
                        "[torch.onnx] Run decomposition...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 14).\n",
                        "Failed to convert the model to the target version 14 using the ONNX C API. The model was not modified\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
                        "    converted_proto = _c_api_utils.call_onnx_api(\n",
                        "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
                        "    result = func(proto)\n",
                        "             ^^^^^^^^^^^\n",
                        "  File \"/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
                        "    return onnx.version_converter.convert_version(\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
                        "    converted_model_str = C.convert_version(model_str, target_version)\n",
                        "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "RuntimeError: /Users/runner/work/onnx/onnx/onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n",
                        "Skipping constant folding for op Split with multiple outputs.\n",
                        "Skipping constant folding for op Split with multiple outputs.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[torch.onnx] Run decomposition... \n",
                        "[torch.onnx] Translate the graph into ONNX...\n",
                        "[torch.onnx] Translate the graph into ONNX... \n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Skipping constant folding for op Split with multiple outputs.\n",
                        "Skipping constant folding for op Split with multiple outputs.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Applied 77 of general pattern rewrite rules.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ONNXProgram(\n",
                            "    model=\n",
                            "        <\n",
                            "            ir_version=10,\n",
                            "            opset_imports={'': 18},\n",
                            "            producer_name='pytorch',\n",
                            "            producer_version='2.9.0',\n",
                            "            domain=None,\n",
                            "            model_version=None,\n",
                            "        >\n",
                            "        graph(\n",
                            "            name=main_graph,\n",
                            "            inputs=(\n",
                            "                %\"images\"<FLOAT,[s34,3,64,256]>,\n",
                            "                %\"tgt\"<INT64,[s34,s59]>\n",
                            "            ),\n",
                            "            outputs=(\n",
                            "                %\"output\"<FLOAT,[1,s59,40]>\n",
                            "            ),\n",
                            "            initializers=(\n",
                            "                %\"adapter.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.self_attn.in_proj_bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.linear2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.self_attn.in_proj_bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.linear2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.self_attn.in_proj_bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.multihead_attn.in_proj_bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.multihead_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.linear2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.norm3.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.norm3.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.self_attn.in_proj_bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.multihead_attn.in_proj_bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.multihead_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.linear2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.norm3.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.norm3.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
                            "                %\"fc_out.bias\"<FLOAT,[40]>{TorchTensor(...)},\n",
                            "                %\"feature_extractor.backbone.0.weight\"<FLOAT,[64,3,7,7]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.4.0.conv1.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.4.0.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.4.1.conv1.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.4.1.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.0.conv1.weight\"<FLOAT,[128,64,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.0.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.0.downsample.0.weight\"<FLOAT,[128,64,1,1]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.1.conv1.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.1.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.0.conv1.weight\"<FLOAT,[256,128,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.0.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.0.downsample.0.weight\"<FLOAT,[256,128,1,1]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.1.conv1.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.1.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.0.conv1.weight\"<FLOAT,[512,256,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.0.conv2.weight\"<FLOAT,[512,512,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.0.downsample.0.weight\"<FLOAT,[512,256,1,1]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.1.conv1.weight\"<FLOAT,[512,512,3,3]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.1.conv2.weight\"<FLOAT,[512,512,3,3]>{Tensor(...)},\n",
                            "                %\"adapter.weight\"<FLOAT,[256,512,1,1]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.self_attn.out_proj.weight\"<FLOAT,[256,256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.0.linear1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.self_attn.out_proj.weight\"<FLOAT,[256,256]>{TorchTensor(...)},\n",
                            "                %\"transformer.encoder.layers.1.linear1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.self_attn.out_proj.weight\"<FLOAT,[256,256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.multihead_attn.in_proj_weight\"<FLOAT,[768,256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.multihead_attn.out_proj.weight\"<FLOAT,[256,256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.0.linear1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.self_attn.out_proj.weight\"<FLOAT,[256,256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.multihead_attn.in_proj_weight\"<FLOAT,[768,256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.multihead_attn.out_proj.weight\"<FLOAT,[256,256]>{TorchTensor(...)},\n",
                            "                %\"transformer.decoder.layers.1.linear1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
                            "                %\"embedding.weight\"<FLOAT,[40,256]>{TorchTensor(...)},\n",
                            "                %\"pos_encoder.pe\"<FLOAT,[32,1,256]>{TorchTensor(...)},\n",
                            "                %\"val_188\"<INT64,[1]>{Tensor<INT64,[1]>(array([2]), name='val_188')},\n",
                            "                %\"slice_1\"<FLOAT,[8,1,256]>{Tensor(...)},\n",
                            "                %\"val_207\"<INT64,[1]>{Tensor<INT64,[1]>(array([0]), name='val_207')},\n",
                            "                %\"val_209\"<FLOAT,[256,768]>{Tensor(...)},\n",
                            "                %\"val_216\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   1,   3, 256]), name='val_216')},\n",
                            "                %\"val_259\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name='val_259')},\n",
                            "                %\"val_294\"<FLOAT,[256,1024]>{Tensor(...)},\n",
                            "                %\"val_296\"<FLOAT,[1024,256]>{Tensor(...)},\n",
                            "                %\"val_300\"<FLOAT,[256,768]>{Tensor(...)},\n",
                            "                %\"val_312\"<INT64,[3]>{Tensor<INT64,[3]>(array([ 8,  4, 64]), name='val_312')},\n",
                            "                %\"val_328\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 1,  4,  8, 64]), name='val_328')},\n",
                            "                %\"val_360\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 1,  4, 64,  8]), name='val_360')},\n",
                            "                %\"val_362\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.35355338], dtype=float32), name='val_362')},\n",
                            "                %\"val_372\"<INT64,[2]>{Tensor<INT64,[2]>(array([  8, 256]), name='val_372')},\n",
                            "                %\"val_377\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8,   1, 256]), name='val_377')},\n",
                            "                %\"val_380\"<FLOAT,[256,1024]>{Tensor(...)},\n",
                            "                %\"val_382\"<FLOAT,[1024,256]>{Tensor(...)},\n",
                            "                %\"val_388\"<FLOAT,[256,768]>{Tensor(...)},\n",
                            "                %\"val_478\"<INT64,[4]>{Tensor<INT64,[4]>(array([  8,   1,   2, 256]), name='val_478')},\n",
                            "                %\"val_551\"<FLOAT,[256,1024]>{Tensor(...)},\n",
                            "                %\"val_553\"<FLOAT,[1024,256]>{Tensor(...)},\n",
                            "                %\"val_557\"<FLOAT,[256,768]>{Tensor(...)},\n",
                            "                %\"val_719\"<FLOAT,[256,1024]>{Tensor(...)},\n",
                            "                %\"val_721\"<FLOAT,[1024,256]>{Tensor(...)},\n",
                            "                %\"val_727\"<FLOAT,[256,40]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.0.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.4.0.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.4.0.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.4.1.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.4.1.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.0.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.0.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.0.downsample.0.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.1.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.5.1.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.0.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.0.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.0.downsample.0.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.1.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.6.1.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.0.conv1.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.0.conv2.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.0.downsample.0.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.1.conv1.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
                            "                %\"feature_extractor.backbone.7.1.conv2.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
                            "                %\"val_187\"<INT64,[]>{Tensor<INT64,[]>(array(2), name='val_187')},\n",
                            "                %\"val_189\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_189')},\n",
                            "                %\"val_193\"<INT64,[]>{Tensor<INT64,[]>(array(8), name='val_193')},\n",
                            "                %\"val_208\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_208')},\n",
                            "                %\"val_218\"<INT64,[1]>{Tensor<INT64,[1]>(array([-2]), name='val_218')},\n",
                            "                %\"val_219\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='val_219')},\n",
                            "                %\"val_220\"<INT64,[]>{Tensor<INT64,[]>(array(4), name='val_220')},\n",
                            "                %\"val_221\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_221')},\n",
                            "                %\"val_222\"<INT64,[1]>{Tensor<INT64,[1]>(array([8]), name='val_222')},\n",
                            "                %\"val_224\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_224')},\n",
                            "                %\"val_238\"<INT64,[1]>{Tensor<INT64,[1]>(array([4]), name='val_238')},\n",
                            "                %\"val_264\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_264')},\n",
                            "                %\"val_268\"<INT64,[1]>{Tensor<INT64,[1]>(array([-9223372036854775808]), name='val_268')},\n",
                            "                %\"val_285\"<INT64,[1]>{Tensor<INT64,[1]>(array([256]), name='val_285')},\n",
                            "                %\"val_393\"<INT64,[1]>{Tensor<INT64,[1]>(array([3]), name='val_393')},\n",
                            "                %\"val_468\"<INT64,[2]>{Tensor<INT64,[2]>(array([256, 512]), name='val_468')}\n",
                            "            ),\n",
                            "        ) {\n",
                            "              0 |  # node_Shape_0\n",
                            "                   %\"val_0\"<INT64,[1]>  ::Shape(%\"images\") {end=1, start=0}\n",
                            "              1 |  # node_sym_size_int_14\n",
                            "                   %\"sym_size_int_14\"<INT64,[]>  ::Squeeze(%\"val_0\")\n",
                            "              2 |  # node_Shape_1\n",
                            "                   %\"val_1\"<INT64,[1]>  ::Shape(%\"tgt\") {end=2, start=1}\n",
                            "              3 |  # node_sym_size_int_16\n",
                            "                   %\"sym_size_int_16\"<INT64,[]>  ::Squeeze(%\"val_1\")\n",
                            "              4 |  # node_Conv_818\n",
                            "                   %\"getitem\"<FLOAT,[1,64,32,128]>  ::Conv(%\"images\", %\"feature_extractor.backbone.0.weight\"{...}, %\"feature_extractor.backbone.0.weight_bias\"{...}) {group=1, pads=(3, 3, 3, 3), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "              5 |  # node_relu\n",
                            "                   %\"relu\"<FLOAT,[1,64,32,128]>  ::Relu(%\"getitem\")\n",
                            "              6 |  # node_max_pool2d\n",
                            "                   %\"max_pool2d\"<FLOAT,[1,64,16,64]>  ::MaxPool(%\"relu\") {storage_order=0, ceil_mode=0, pads=(1, 1, 1, 1), kernel_shape=(3, 3), strides=(2, 2), dilations=(1, 1), auto_pad='NOTSET'}\n",
                            "              7 |  # node_Conv_820\n",
                            "                   %\"getitem_3\"<FLOAT,[1,64,16,64]>  ::Conv(%\"max_pool2d\", %\"feature_extractor.backbone.4.0.conv1.weight\"{...}, %\"feature_extractor.backbone.4.0.conv1.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "              8 |  # node_relu_1\n",
                            "                   %\"relu_1\"<FLOAT,[1,64,16,64]>  ::Relu(%\"getitem_3\")\n",
                            "              9 |  # node_Conv_822\n",
                            "                   %\"getitem_6\"<FLOAT,[1,64,16,64]>  ::Conv(%\"relu_1\", %\"feature_extractor.backbone.4.0.conv2.weight\"{...}, %\"feature_extractor.backbone.4.0.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             10 |  # node_add_5\n",
                            "                   %\"add_5\"<FLOAT,[1,64,16,64]>  ::Add(%\"getitem_6\", %\"max_pool2d\")\n",
                            "             11 |  # node_relu_2\n",
                            "                   %\"relu_2\"<FLOAT,[1,64,16,64]>  ::Relu(%\"add_5\")\n",
                            "             12 |  # node_Conv_824\n",
                            "                   %\"getitem_9\"<FLOAT,[1,64,16,64]>  ::Conv(%\"relu_2\", %\"feature_extractor.backbone.4.1.conv1.weight\"{...}, %\"feature_extractor.backbone.4.1.conv1.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             13 |  # node_relu_3\n",
                            "                   %\"relu_3\"<FLOAT,[1,64,16,64]>  ::Relu(%\"getitem_9\")\n",
                            "             14 |  # node_Conv_826\n",
                            "                   %\"getitem_12\"<FLOAT,[1,64,16,64]>  ::Conv(%\"relu_3\", %\"feature_extractor.backbone.4.1.conv2.weight\"{...}, %\"feature_extractor.backbone.4.1.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             15 |  # node_add_6\n",
                            "                   %\"add_6\"<FLOAT,[1,64,16,64]>  ::Add(%\"getitem_12\", %\"relu_2\")\n",
                            "             16 |  # node_relu_4\n",
                            "                   %\"relu_4\"<FLOAT,[1,64,16,64]>  ::Relu(%\"add_6\")\n",
                            "             17 |  # node_Conv_828\n",
                            "                   %\"getitem_15\"<FLOAT,[1,128,8,32]>  ::Conv(%\"relu_4\", %\"feature_extractor.backbone.5.0.conv1.weight\"{...}, %\"feature_extractor.backbone.5.0.conv1.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             18 |  # node_relu_5\n",
                            "                   %\"relu_5\"<FLOAT,[1,128,8,32]>  ::Relu(%\"getitem_15\")\n",
                            "             19 |  # node_Conv_830\n",
                            "                   %\"getitem_18\"<FLOAT,[1,128,8,32]>  ::Conv(%\"relu_5\", %\"feature_extractor.backbone.5.0.conv2.weight\"{...}, %\"feature_extractor.backbone.5.0.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             20 |  # node_Conv_832\n",
                            "                   %\"getitem_21\"<FLOAT,[1,128,8,32]>  ::Conv(%\"relu_4\", %\"feature_extractor.backbone.5.0.downsample.0.weight\"{...}, %\"feature_extractor.backbone.5.0.downsample.0.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             21 |  # node_add_7\n",
                            "                   %\"add_7\"<FLOAT,[1,128,8,32]>  ::Add(%\"getitem_18\", %\"getitem_21\")\n",
                            "             22 |  # node_relu_6\n",
                            "                   %\"relu_6\"<FLOAT,[1,128,8,32]>  ::Relu(%\"add_7\")\n",
                            "             23 |  # node_Conv_834\n",
                            "                   %\"getitem_24\"<FLOAT,[1,128,8,32]>  ::Conv(%\"relu_6\", %\"feature_extractor.backbone.5.1.conv1.weight\"{...}, %\"feature_extractor.backbone.5.1.conv1.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             24 |  # node_relu_7\n",
                            "                   %\"relu_7\"<FLOAT,[1,128,8,32]>  ::Relu(%\"getitem_24\")\n",
                            "             25 |  # node_Conv_836\n",
                            "                   %\"getitem_27\"<FLOAT,[1,128,8,32]>  ::Conv(%\"relu_7\", %\"feature_extractor.backbone.5.1.conv2.weight\"{...}, %\"feature_extractor.backbone.5.1.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             26 |  # node_add_8\n",
                            "                   %\"add_8\"<FLOAT,[1,128,8,32]>  ::Add(%\"getitem_27\", %\"relu_6\")\n",
                            "             27 |  # node_relu_8\n",
                            "                   %\"relu_8\"<FLOAT,[1,128,8,32]>  ::Relu(%\"add_8\")\n",
                            "             28 |  # node_Conv_838\n",
                            "                   %\"getitem_30\"<FLOAT,[1,256,4,16]>  ::Conv(%\"relu_8\", %\"feature_extractor.backbone.6.0.conv1.weight\"{...}, %\"feature_extractor.backbone.6.0.conv1.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             29 |  # node_relu_9\n",
                            "                   %\"relu_9\"<FLOAT,[1,256,4,16]>  ::Relu(%\"getitem_30\")\n",
                            "             30 |  # node_Conv_840\n",
                            "                   %\"getitem_33\"<FLOAT,[1,256,4,16]>  ::Conv(%\"relu_9\", %\"feature_extractor.backbone.6.0.conv2.weight\"{...}, %\"feature_extractor.backbone.6.0.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             31 |  # node_Conv_842\n",
                            "                   %\"getitem_36\"<FLOAT,[1,256,4,16]>  ::Conv(%\"relu_8\", %\"feature_extractor.backbone.6.0.downsample.0.weight\"{...}, %\"feature_extractor.backbone.6.0.downsample.0.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             32 |  # node_add_9\n",
                            "                   %\"add_9\"<FLOAT,[1,256,4,16]>  ::Add(%\"getitem_33\", %\"getitem_36\")\n",
                            "             33 |  # node_relu_10\n",
                            "                   %\"relu_10\"<FLOAT,[1,256,4,16]>  ::Relu(%\"add_9\")\n",
                            "             34 |  # node_Conv_844\n",
                            "                   %\"getitem_39\"<FLOAT,[1,256,4,16]>  ::Conv(%\"relu_10\", %\"feature_extractor.backbone.6.1.conv1.weight\"{...}, %\"feature_extractor.backbone.6.1.conv1.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             35 |  # node_relu_11\n",
                            "                   %\"relu_11\"<FLOAT,[1,256,4,16]>  ::Relu(%\"getitem_39\")\n",
                            "             36 |  # node_Conv_846\n",
                            "                   %\"getitem_42\"<FLOAT,[1,256,4,16]>  ::Conv(%\"relu_11\", %\"feature_extractor.backbone.6.1.conv2.weight\"{...}, %\"feature_extractor.backbone.6.1.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             37 |  # node_add_10\n",
                            "                   %\"add_10\"<FLOAT,[1,256,4,16]>  ::Add(%\"getitem_42\", %\"relu_10\")\n",
                            "             38 |  # node_relu_12\n",
                            "                   %\"relu_12\"<FLOAT,[1,256,4,16]>  ::Relu(%\"add_10\")\n",
                            "             39 |  # node_Conv_848\n",
                            "                   %\"getitem_45\"<FLOAT,[1,512,2,8]>  ::Conv(%\"relu_12\", %\"feature_extractor.backbone.7.0.conv1.weight\"{...}, %\"feature_extractor.backbone.7.0.conv1.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             40 |  # node_relu_13\n",
                            "                   %\"relu_13\"<FLOAT,[1,512,2,8]>  ::Relu(%\"getitem_45\")\n",
                            "             41 |  # node_Conv_850\n",
                            "                   %\"getitem_48\"<FLOAT,[1,512,2,8]>  ::Conv(%\"relu_13\", %\"feature_extractor.backbone.7.0.conv2.weight\"{...}, %\"feature_extractor.backbone.7.0.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             42 |  # node_Conv_852\n",
                            "                   %\"getitem_51\"<FLOAT,[1,512,2,8]>  ::Conv(%\"relu_12\", %\"feature_extractor.backbone.7.0.downsample.0.weight\"{...}, %\"feature_extractor.backbone.7.0.downsample.0.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             43 |  # node_add_11\n",
                            "                   %\"add_11\"<FLOAT,[1,512,2,8]>  ::Add(%\"getitem_48\", %\"getitem_51\")\n",
                            "             44 |  # node_relu_14\n",
                            "                   %\"relu_14\"<FLOAT,[1,512,2,8]>  ::Relu(%\"add_11\")\n",
                            "             45 |  # node_Conv_854\n",
                            "                   %\"getitem_54\"<FLOAT,[1,512,2,8]>  ::Conv(%\"relu_14\", %\"feature_extractor.backbone.7.1.conv1.weight\"{...}, %\"feature_extractor.backbone.7.1.conv1.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             46 |  # node_relu_15\n",
                            "                   %\"relu_15\"<FLOAT,[1,512,2,8]>  ::Relu(%\"getitem_54\")\n",
                            "             47 |  # node_Conv_856\n",
                            "                   %\"getitem_57\"<FLOAT,[1,512,2,8]>  ::Conv(%\"relu_15\", %\"feature_extractor.backbone.7.1.conv2.weight\"{...}, %\"feature_extractor.backbone.7.1.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             48 |  # node_add_12\n",
                            "                   %\"add_12\"<FLOAT,[1,512,2,8]>  ::Add(%\"getitem_57\", %\"relu_14\")\n",
                            "             49 |  # node_relu_16\n",
                            "                   %\"relu_16\"<FLOAT,[1,512,2,8]>  ::Relu(%\"add_12\")\n",
                            "             50 |  # node_conv2d_20\n",
                            "                   %\"conv2d_20\"<FLOAT,[1,256,2,8]>  ::Conv(%\"relu_16\", %\"adapter.weight\"{...}, %\"adapter.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
                            "             51 |  # node_max_1__0\n",
                            "                   %\"getitem_60\"<FLOAT,[1,256,8]>  ::ReduceMax(%\"conv2d_20\", %\"val_188\"{[2]}) {noop_with_empty_axes=0, keepdims=0}\n",
                            "             52 |  # node_permute\n",
                            "                   %\"permute\"<FLOAT,[8,1,256]>  ::Transpose(%\"getitem_60\") {perm=(2, 0, 1)}\n",
                            "             53 |  # node_add_13\n",
                            "                   %\"add_13\"<FLOAT,[8,1,256]>  ::Add(%\"permute\", %\"slice_1\"{...})\n",
                            "             54 |  # node_embedding\n",
                            "                   %\"embedding\"<FLOAT,[s34,s59,256]>  ::Gather(%\"embedding.weight\"{...}, %\"tgt\") {axis=0}\n",
                            "             55 |  # node_permute_1\n",
                            "                   %\"permute_1\"<FLOAT,[s59,s34,256]>  ::Transpose(%\"embedding\") {perm=(1, 0, 2)}\n",
                            "             56 |  # node_slice_2\n",
                            "                   %\"slice_2\"<FLOAT,[s59,1,256]>  ::Slice(%\"pos_encoder.pe\"{...}, %\"val_207\"{[0]}, %\"val_1\", %\"val_207\"{[0]}, %\"val_208\"{[1]})\n",
                            "             57 |  # node_add_26\n",
                            "                   %\"add_26\"<FLOAT,[s59,1,256]>  ::Add(%\"permute_1\", %\"slice_2\")\n",
                            "             58 |  # node_MatMul_209\n",
                            "                   %\"val_210\"<FLOAT,[8,1,768]>  ::MatMul(%\"add_13\", %\"val_209\"{...})\n",
                            "             59 |  # node_linear\n",
                            "                   %\"linear\"<FLOAT,[8,1,768]>  ::Add(%\"val_210\", %\"transformer.encoder.layers.0.self_attn.in_proj_bias\"{...})\n",
                            "             60 |  # node_view\n",
                            "                   %\"view\"<FLOAT,[8,1,3,256]>  ::Reshape(%\"linear\", %\"val_216\"{[8, 1, 3, 256]}) {allowzero=1}\n",
                            "             61 |  # node_unsqueeze\n",
                            "                   %\"unsqueeze\"<FLOAT,[1,8,1,3,256]>  ::Unsqueeze(%\"view\", %\"val_207\"{[0]})\n",
                            "             62 |  # node_transpose\n",
                            "                   %\"transpose\"<FLOAT,[3,8,1,1,256]>  ::Transpose(%\"unsqueeze\") {perm=(3, 1, 2, 0, 4)}\n",
                            "             63 |  # node_squeeze\n",
                            "                   %\"squeeze\"<FLOAT,[3,8,1,256]>  ::Squeeze(%\"transpose\", %\"val_218\"{[-2]})\n",
                            "             64 |  # node_select\n",
                            "                   %\"select\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze\", %\"val_189\"{0}) {axis=0}\n",
                            "             65 |  # node_select_1\n",
                            "                   %\"select_1\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze\", %\"val_219\"{1}) {axis=0}\n",
                            "             66 |  # node_select_2\n",
                            "                   %\"select_2\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze\", %\"val_187\"{2}) {axis=0}\n",
                            "             67 |  # node_mul_17\n",
                            "                   %\"mul_17\"<INT64,[]>  ::Mul(%\"sym_size_int_14\", %\"val_220\"{4})\n",
                            "             68 |  # node_Reshape_222\n",
                            "                   %\"val_223\"<INT64,[1]>  ::Reshape(%\"mul_17\", %\"val_221\"{[-1]}) {allowzero=0}\n",
                            "             69 |  # node_Concat_224\n",
                            "                   %\"val_225\"<INT64,[3]>  ::Concat(%\"val_222\"{[8]}, %\"val_223\", %\"val_224\"{[64]}) {axis=0}\n",
                            "             70 |  # node_view_1\n",
                            "                   %\"view_1\"<FLOAT,[8,4*s34,(64//s34)]>  ::Reshape(%\"select\", %\"val_225\") {allowzero=1}\n",
                            "             71 |  # node_transpose_1\n",
                            "                   %\"transpose_1\"<FLOAT,[4*s34,8,(64//s34)]>  ::Transpose(%\"view_1\") {perm=(1, 0, 2)}\n",
                            "             72 |  # node_view_2\n",
                            "                   %\"view_2\"<FLOAT,[8,4*s34,(64//s34)]>  ::Reshape(%\"select_1\", %\"val_225\") {allowzero=1}\n",
                            "             73 |  # node_transpose_2\n",
                            "                   %\"transpose_2\"<FLOAT,[4*s34,8,(64//s34)]>  ::Transpose(%\"view_2\") {perm=(1, 0, 2)}\n",
                            "             74 |  # node_view_3\n",
                            "                   %\"view_3\"<FLOAT,[8,4*s34,(64//s34)]>  ::Reshape(%\"select_2\", %\"val_225\") {allowzero=1}\n",
                            "             75 |  # node_transpose_3\n",
                            "                   %\"transpose_3\"<FLOAT,[4*s34,8,(64//s34)]>  ::Transpose(%\"view_3\") {perm=(1, 0, 2)}\n",
                            "             76 |  # node_Concat_240\n",
                            "                   %\"val_241\"<INT64,[4]>  ::Concat(%\"val_0\", %\"val_238\"{[4]}, %\"val_222\"{[8]}, %\"val_224\"{[64]}) {axis=0}\n",
                            "             77 |  # node_view_4\n",
                            "                   %\"view_4\"<FLOAT,[s34,4,8,(64//s34)]>  ::Reshape(%\"transpose_1\", %\"val_241\") {allowzero=1}\n",
                            "             78 |  # node_view_5\n",
                            "                   %\"view_5\"<FLOAT,[s34,4,8,(64//s34)]>  ::Reshape(%\"transpose_2\", %\"val_241\") {allowzero=1}\n",
                            "             79 |  # node_view_6\n",
                            "                   %\"view_6\"<FLOAT,[s34,4,8,(64//s34)]>  ::Reshape(%\"transpose_3\", %\"val_241\") {allowzero=1}\n",
                            "             80 |  # node_Shape_253\n",
                            "                   %\"val_254\"<INT64,[4]>  ::Shape(%\"view_4\") {start=0}\n",
                            "             81 |  # node_Gather_255\n",
                            "                   %\"val_256\"<INT64,[1]>  ::Gather(%\"val_254\", %\"val_221\"{[-1]}) {axis=0}\n",
                            "             82 |  # node_Cast_769\n",
                            "                   %\"val_257\"<FLOAT,[1]>  ::Cast(%\"val_256\") {to=1}\n",
                            "             83 |  # node_Sqrt_259\n",
                            "                   %\"val_260\"<FLOAT,[1]>  ::Sqrt(%\"val_257\")\n",
                            "             84 |  # node_Div_260\n",
                            "                   %\"val_261\"<FLOAT,[1]>  ::Div(%\"val_259\"{1.0}, %\"val_260\")\n",
                            "             85 |  # node_Shape_262\n",
                            "                   %\"val_263\"<INT64,[4]>  ::Shape(%\"view_5\") {start=0}\n",
                            "             86 |  # node_Slice_265\n",
                            "                   %\"val_266\"<INT64,[1]>  ::Slice(%\"val_263\", %\"val_221\"{[-1]}, %\"val_264\"{[9223372036854775807]})\n",
                            "             87 |  # node_Slice_266\n",
                            "                   %\"val_267\"<INT64,[1]>  ::Slice(%\"val_263\", %\"val_218\"{[-2]}, %\"val_221\"{[-1]})\n",
                            "             88 |  # node_Slice_268\n",
                            "                   %\"val_269\"<INT64,[2]>  ::Slice(%\"val_263\", %\"val_268\"{[-9223372036854775808]}, %\"val_218\"{[-2]})\n",
                            "             89 |  # node_Concat_270\n",
                            "                   %\"val_271\"<INT64,[3]>  ::Concat(%\"val_221\"{[-1]}, %\"val_267\", %\"val_266\") {axis=0}\n",
                            "             90 |  # node_Reshape_271\n",
                            "                   %\"val_272\"<FLOAT,[None,None,None]>  ::Reshape(%\"view_5\", %\"val_271\") {allowzero=0}\n",
                            "             91 |  # node_Transpose_272\n",
                            "                   %\"val_273\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_272\") {perm=(0, 2, 1)}\n",
                            "             92 |  # node_Concat_273\n",
                            "                   %\"val_274\"<INT64,[4]>  ::Concat(%\"val_269\", %\"val_266\", %\"val_267\") {axis=0}\n",
                            "             93 |  # node_Reshape_274\n",
                            "                   %\"val_275\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_273\", %\"val_274\") {allowzero=0}\n",
                            "             94 |  # node_Sqrt_275\n",
                            "                   %\"val_276\"<FLOAT,[1]>  ::Sqrt(%\"val_261\")\n",
                            "             95 |  # node_Mul_276\n",
                            "                   %\"val_277\"<FLOAT,[s34,4,8,(64//s34)]>  ::Mul(%\"view_4\", %\"val_276\")\n",
                            "             96 |  # node_Mul_279\n",
                            "                   %\"val_280\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_275\", %\"val_276\")\n",
                            "             97 |  # node_MatMul_280\n",
                            "                   %\"val_281\"<FLOAT,[None,4,8,None]>  ::MatMul(%\"val_277\", %\"val_280\")\n",
                            "             98 |  # node_Softmax_281\n",
                            "                   %\"val_282\"<FLOAT,[None,4,8,None]>  ::Softmax(%\"val_281\") {axis=-1}\n",
                            "             99 |  # node_scaled_dot_product_attention\n",
                            "                   %\"scaled_dot_product_attention\"<FLOAT,[s34,4,8,(64//s34)]>  ::MatMul(%\"val_282\", %\"view_6\")\n",
                            "            100 |  # node_permute_2\n",
                            "                   %\"permute_2\"<FLOAT,[8,s34,4,(64//s34)]>  ::Transpose(%\"scaled_dot_product_attention\") {perm=(2, 0, 1, 3)}\n",
                            "            101 |  # node_mul_47\n",
                            "                   %\"mul_47\"<INT64,[]>  ::Mul(%\"sym_size_int_14\", %\"val_193\"{8})\n",
                            "            102 |  # node_Reshape_283\n",
                            "                   %\"val_284\"<INT64,[1]>  ::Reshape(%\"mul_47\", %\"val_221\"{[-1]}) {allowzero=0}\n",
                            "            103 |  # node_Concat_285\n",
                            "                   %\"val_286\"<INT64,[2]>  ::Concat(%\"val_284\", %\"val_285\"{[256]}) {axis=0}\n",
                            "            104 |  # node_view_7\n",
                            "                   %\"view_7\"<FLOAT,[8,4*((64//s34))]>  ::Reshape(%\"permute_2\", %\"val_286\") {allowzero=1}\n",
                            "            105 |  # node_linear_1\n",
                            "                   %\"linear_1\"<FLOAT,[8,256]>  ::Gemm(%\"view_7\", %\"transformer.encoder.layers.0.self_attn.out_proj.weight\"{...}, %\"transformer.encoder.layers.0.self_attn.out_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
                            "            106 |  # node_Concat_290\n",
                            "                   %\"val_291\"<INT64,[3]>  ::Concat(%\"val_222\"{[8]}, %\"val_0\", %\"val_285\"{[256]}) {axis=0}\n",
                            "            107 |  # node_view_8\n",
                            "                   %\"view_8\"<FLOAT,[8,s34,(256//s34)]>  ::Reshape(%\"linear_1\", %\"val_291\") {allowzero=1}\n",
                            "            108 |  # node_add_91\n",
                            "                   %\"add_91\"<FLOAT,[8,1,256]>  ::Add(%\"add_13\", %\"view_8\")\n",
                            "            109 |  # node_layer_norm\n",
                            "                   %\"layer_norm\"<FLOAT,[8,1,256]>  ::LayerNormalization(%\"add_91\", %\"transformer.encoder.layers.0.norm1.weight\"{...}, %\"transformer.encoder.layers.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            110 |  # node_MatMul_292\n",
                            "                   %\"val_295\"<FLOAT,[8,1,1024]>  ::MatMul(%\"layer_norm\", %\"val_294\"{...})\n",
                            "            111 |  # node_linear_2\n",
                            "                   %\"linear_2\"<FLOAT,[8,1,1024]>  ::Add(%\"val_295\", %\"transformer.encoder.layers.0.linear1.bias\"{...})\n",
                            "            112 |  # node_relu_17\n",
                            "                   %\"relu_17\"<FLOAT,[8,1,1024]>  ::Relu(%\"linear_2\")\n",
                            "            113 |  # node_MatMul_294\n",
                            "                   %\"val_297\"<FLOAT,[8,1,256]>  ::MatMul(%\"relu_17\", %\"val_296\"{...})\n",
                            "            114 |  # node_linear_3\n",
                            "                   %\"linear_3\"<FLOAT,[8,1,256]>  ::Add(%\"val_297\", %\"transformer.encoder.layers.0.linear2.bias\"{...})\n",
                            "            115 |  # node_add_92\n",
                            "                   %\"add_92\"<FLOAT,[8,1,256]>  ::Add(%\"layer_norm\", %\"linear_3\")\n",
                            "            116 |  # node_layer_norm_1\n",
                            "                   %\"layer_norm_1\"<FLOAT,[8,1,256]>  ::LayerNormalization(%\"add_92\", %\"transformer.encoder.layers.0.norm2.weight\"{...}, %\"transformer.encoder.layers.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            117 |  # node_MatMul_296\n",
                            "                   %\"val_301\"<FLOAT,[8,1,768]>  ::MatMul(%\"layer_norm_1\", %\"val_300\"{...})\n",
                            "            118 |  # node_linear_4\n",
                            "                   %\"linear_4\"<FLOAT,[8,1,768]>  ::Add(%\"val_301\", %\"transformer.encoder.layers.1.self_attn.in_proj_bias\"{...})\n",
                            "            119 |  # node_view_9\n",
                            "                   %\"view_9\"<FLOAT,[8,1,3,256]>  ::Reshape(%\"linear_4\", %\"val_216\"{[8, 1, 3, 256]}) {allowzero=1}\n",
                            "            120 |  # node_unsqueeze_1\n",
                            "                   %\"unsqueeze_1\"<FLOAT,[1,8,1,3,256]>  ::Unsqueeze(%\"view_9\", %\"val_207\"{[0]})\n",
                            "            121 |  # node_transpose_4\n",
                            "                   %\"transpose_4\"<FLOAT,[3,8,1,1,256]>  ::Transpose(%\"unsqueeze_1\") {perm=(3, 1, 2, 0, 4)}\n",
                            "            122 |  # node_squeeze_1\n",
                            "                   %\"squeeze_1\"<FLOAT,[3,8,1,256]>  ::Squeeze(%\"transpose_4\", %\"val_218\"{[-2]})\n",
                            "            123 |  # node_select_3\n",
                            "                   %\"select_3\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze_1\", %\"val_189\"{0}) {axis=0}\n",
                            "            124 |  # node_select_4\n",
                            "                   %\"select_4\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze_1\", %\"val_219\"{1}) {axis=0}\n",
                            "            125 |  # node_select_5\n",
                            "                   %\"select_5\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze_1\", %\"val_187\"{2}) {axis=0}\n",
                            "            126 |  # node_view_10\n",
                            "                   %\"view_10\"<FLOAT,[8,4,64]>  ::Reshape(%\"select_3\", %\"val_312\"{[8, 4, 64]}) {allowzero=1}\n",
                            "            127 |  # node_transpose_5\n",
                            "                   %\"transpose_5\"<FLOAT,[4,8,64]>  ::Transpose(%\"view_10\") {perm=(1, 0, 2)}\n",
                            "            128 |  # node_view_11\n",
                            "                   %\"view_11\"<FLOAT,[8,4,64]>  ::Reshape(%\"select_4\", %\"val_312\"{[8, 4, 64]}) {allowzero=1}\n",
                            "            129 |  # node_transpose_6\n",
                            "                   %\"transpose_6\"<FLOAT,[4,8,64]>  ::Transpose(%\"view_11\") {perm=(1, 0, 2)}\n",
                            "            130 |  # node_view_12\n",
                            "                   %\"view_12\"<FLOAT,[8,4,64]>  ::Reshape(%\"select_5\", %\"val_312\"{[8, 4, 64]}) {allowzero=1}\n",
                            "            131 |  # node_transpose_7\n",
                            "                   %\"transpose_7\"<FLOAT,[4,8,64]>  ::Transpose(%\"view_12\") {perm=(1, 0, 2)}\n",
                            "            132 |  # node_view_13\n",
                            "                   %\"view_13\"<FLOAT,[1,4,8,64]>  ::Reshape(%\"transpose_5\", %\"val_328\"{[1, 4, 8, 64]}) {allowzero=1}\n",
                            "            133 |  # node_view_15\n",
                            "                   %\"view_15\"<FLOAT,[1,4,8,64]>  ::Reshape(%\"transpose_7\", %\"val_328\"{[1, 4, 8, 64]}) {allowzero=1}\n",
                            "            134 |  # node_Transpose_354\n",
                            "                   %\"val_359\"<FLOAT,[4,64,8]>  ::Transpose(%\"transpose_6\") {perm=(0, 2, 1)}\n",
                            "            135 |  # node_Reshape_356\n",
                            "                   %\"val_361\"<FLOAT,[1,4,64,8]>  ::Reshape(%\"val_359\", %\"val_360\"{[1, 4, 64, 8]}) {allowzero=0}\n",
                            "            136 |  # node_Mul_358\n",
                            "                   %\"val_363\"<FLOAT,[1,4,8,64]>  ::Mul(%\"view_13\", %\"val_362\"{[0.3535533845424652]})\n",
                            "            137 |  # node_Mul_361\n",
                            "                   %\"val_366\"<FLOAT,[1,4,64,8]>  ::Mul(%\"val_361\", %\"val_362\"{[0.3535533845424652]})\n",
                            "            138 |  # node_MatMul_362\n",
                            "                   %\"val_367\"<FLOAT,[1,4,8,8]>  ::MatMul(%\"val_363\", %\"val_366\")\n",
                            "            139 |  # node_Softmax_363\n",
                            "                   %\"val_368\"<FLOAT,[1,4,8,8]>  ::Softmax(%\"val_367\") {axis=-1}\n",
                            "            140 |  # node_scaled_dot_product_attention_1\n",
                            "                   %\"scaled_dot_product_attention_1\"<FLOAT,[1,4,8,64]>  ::MatMul(%\"val_368\", %\"view_15\")\n",
                            "            141 |  # node_permute_3\n",
                            "                   %\"permute_3\"<FLOAT,[8,1,4,64]>  ::Transpose(%\"scaled_dot_product_attention_1\") {perm=(2, 0, 1, 3)}\n",
                            "            142 |  # node_view_16\n",
                            "                   %\"view_16\"<FLOAT,[8,256]>  ::Reshape(%\"permute_3\", %\"val_372\"{[8, 256]}) {allowzero=1}\n",
                            "            143 |  # node_linear_5\n",
                            "                   %\"linear_5\"<FLOAT,[8,256]>  ::Gemm(%\"view_16\", %\"transformer.encoder.layers.1.self_attn.out_proj.weight\"{...}, %\"transformer.encoder.layers.1.self_attn.out_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
                            "            144 |  # node_view_17\n",
                            "                   %\"view_17\"<FLOAT,[8,1,256]>  ::Reshape(%\"linear_5\", %\"val_377\"{[8, 1, 256]}) {allowzero=1}\n",
                            "            145 |  # node_add_93\n",
                            "                   %\"add_93\"<FLOAT,[8,1,256]>  ::Add(%\"layer_norm_1\", %\"view_17\")\n",
                            "            146 |  # node_layer_norm_2\n",
                            "                   %\"layer_norm_2\"<FLOAT,[8,1,256]>  ::LayerNormalization(%\"add_93\", %\"transformer.encoder.layers.1.norm1.weight\"{...}, %\"transformer.encoder.layers.1.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            147 |  # node_MatMul_374\n",
                            "                   %\"val_381\"<FLOAT,[8,1,1024]>  ::MatMul(%\"layer_norm_2\", %\"val_380\"{...})\n",
                            "            148 |  # node_linear_6\n",
                            "                   %\"linear_6\"<FLOAT,[8,1,1024]>  ::Add(%\"val_381\", %\"transformer.encoder.layers.1.linear1.bias\"{...})\n",
                            "            149 |  # node_relu_18\n",
                            "                   %\"relu_18\"<FLOAT,[8,1,1024]>  ::Relu(%\"linear_6\")\n",
                            "            150 |  # node_MatMul_376\n",
                            "                   %\"val_383\"<FLOAT,[8,1,256]>  ::MatMul(%\"relu_18\", %\"val_382\"{...})\n",
                            "            151 |  # node_linear_7\n",
                            "                   %\"linear_7\"<FLOAT,[8,1,256]>  ::Add(%\"val_383\", %\"transformer.encoder.layers.1.linear2.bias\"{...})\n",
                            "            152 |  # node_add_94\n",
                            "                   %\"add_94\"<FLOAT,[8,1,256]>  ::Add(%\"layer_norm_2\", %\"linear_7\")\n",
                            "            153 |  # node_layer_norm_3\n",
                            "                   %\"layer_norm_3\"<FLOAT,[8,1,256]>  ::LayerNormalization(%\"add_94\", %\"transformer.encoder.layers.1.norm2.weight\"{...}, %\"transformer.encoder.layers.1.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            154 |  # node_layer_norm_4\n",
                            "                   %\"layer_norm_4\"<FLOAT,[8,1,256]>  ::LayerNormalization(%\"layer_norm_3\", %\"transformer.encoder.norm.weight\"{...}, %\"transformer.encoder.norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            155 |  # node_MatMul_378\n",
                            "                   %\"val_389\"<FLOAT,[s59,1,768]>  ::MatMul(%\"add_26\", %\"val_388\"{...})\n",
                            "            156 |  # node_linear_8\n",
                            "                   %\"linear_8\"<FLOAT,[s59,1,768]>  ::Add(%\"val_389\", %\"transformer.decoder.layers.0.self_attn.in_proj_bias\"{...})\n",
                            "            157 |  # node_Concat_384\n",
                            "                   %\"val_395\"<INT64,[4]>  ::Concat(%\"val_1\", %\"val_208\"{[1]}, %\"val_393\"{[3]}, %\"val_285\"{[256]}) {axis=0}\n",
                            "            158 |  # node_view_18\n",
                            "                   %\"view_18\"<FLOAT,[s59,1,3,256]>  ::Reshape(%\"linear_8\", %\"val_395\") {allowzero=1}\n",
                            "            159 |  # node_unsqueeze_2\n",
                            "                   %\"unsqueeze_2\"<FLOAT,[1,s59,1,3,256]>  ::Unsqueeze(%\"view_18\", %\"val_207\"{[0]})\n",
                            "            160 |  # node_transpose_8\n",
                            "                   %\"transpose_8\"<FLOAT,[3,s59,1,1,256]>  ::Transpose(%\"unsqueeze_2\") {perm=(3, 1, 2, 0, 4)}\n",
                            "            161 |  # node_squeeze_2\n",
                            "                   %\"squeeze_2\"<FLOAT,[3,s59,1,256]>  ::Squeeze(%\"transpose_8\", %\"val_218\"{[-2]})\n",
                            "            162 |  # node_select_6\n",
                            "                   %\"select_6\"<FLOAT,[s59,1,256]>  ::Gather(%\"squeeze_2\", %\"val_189\"{0}) {axis=0}\n",
                            "            163 |  # node_select_7\n",
                            "                   %\"select_7\"<FLOAT,[s59,1,256]>  ::Gather(%\"squeeze_2\", %\"val_219\"{1}) {axis=0}\n",
                            "            164 |  # node_select_8\n",
                            "                   %\"select_8\"<FLOAT,[s59,1,256]>  ::Gather(%\"squeeze_2\", %\"val_187\"{2}) {axis=0}\n",
                            "            165 |  # node_Concat_389\n",
                            "                   %\"val_400\"<INT64,[3]>  ::Concat(%\"val_1\", %\"val_223\", %\"val_224\"{[64]}) {axis=0}\n",
                            "            166 |  # node_view_19\n",
                            "                   %\"view_19\"<FLOAT,[s59,4*s34,(64//s34)]>  ::Reshape(%\"select_6\", %\"val_400\") {allowzero=1}\n",
                            "            167 |  # node_transpose_9\n",
                            "                   %\"transpose_9\"<FLOAT,[4*s34,s59,(64//s34)]>  ::Transpose(%\"view_19\") {perm=(1, 0, 2)}\n",
                            "            168 |  # node_view_20\n",
                            "                   %\"view_20\"<FLOAT,[s59,4*s34,(64//s34)]>  ::Reshape(%\"select_7\", %\"val_400\") {allowzero=1}\n",
                            "            169 |  # node_transpose_10\n",
                            "                   %\"transpose_10\"<FLOAT,[4*s34,s59,(64//s34)]>  ::Transpose(%\"view_20\") {perm=(1, 0, 2)}\n",
                            "            170 |  # node_view_21\n",
                            "                   %\"view_21\"<FLOAT,[s59,4*s34,(64//s34)]>  ::Reshape(%\"select_8\", %\"val_400\") {allowzero=1}\n",
                            "            171 |  # node_transpose_11\n",
                            "                   %\"transpose_11\"<FLOAT,[4*s34,s59,(64//s34)]>  ::Transpose(%\"view_21\") {perm=(1, 0, 2)}\n",
                            "            172 |  # node_Concat_405\n",
                            "                   %\"val_416\"<INT64,[4]>  ::Concat(%\"val_0\", %\"val_238\"{[4]}, %\"val_1\", %\"val_224\"{[64]}) {axis=0}\n",
                            "            173 |  # node_view_22\n",
                            "                   %\"view_22\"<FLOAT,[s34,4,s59,(64//s34)]>  ::Reshape(%\"transpose_9\", %\"val_416\") {allowzero=1}\n",
                            "            174 |  # node_view_23\n",
                            "                   %\"view_23\"<FLOAT,[s34,4,s59,(64//s34)]>  ::Reshape(%\"transpose_10\", %\"val_416\") {allowzero=1}\n",
                            "            175 |  # node_view_24\n",
                            "                   %\"view_24\"<FLOAT,[s34,4,s59,(64//s34)]>  ::Reshape(%\"transpose_11\", %\"val_416\") {allowzero=1}\n",
                            "            176 |  # node_Shape_418\n",
                            "                   %\"val_429\"<INT64,[4]>  ::Shape(%\"view_22\") {start=0}\n",
                            "            177 |  # node_Gather_420\n",
                            "                   %\"val_431\"<INT64,[1]>  ::Gather(%\"val_429\", %\"val_221\"{[-1]}) {axis=0}\n",
                            "            178 |  # node_Cast_780\n",
                            "                   %\"val_432\"<FLOAT,[1]>  ::Cast(%\"val_431\") {to=1}\n",
                            "            179 |  # node_Sqrt_424\n",
                            "                   %\"val_435\"<FLOAT,[1]>  ::Sqrt(%\"val_432\")\n",
                            "            180 |  # node_Div_425\n",
                            "                   %\"val_436\"<FLOAT,[1]>  ::Div(%\"val_259\"{1.0}, %\"val_435\")\n",
                            "            181 |  # node_Shape_427\n",
                            "                   %\"val_438\"<INT64,[4]>  ::Shape(%\"view_23\") {start=0}\n",
                            "            182 |  # node_Slice_429\n",
                            "                   %\"val_440\"<INT64,[1]>  ::Slice(%\"val_438\", %\"val_221\"{[-1]}, %\"val_264\"{[9223372036854775807]})\n",
                            "            183 |  # node_Slice_430\n",
                            "                   %\"val_441\"<INT64,[1]>  ::Slice(%\"val_438\", %\"val_218\"{[-2]}, %\"val_221\"{[-1]})\n",
                            "            184 |  # node_Slice_432\n",
                            "                   %\"val_443\"<INT64,[2]>  ::Slice(%\"val_438\", %\"val_268\"{[-9223372036854775808]}, %\"val_218\"{[-2]})\n",
                            "            185 |  # node_Concat_434\n",
                            "                   %\"val_445\"<INT64,[3]>  ::Concat(%\"val_221\"{[-1]}, %\"val_441\", %\"val_440\") {axis=0}\n",
                            "            186 |  # node_Reshape_435\n",
                            "                   %\"val_446\"<FLOAT,[None,None,None]>  ::Reshape(%\"view_23\", %\"val_445\") {allowzero=0}\n",
                            "            187 |  # node_Transpose_436\n",
                            "                   %\"val_447\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_446\") {perm=(0, 2, 1)}\n",
                            "            188 |  # node_Concat_437\n",
                            "                   %\"val_448\"<INT64,[4]>  ::Concat(%\"val_443\", %\"val_440\", %\"val_441\") {axis=0}\n",
                            "            189 |  # node_Reshape_438\n",
                            "                   %\"val_449\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_447\", %\"val_448\") {allowzero=0}\n",
                            "            190 |  # node_Sqrt_439\n",
                            "                   %\"val_450\"<FLOAT,[1]>  ::Sqrt(%\"val_436\")\n",
                            "            191 |  # node_Mul_440\n",
                            "                   %\"val_451\"<FLOAT,[s34,4,s59,(64//s34)]>  ::Mul(%\"view_22\", %\"val_450\")\n",
                            "            192 |  # node_Mul_443\n",
                            "                   %\"val_454\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_449\", %\"val_450\")\n",
                            "            193 |  # node_MatMul_444\n",
                            "                   %\"val_455\"<FLOAT,[None,4,s59,None]>  ::MatMul(%\"val_451\", %\"val_454\")\n",
                            "            194 |  # node_Softmax_445\n",
                            "                   %\"val_456\"<FLOAT,[None,4,s59,None]>  ::Softmax(%\"val_455\") {axis=-1}\n",
                            "            195 |  # node_scaled_dot_product_attention_2\n",
                            "                   %\"scaled_dot_product_attention_2\"<FLOAT,[s34,4,s59,(64//s34)]>  ::MatMul(%\"val_456\", %\"view_24\")\n",
                            "            196 |  # node_permute_4\n",
                            "                   %\"permute_4\"<FLOAT,[s59,s34,4,(64//s34)]>  ::Transpose(%\"scaled_dot_product_attention_2\") {perm=(2, 0, 1, 3)}\n",
                            "            197 |  # node_mul_118\n",
                            "                   %\"mul_118\"<INT64,[]>  ::Mul(%\"sym_size_int_14\", %\"sym_size_int_16\")\n",
                            "            198 |  # node_Reshape_447\n",
                            "                   %\"val_458\"<INT64,[1]>  ::Reshape(%\"mul_118\", %\"val_221\"{[-1]}) {allowzero=0}\n",
                            "            199 |  # node_Concat_449\n",
                            "                   %\"val_460\"<INT64,[2]>  ::Concat(%\"val_458\", %\"val_285\"{[256]}) {axis=0}\n",
                            "            200 |  # node_view_25\n",
                            "                   %\"view_25\"<FLOAT,[s59,4*((64//s34))]>  ::Reshape(%\"permute_4\", %\"val_460\") {allowzero=1}\n",
                            "            201 |  # node_linear_9\n",
                            "                   %\"linear_9\"<FLOAT,[s59,256]>  ::Gemm(%\"view_25\", %\"transformer.decoder.layers.0.self_attn.out_proj.weight\"{...}, %\"transformer.decoder.layers.0.self_attn.out_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
                            "            202 |  # node_Concat_454\n",
                            "                   %\"val_465\"<INT64,[3]>  ::Concat(%\"val_1\", %\"val_0\", %\"val_285\"{[256]}) {axis=0}\n",
                            "            203 |  # node_view_26\n",
                            "                   %\"view_26\"<FLOAT,[s59,s34,(256//s34)]>  ::Reshape(%\"linear_9\", %\"val_465\") {allowzero=1}\n",
                            "            204 |  # node_add_198\n",
                            "                   %\"add_198\"<FLOAT,[s59,s34,256]>  ::Add(%\"add_26\", %\"view_26\")\n",
                            "            205 |  # node_layer_norm_5\n",
                            "                   %\"layer_norm_5\"<FLOAT,[s59,1,256]>  ::LayerNormalization(%\"add_198\", %\"transformer.decoder.layers.0.norm1.weight\"{...}, %\"transformer.decoder.layers.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            206 |  # node_Split_784\n",
                            "                   %\"split_with_sizes_split_0\"<FLOAT,[256,256]>, %\"split_with_sizes_split_1\"<FLOAT,[512,256]>  ::Split(%\"transformer.decoder.layers.0.multihead_attn.in_proj_weight\"{...}, %\"val_468\"{[256, 512]}) {axis=0}\n",
                            "            207 |  # node_Split_788\n",
                            "                   %\"split_with_sizes_1_split_0\"<FLOAT,[256]>, %\"split_with_sizes_1_split_1\"<FLOAT,[512]>  ::Split(%\"transformer.decoder.layers.0.multihead_attn.in_proj_bias\"{...}, %\"val_468\"{[256, 512]}) {axis=0}\n",
                            "            208 |  # node_Transpose_456\n",
                            "                   %\"val_469\"<FLOAT,[256,256]>  ::Transpose(%\"split_with_sizes_split_0\") {perm=(1, 0)}\n",
                            "            209 |  # node_MatMul_457\n",
                            "                   %\"val_470\"<FLOAT,[s59,1,256]>  ::MatMul(%\"layer_norm_5\", %\"val_469\")\n",
                            "            210 |  # node_linear_10\n",
                            "                   %\"linear_10\"<FLOAT,[s59,1,256]>  ::Add(%\"val_470\", %\"split_with_sizes_1_split_0\")\n",
                            "            211 |  # node_Transpose_458\n",
                            "                   %\"val_471\"<FLOAT,[256,512]>  ::Transpose(%\"split_with_sizes_split_1\") {perm=(1, 0)}\n",
                            "            212 |  # node_MatMul_459\n",
                            "                   %\"val_472\"<FLOAT,[8,1,512]>  ::MatMul(%\"layer_norm_4\", %\"val_471\")\n",
                            "            213 |  # node_linear_11\n",
                            "                   %\"linear_11\"<FLOAT,[8,1,512]>  ::Add(%\"val_472\", %\"split_with_sizes_1_split_1\")\n",
                            "            214 |  # node_view_27\n",
                            "                   %\"view_27\"<FLOAT,[8,1,2,256]>  ::Reshape(%\"linear_11\", %\"val_478\"{[8, 1, 2, 256]}) {allowzero=1}\n",
                            "            215 |  # node_unsqueeze_3\n",
                            "                   %\"unsqueeze_3\"<FLOAT,[1,8,1,2,256]>  ::Unsqueeze(%\"view_27\", %\"val_207\"{[0]})\n",
                            "            216 |  # node_transpose_12\n",
                            "                   %\"transpose_12\"<FLOAT,[2,8,1,1,256]>  ::Transpose(%\"unsqueeze_3\") {perm=(3, 1, 2, 0, 4)}\n",
                            "            217 |  # node_squeeze_3\n",
                            "                   %\"squeeze_3\"<FLOAT,[2,8,1,256]>  ::Squeeze(%\"transpose_12\", %\"val_218\"{[-2]})\n",
                            "            218 |  # node_select_9\n",
                            "                   %\"select_9\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze_3\", %\"val_189\"{0}) {axis=0}\n",
                            "            219 |  # node_select_10\n",
                            "                   %\"select_10\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze_3\", %\"val_219\"{1}) {axis=0}\n",
                            "            220 |  # node_Concat_470\n",
                            "                   %\"val_483\"<INT64,[3]>  ::Concat(%\"val_1\", %\"val_238\"{[4]}, %\"val_224\"{[64]}) {axis=0}\n",
                            "            221 |  # node_view_28\n",
                            "                   %\"view_28\"<FLOAT,[s59,4,64]>  ::Reshape(%\"linear_10\", %\"val_483\") {allowzero=1}\n",
                            "            222 |  # node_transpose_13\n",
                            "                   %\"transpose_13\"<FLOAT,[4,s59,64]>  ::Transpose(%\"view_28\") {perm=(1, 0, 2)}\n",
                            "            223 |  # node_view_29\n",
                            "                   %\"view_29\"<FLOAT,[8,4,64]>  ::Reshape(%\"select_9\", %\"val_312\"{[8, 4, 64]}) {allowzero=1}\n",
                            "            224 |  # node_transpose_14\n",
                            "                   %\"transpose_14\"<FLOAT,[4,8,64]>  ::Transpose(%\"view_29\") {perm=(1, 0, 2)}\n",
                            "            225 |  # node_view_30\n",
                            "                   %\"view_30\"<FLOAT,[8,4,64]>  ::Reshape(%\"select_10\", %\"val_312\"{[8, 4, 64]}) {allowzero=1}\n",
                            "            226 |  # node_transpose_15\n",
                            "                   %\"transpose_15\"<FLOAT,[4,8,64]>  ::Transpose(%\"view_30\") {perm=(1, 0, 2)}\n",
                            "            227 |  # node_Concat_486\n",
                            "                   %\"val_499\"<INT64,[4]>  ::Concat(%\"val_208\"{[1]}, %\"val_238\"{[4]}, %\"val_1\", %\"val_224\"{[64]}) {axis=0}\n",
                            "            228 |  # node_view_31\n",
                            "                   %\"view_31\"<FLOAT,[1,4,s59,64]>  ::Reshape(%\"transpose_13\", %\"val_499\") {allowzero=1}\n",
                            "            229 |  # node_view_33\n",
                            "                   %\"view_33\"<FLOAT,[1,4,8,64]>  ::Reshape(%\"transpose_15\", %\"val_328\"{[1, 4, 8, 64]}) {allowzero=1}\n",
                            "            230 |  # node_Transpose_517\n",
                            "                   %\"val_530\"<FLOAT,[4,64,8]>  ::Transpose(%\"transpose_14\") {perm=(0, 2, 1)}\n",
                            "            231 |  # node_Reshape_519\n",
                            "                   %\"val_532\"<FLOAT,[1,4,64,8]>  ::Reshape(%\"val_530\", %\"val_360\"{[1, 4, 64, 8]}) {allowzero=0}\n",
                            "            232 |  # node_Mul_521\n",
                            "                   %\"val_534\"<FLOAT,[1,4,s59,64]>  ::Mul(%\"view_31\", %\"val_362\"{[0.3535533845424652]})\n",
                            "            233 |  # node_Mul_524\n",
                            "                   %\"val_537\"<FLOAT,[1,4,64,8]>  ::Mul(%\"val_532\", %\"val_362\"{[0.3535533845424652]})\n",
                            "            234 |  # node_MatMul_525\n",
                            "                   %\"val_538\"<FLOAT,[1,4,s59,8]>  ::MatMul(%\"val_534\", %\"val_537\")\n",
                            "            235 |  # node_Softmax_526\n",
                            "                   %\"val_539\"<FLOAT,[1,4,s59,8]>  ::Softmax(%\"val_538\") {axis=-1}\n",
                            "            236 |  # node_scaled_dot_product_attention_3\n",
                            "                   %\"scaled_dot_product_attention_3\"<FLOAT,[1,4,s59,64]>  ::MatMul(%\"val_539\", %\"view_33\")\n",
                            "            237 |  # node_permute_5\n",
                            "                   %\"permute_5\"<FLOAT,[s59,1,4,64]>  ::Transpose(%\"scaled_dot_product_attention_3\") {perm=(2, 0, 1, 3)}\n",
                            "            238 |  # node_Concat_530\n",
                            "                   %\"val_543\"<INT64,[2]>  ::Concat(%\"val_1\", %\"val_285\"{[256]}) {axis=0}\n",
                            "            239 |  # node_view_34\n",
                            "                   %\"view_34\"<FLOAT,[s59,256]>  ::Reshape(%\"permute_5\", %\"val_543\") {allowzero=1}\n",
                            "            240 |  # node_linear_12\n",
                            "                   %\"linear_12\"<FLOAT,[s59,256]>  ::Gemm(%\"view_34\", %\"transformer.decoder.layers.0.multihead_attn.out_proj.weight\"{...}, %\"transformer.decoder.layers.0.multihead_attn.out_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
                            "            241 |  # node_Concat_535\n",
                            "                   %\"val_548\"<INT64,[3]>  ::Concat(%\"val_1\", %\"val_208\"{[1]}, %\"val_285\"{[256]}) {axis=0}\n",
                            "            242 |  # node_view_35\n",
                            "                   %\"view_35\"<FLOAT,[s59,1,256]>  ::Reshape(%\"linear_12\", %\"val_548\") {allowzero=1}\n",
                            "            243 |  # node_add_243\n",
                            "                   %\"add_243\"<FLOAT,[s59,1,256]>  ::Add(%\"layer_norm_5\", %\"view_35\")\n",
                            "            244 |  # node_layer_norm_6\n",
                            "                   %\"layer_norm_6\"<FLOAT,[s59,1,256]>  ::LayerNormalization(%\"add_243\", %\"transformer.decoder.layers.0.norm2.weight\"{...}, %\"transformer.decoder.layers.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            245 |  # node_MatMul_537\n",
                            "                   %\"val_552\"<FLOAT,[s59,1,1024]>  ::MatMul(%\"layer_norm_6\", %\"val_551\"{...})\n",
                            "            246 |  # node_linear_13\n",
                            "                   %\"linear_13\"<FLOAT,[s59,1,1024]>  ::Add(%\"val_552\", %\"transformer.decoder.layers.0.linear1.bias\"{...})\n",
                            "            247 |  # node_relu_19\n",
                            "                   %\"relu_19\"<FLOAT,[s59,1,1024]>  ::Relu(%\"linear_13\")\n",
                            "            248 |  # node_MatMul_539\n",
                            "                   %\"val_554\"<FLOAT,[s59,1,256]>  ::MatMul(%\"relu_19\", %\"val_553\"{...})\n",
                            "            249 |  # node_linear_14\n",
                            "                   %\"linear_14\"<FLOAT,[s59,1,256]>  ::Add(%\"val_554\", %\"transformer.decoder.layers.0.linear2.bias\"{...})\n",
                            "            250 |  # node_add_272\n",
                            "                   %\"add_272\"<FLOAT,[s59,1,256]>  ::Add(%\"layer_norm_6\", %\"linear_14\")\n",
                            "            251 |  # node_layer_norm_7\n",
                            "                   %\"layer_norm_7\"<FLOAT,[s59,1,256]>  ::LayerNormalization(%\"add_272\", %\"transformer.decoder.layers.0.norm3.weight\"{...}, %\"transformer.decoder.layers.0.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            252 |  # node_MatMul_541\n",
                            "                   %\"val_558\"<FLOAT,[s59,1,768]>  ::MatMul(%\"layer_norm_7\", %\"val_557\"{...})\n",
                            "            253 |  # node_linear_15\n",
                            "                   %\"linear_15\"<FLOAT,[s59,1,768]>  ::Add(%\"val_558\", %\"transformer.decoder.layers.1.self_attn.in_proj_bias\"{...})\n",
                            "            254 |  # node_view_36\n",
                            "                   %\"view_36\"<FLOAT,[s59,1,3,256]>  ::Reshape(%\"linear_15\", %\"val_395\") {allowzero=1}\n",
                            "            255 |  # node_unsqueeze_4\n",
                            "                   %\"unsqueeze_4\"<FLOAT,[1,s59,1,3,256]>  ::Unsqueeze(%\"view_36\", %\"val_207\"{[0]})\n",
                            "            256 |  # node_transpose_16\n",
                            "                   %\"transpose_16\"<FLOAT,[3,s59,1,1,256]>  ::Transpose(%\"unsqueeze_4\") {perm=(3, 1, 2, 0, 4)}\n",
                            "            257 |  # node_squeeze_4\n",
                            "                   %\"squeeze_4\"<FLOAT,[3,s59,1,256]>  ::Squeeze(%\"transpose_16\", %\"val_218\"{[-2]})\n",
                            "            258 |  # node_select_11\n",
                            "                   %\"select_11\"<FLOAT,[s59,1,256]>  ::Gather(%\"squeeze_4\", %\"val_189\"{0}) {axis=0}\n",
                            "            259 |  # node_select_12\n",
                            "                   %\"select_12\"<FLOAT,[s59,1,256]>  ::Gather(%\"squeeze_4\", %\"val_219\"{1}) {axis=0}\n",
                            "            260 |  # node_select_13\n",
                            "                   %\"select_13\"<FLOAT,[s59,1,256]>  ::Gather(%\"squeeze_4\", %\"val_187\"{2}) {axis=0}\n",
                            "            261 |  # node_view_37\n",
                            "                   %\"view_37\"<FLOAT,[s59,4,64]>  ::Reshape(%\"select_11\", %\"val_483\") {allowzero=1}\n",
                            "            262 |  # node_transpose_17\n",
                            "                   %\"transpose_17\"<FLOAT,[4,s59,64]>  ::Transpose(%\"view_37\") {perm=(1, 0, 2)}\n",
                            "            263 |  # node_view_38\n",
                            "                   %\"view_38\"<FLOAT,[s59,4,64]>  ::Reshape(%\"select_12\", %\"val_483\") {allowzero=1}\n",
                            "            264 |  # node_transpose_18\n",
                            "                   %\"transpose_18\"<FLOAT,[4,s59,64]>  ::Transpose(%\"view_38\") {perm=(1, 0, 2)}\n",
                            "            265 |  # node_view_39\n",
                            "                   %\"view_39\"<FLOAT,[s59,4,64]>  ::Reshape(%\"select_13\", %\"val_483\") {allowzero=1}\n",
                            "            266 |  # node_transpose_19\n",
                            "                   %\"transpose_19\"<FLOAT,[4,s59,64]>  ::Transpose(%\"view_39\") {perm=(1, 0, 2)}\n",
                            "            267 |  # node_view_40\n",
                            "                   %\"view_40\"<FLOAT,[1,4,s59,64]>  ::Reshape(%\"transpose_17\", %\"val_499\") {allowzero=1}\n",
                            "            268 |  # node_view_41\n",
                            "                   %\"view_41\"<FLOAT,[1,4,s59,64]>  ::Reshape(%\"transpose_18\", %\"val_499\") {allowzero=1}\n",
                            "            269 |  # node_view_42\n",
                            "                   %\"view_42\"<FLOAT,[1,4,s59,64]>  ::Reshape(%\"transpose_19\", %\"val_499\") {allowzero=1}\n",
                            "            270 |  # node_Shape_590\n",
                            "                   %\"val_607\"<INT64,[4]>  ::Shape(%\"view_41\") {start=0}\n",
                            "            271 |  # node_Slice_592\n",
                            "                   %\"val_609\"<INT64,[1]>  ::Slice(%\"val_607\", %\"val_221\"{[-1]}, %\"val_264\"{[9223372036854775807]})\n",
                            "            272 |  # node_Slice_593\n",
                            "                   %\"val_610\"<INT64,[1]>  ::Slice(%\"val_607\", %\"val_218\"{[-2]}, %\"val_221\"{[-1]})\n",
                            "            273 |  # node_Slice_595\n",
                            "                   %\"val_612\"<INT64,[2]>  ::Slice(%\"val_607\", %\"val_268\"{[-9223372036854775808]}, %\"val_218\"{[-2]})\n",
                            "            274 |  # node_Concat_597\n",
                            "                   %\"val_614\"<INT64,[3]>  ::Concat(%\"val_221\"{[-1]}, %\"val_610\", %\"val_609\") {axis=0}\n",
                            "            275 |  # node_Reshape_598\n",
                            "                   %\"val_615\"<FLOAT,[None,None,None]>  ::Reshape(%\"view_41\", %\"val_614\") {allowzero=0}\n",
                            "            276 |  # node_Transpose_599\n",
                            "                   %\"val_616\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_615\") {perm=(0, 2, 1)}\n",
                            "            277 |  # node_Concat_600\n",
                            "                   %\"val_617\"<INT64,[4]>  ::Concat(%\"val_612\", %\"val_609\", %\"val_610\") {axis=0}\n",
                            "            278 |  # node_Reshape_601\n",
                            "                   %\"val_618\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_616\", %\"val_617\") {allowzero=0}\n",
                            "            279 |  # node_Mul_603\n",
                            "                   %\"val_620\"<FLOAT,[1,4,s59,64]>  ::Mul(%\"view_40\", %\"val_362\"{[0.3535533845424652]})\n",
                            "            280 |  # node_Mul_606\n",
                            "                   %\"val_623\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_618\", %\"val_362\"{[0.3535533845424652]})\n",
                            "            281 |  # node_MatMul_607\n",
                            "                   %\"val_624\"<FLOAT,[None,4,s59,None]>  ::MatMul(%\"val_620\", %\"val_623\")\n",
                            "            282 |  # node_Softmax_608\n",
                            "                   %\"val_625\"<FLOAT,[None,4,s59,None]>  ::Softmax(%\"val_624\") {axis=-1}\n",
                            "            283 |  # node_scaled_dot_product_attention_4\n",
                            "                   %\"scaled_dot_product_attention_4\"<FLOAT,[1,4,s59,64]>  ::MatMul(%\"val_625\", %\"view_42\")\n",
                            "            284 |  # node_permute_6\n",
                            "                   %\"permute_6\"<FLOAT,[s59,1,4,64]>  ::Transpose(%\"scaled_dot_product_attention_4\") {perm=(2, 0, 1, 3)}\n",
                            "            285 |  # node_view_43\n",
                            "                   %\"view_43\"<FLOAT,[s59,256]>  ::Reshape(%\"permute_6\", %\"val_543\") {allowzero=1}\n",
                            "            286 |  # node_linear_16\n",
                            "                   %\"linear_16\"<FLOAT,[s59,256]>  ::Gemm(%\"view_43\", %\"transformer.decoder.layers.1.self_attn.out_proj.weight\"{...}, %\"transformer.decoder.layers.1.self_attn.out_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
                            "            287 |  # node_view_44\n",
                            "                   %\"view_44\"<FLOAT,[s59,1,256]>  ::Reshape(%\"linear_16\", %\"val_548\") {allowzero=1}\n",
                            "            288 |  # node_add_373\n",
                            "                   %\"add_373\"<FLOAT,[s59,1,256]>  ::Add(%\"layer_norm_7\", %\"view_44\")\n",
                            "            289 |  # node_layer_norm_8\n",
                            "                   %\"layer_norm_8\"<FLOAT,[s59,1,256]>  ::LayerNormalization(%\"add_373\", %\"transformer.decoder.layers.1.norm1.weight\"{...}, %\"transformer.decoder.layers.1.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            290 |  # node_Split_803\n",
                            "                   %\"split_with_sizes_2_split_0\"<FLOAT,[256,256]>, %\"split_with_sizes_2_split_1\"<FLOAT,[512,256]>  ::Split(%\"transformer.decoder.layers.1.multihead_attn.in_proj_weight\"{...}, %\"val_468\"{[256, 512]}) {axis=0}\n",
                            "            291 |  # node_Split_807\n",
                            "                   %\"split_with_sizes_3_split_0\"<FLOAT,[256]>, %\"split_with_sizes_3_split_1\"<FLOAT,[512]>  ::Split(%\"transformer.decoder.layers.1.multihead_attn.in_proj_bias\"{...}, %\"val_468\"{[256, 512]}) {axis=0}\n",
                            "            292 |  # node_Transpose_618\n",
                            "                   %\"val_637\"<FLOAT,[256,256]>  ::Transpose(%\"split_with_sizes_2_split_0\") {perm=(1, 0)}\n",
                            "            293 |  # node_MatMul_619\n",
                            "                   %\"val_638\"<FLOAT,[s59,1,256]>  ::MatMul(%\"layer_norm_8\", %\"val_637\")\n",
                            "            294 |  # node_linear_17\n",
                            "                   %\"linear_17\"<FLOAT,[s59,1,256]>  ::Add(%\"val_638\", %\"split_with_sizes_3_split_0\")\n",
                            "            295 |  # node_Transpose_620\n",
                            "                   %\"val_639\"<FLOAT,[256,512]>  ::Transpose(%\"split_with_sizes_2_split_1\") {perm=(1, 0)}\n",
                            "            296 |  # node_MatMul_621\n",
                            "                   %\"val_640\"<FLOAT,[8,1,512]>  ::MatMul(%\"layer_norm_4\", %\"val_639\")\n",
                            "            297 |  # node_linear_18\n",
                            "                   %\"linear_18\"<FLOAT,[8,1,512]>  ::Add(%\"val_640\", %\"split_with_sizes_3_split_1\")\n",
                            "            298 |  # node_view_45\n",
                            "                   %\"view_45\"<FLOAT,[8,1,2,256]>  ::Reshape(%\"linear_18\", %\"val_478\"{[8, 1, 2, 256]}) {allowzero=1}\n",
                            "            299 |  # node_unsqueeze_5\n",
                            "                   %\"unsqueeze_5\"<FLOAT,[1,8,1,2,256]>  ::Unsqueeze(%\"view_45\", %\"val_207\"{[0]})\n",
                            "            300 |  # node_transpose_20\n",
                            "                   %\"transpose_20\"<FLOAT,[2,8,1,1,256]>  ::Transpose(%\"unsqueeze_5\") {perm=(3, 1, 2, 0, 4)}\n",
                            "            301 |  # node_squeeze_5\n",
                            "                   %\"squeeze_5\"<FLOAT,[2,8,1,256]>  ::Squeeze(%\"transpose_20\", %\"val_218\"{[-2]})\n",
                            "            302 |  # node_select_14\n",
                            "                   %\"select_14\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze_5\", %\"val_189\"{0}) {axis=0}\n",
                            "            303 |  # node_select_15\n",
                            "                   %\"select_15\"<FLOAT,[8,1,256]>  ::Gather(%\"squeeze_5\", %\"val_219\"{1}) {axis=0}\n",
                            "            304 |  # node_view_46\n",
                            "                   %\"view_46\"<FLOAT,[s59,4,64]>  ::Reshape(%\"linear_17\", %\"val_483\") {allowzero=1}\n",
                            "            305 |  # node_transpose_21\n",
                            "                   %\"transpose_21\"<FLOAT,[4,s59,64]>  ::Transpose(%\"view_46\") {perm=(1, 0, 2)}\n",
                            "            306 |  # node_view_47\n",
                            "                   %\"view_47\"<FLOAT,[8,4,64]>  ::Reshape(%\"select_14\", %\"val_312\"{[8, 4, 64]}) {allowzero=1}\n",
                            "            307 |  # node_transpose_22\n",
                            "                   %\"transpose_22\"<FLOAT,[4,8,64]>  ::Transpose(%\"view_47\") {perm=(1, 0, 2)}\n",
                            "            308 |  # node_view_48\n",
                            "                   %\"view_48\"<FLOAT,[8,4,64]>  ::Reshape(%\"select_15\", %\"val_312\"{[8, 4, 64]}) {allowzero=1}\n",
                            "            309 |  # node_transpose_23\n",
                            "                   %\"transpose_23\"<FLOAT,[4,8,64]>  ::Transpose(%\"view_48\") {perm=(1, 0, 2)}\n",
                            "            310 |  # node_view_49\n",
                            "                   %\"view_49\"<FLOAT,[1,4,s59,64]>  ::Reshape(%\"transpose_21\", %\"val_499\") {allowzero=1}\n",
                            "            311 |  # node_view_51\n",
                            "                   %\"view_51\"<FLOAT,[1,4,8,64]>  ::Reshape(%\"transpose_23\", %\"val_328\"{[1, 4, 8, 64]}) {allowzero=1}\n",
                            "            312 |  # node_Transpose_679\n",
                            "                   %\"val_698\"<FLOAT,[4,64,8]>  ::Transpose(%\"transpose_22\") {perm=(0, 2, 1)}\n",
                            "            313 |  # node_Reshape_681\n",
                            "                   %\"val_700\"<FLOAT,[1,4,64,8]>  ::Reshape(%\"val_698\", %\"val_360\"{[1, 4, 64, 8]}) {allowzero=0}\n",
                            "            314 |  # node_Mul_683\n",
                            "                   %\"val_702\"<FLOAT,[1,4,s59,64]>  ::Mul(%\"view_49\", %\"val_362\"{[0.3535533845424652]})\n",
                            "            315 |  # node_Mul_686\n",
                            "                   %\"val_705\"<FLOAT,[1,4,64,8]>  ::Mul(%\"val_700\", %\"val_362\"{[0.3535533845424652]})\n",
                            "            316 |  # node_MatMul_687\n",
                            "                   %\"val_706\"<FLOAT,[1,4,s59,8]>  ::MatMul(%\"val_702\", %\"val_705\")\n",
                            "            317 |  # node_Softmax_688\n",
                            "                   %\"val_707\"<FLOAT,[1,4,s59,8]>  ::Softmax(%\"val_706\") {axis=-1}\n",
                            "            318 |  # node_scaled_dot_product_attention_5\n",
                            "                   %\"scaled_dot_product_attention_5\"<FLOAT,[1,4,s59,64]>  ::MatMul(%\"val_707\", %\"view_51\")\n",
                            "            319 |  # node_permute_7\n",
                            "                   %\"permute_7\"<FLOAT,[s59,1,4,64]>  ::Transpose(%\"scaled_dot_product_attention_5\") {perm=(2, 0, 1, 3)}\n",
                            "            320 |  # node_view_52\n",
                            "                   %\"view_52\"<FLOAT,[s59,256]>  ::Reshape(%\"permute_7\", %\"val_543\") {allowzero=1}\n",
                            "            321 |  # node_linear_19\n",
                            "                   %\"linear_19\"<FLOAT,[s59,256]>  ::Gemm(%\"view_52\", %\"transformer.decoder.layers.1.multihead_attn.out_proj.weight\"{...}, %\"transformer.decoder.layers.1.multihead_attn.out_proj.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
                            "            322 |  # node_view_53\n",
                            "                   %\"view_53\"<FLOAT,[s59,1,256]>  ::Reshape(%\"linear_19\", %\"val_548\") {allowzero=1}\n",
                            "            323 |  # node_add_418\n",
                            "                   %\"add_418\"<FLOAT,[s59,1,256]>  ::Add(%\"layer_norm_8\", %\"view_53\")\n",
                            "            324 |  # node_layer_norm_9\n",
                            "                   %\"layer_norm_9\"<FLOAT,[s59,1,256]>  ::LayerNormalization(%\"add_418\", %\"transformer.decoder.layers.1.norm2.weight\"{...}, %\"transformer.decoder.layers.1.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            325 |  # node_MatMul_699\n",
                            "                   %\"val_720\"<FLOAT,[s59,1,1024]>  ::MatMul(%\"layer_norm_9\", %\"val_719\"{...})\n",
                            "            326 |  # node_linear_20\n",
                            "                   %\"linear_20\"<FLOAT,[s59,1,1024]>  ::Add(%\"val_720\", %\"transformer.decoder.layers.1.linear1.bias\"{...})\n",
                            "            327 |  # node_relu_20\n",
                            "                   %\"relu_20\"<FLOAT,[s59,1,1024]>  ::Relu(%\"linear_20\")\n",
                            "            328 |  # node_MatMul_701\n",
                            "                   %\"val_722\"<FLOAT,[s59,1,256]>  ::MatMul(%\"relu_20\", %\"val_721\"{...})\n",
                            "            329 |  # node_linear_21\n",
                            "                   %\"linear_21\"<FLOAT,[s59,1,256]>  ::Add(%\"val_722\", %\"transformer.decoder.layers.1.linear2.bias\"{...})\n",
                            "            330 |  # node_add_447\n",
                            "                   %\"add_447\"<FLOAT,[s59,1,256]>  ::Add(%\"layer_norm_9\", %\"linear_21\")\n",
                            "            331 |  # node_layer_norm_10\n",
                            "                   %\"layer_norm_10\"<FLOAT,[s59,1,256]>  ::LayerNormalization(%\"add_447\", %\"transformer.decoder.layers.1.norm3.weight\"{...}, %\"transformer.decoder.layers.1.norm3.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            332 |  # node_layer_norm_11\n",
                            "                   %\"layer_norm_11\"<FLOAT,[s59,1,256]>  ::LayerNormalization(%\"layer_norm_10\", %\"transformer.decoder.norm.weight\"{...}, %\"transformer.decoder.norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
                            "            333 |  # node_MatMul_703\n",
                            "                   %\"val_728\"<FLOAT,[s59,1,40]>  ::MatMul(%\"layer_norm_11\", %\"val_727\"{...})\n",
                            "            334 |  # node_linear_22\n",
                            "                   %\"linear_22\"<FLOAT,[s59,1,40]>  ::Add(%\"val_728\", %\"fc_out.bias\"{...})\n",
                            "            335 |  # node_permute_8\n",
                            "                   %\"output\"<FLOAT,[1,s59,40]>  ::Transpose(%\"linear_22\") {perm=(1, 0, 2)}\n",
                            "            return %\"output\"<FLOAT,[1,s59,40]>\n",
                            "        }\n",
                            "\n",
                            "\n",
                            "    ,\n",
                            "    exported_program=\n",
                            "        ExportedProgram:\n",
                            "            class GraphModule(torch.nn.Module):\n",
                            "                def forward(self, p_feature_extractor_backbone_0_weight: \"f32[64, 3, 7, 7]\", p_feature_extractor_backbone_1_weight: \"f32[64]\", p_feature_extractor_backbone_1_bias: \"f32[64]\", p_feature_extractor_backbone_4_0_conv1_weight: \"f32[64, 64, 3, 3]\", p_feature_extractor_backbone_4_0_bn1_weight: \"f32[64]\", p_feature_extractor_backbone_4_0_bn1_bias: \"f32[64]\", p_feature_extractor_backbone_4_0_conv2_weight: \"f32[64, 64, 3, 3]\", p_feature_extractor_backbone_4_0_bn2_weight: \"f32[64]\", p_feature_extractor_backbone_4_0_bn2_bias: \"f32[64]\", p_feature_extractor_backbone_4_1_conv1_weight: \"f32[64, 64, 3, 3]\", p_feature_extractor_backbone_4_1_bn1_weight: \"f32[64]\", p_feature_extractor_backbone_4_1_bn1_bias: \"f32[64]\", p_feature_extractor_backbone_4_1_conv2_weight: \"f32[64, 64, 3, 3]\", p_feature_extractor_backbone_4_1_bn2_weight: \"f32[64]\", p_feature_extractor_backbone_4_1_bn2_bias: \"f32[64]\", p_feature_extractor_backbone_5_0_conv1_weight: \"f32[128, 64, 3, 3]\", p_feature_extractor_backbone_5_0_bn1_weight: \"f32[128]\", p_feature_extractor_backbone_5_0_bn1_bias: \"f32[128]\", p_feature_extractor_backbone_5_0_conv2_weight: \"f32[128, 128, 3, 3]\", p_feature_extractor_backbone_5_0_bn2_weight: \"f32[128]\", p_feature_extractor_backbone_5_0_bn2_bias: \"f32[128]\", p_feature_extractor_backbone_5_0_downsample_0_weight: \"f32[128, 64, 1, 1]\", p_feature_extractor_backbone_5_0_downsample_1_weight: \"f32[128]\", p_feature_extractor_backbone_5_0_downsample_1_bias: \"f32[128]\", p_feature_extractor_backbone_5_1_conv1_weight: \"f32[128, 128, 3, 3]\", p_feature_extractor_backbone_5_1_bn1_weight: \"f32[128]\", p_feature_extractor_backbone_5_1_bn1_bias: \"f32[128]\", p_feature_extractor_backbone_5_1_conv2_weight: \"f32[128, 128, 3, 3]\", p_feature_extractor_backbone_5_1_bn2_weight: \"f32[128]\", p_feature_extractor_backbone_5_1_bn2_bias: \"f32[128]\", p_feature_extractor_backbone_6_0_conv1_weight: \"f32[256, 128, 3, 3]\", p_feature_extractor_backbone_6_0_bn1_weight: \"f32[256]\", p_feature_extractor_backbone_6_0_bn1_bias: \"f32[256]\", p_feature_extractor_backbone_6_0_conv2_weight: \"f32[256, 256, 3, 3]\", p_feature_extractor_backbone_6_0_bn2_weight: \"f32[256]\", p_feature_extractor_backbone_6_0_bn2_bias: \"f32[256]\", p_feature_extractor_backbone_6_0_downsample_0_weight: \"f32[256, 128, 1, 1]\", p_feature_extractor_backbone_6_0_downsample_1_weight: \"f32[256]\", p_feature_extractor_backbone_6_0_downsample_1_bias: \"f32[256]\", p_feature_extractor_backbone_6_1_conv1_weight: \"f32[256, 256, 3, 3]\", p_feature_extractor_backbone_6_1_bn1_weight: \"f32[256]\", p_feature_extractor_backbone_6_1_bn1_bias: \"f32[256]\", p_feature_extractor_backbone_6_1_conv2_weight: \"f32[256, 256, 3, 3]\", p_feature_extractor_backbone_6_1_bn2_weight: \"f32[256]\", p_feature_extractor_backbone_6_1_bn2_bias: \"f32[256]\", p_feature_extractor_backbone_7_0_conv1_weight: \"f32[512, 256, 3, 3]\", p_feature_extractor_backbone_7_0_bn1_weight: \"f32[512]\", p_feature_extractor_backbone_7_0_bn1_bias: \"f32[512]\", p_feature_extractor_backbone_7_0_conv2_weight: \"f32[512, 512, 3, 3]\", p_feature_extractor_backbone_7_0_bn2_weight: \"f32[512]\", p_feature_extractor_backbone_7_0_bn2_bias: \"f32[512]\", p_feature_extractor_backbone_7_0_downsample_0_weight: \"f32[512, 256, 1, 1]\", p_feature_extractor_backbone_7_0_downsample_1_weight: \"f32[512]\", p_feature_extractor_backbone_7_0_downsample_1_bias: \"f32[512]\", p_feature_extractor_backbone_7_1_conv1_weight: \"f32[512, 512, 3, 3]\", p_feature_extractor_backbone_7_1_bn1_weight: \"f32[512]\", p_feature_extractor_backbone_7_1_bn1_bias: \"f32[512]\", p_feature_extractor_backbone_7_1_conv2_weight: \"f32[512, 512, 3, 3]\", p_feature_extractor_backbone_7_1_bn2_weight: \"f32[512]\", p_feature_extractor_backbone_7_1_bn2_bias: \"f32[512]\", p_adapter_weight: \"f32[256, 512, 1, 1]\", p_adapter_bias: \"f32[256]\", p_transformer_encoder_layers_0_self_attn_in_proj_weight: \"f32[768, 256]\", p_transformer_encoder_layers_0_self_attn_in_proj_bias: \"f32[768]\", p_transformer_encoder_layers_0_self_attn_out_proj_weight: \"f32[256, 256]\", p_transformer_encoder_layers_0_self_attn_out_proj_bias: \"f32[256]\", p_transformer_encoder_layers_0_linear1_weight: \"f32[1024, 256]\", p_transformer_encoder_layers_0_linear1_bias: \"f32[1024]\", p_transformer_encoder_layers_0_linear2_weight: \"f32[256, 1024]\", p_transformer_encoder_layers_0_linear2_bias: \"f32[256]\", p_transformer_encoder_layers_0_norm1_weight: \"f32[256]\", p_transformer_encoder_layers_0_norm1_bias: \"f32[256]\", p_transformer_encoder_layers_0_norm2_weight: \"f32[256]\", p_transformer_encoder_layers_0_norm2_bias: \"f32[256]\", p_transformer_encoder_layers_1_self_attn_in_proj_weight: \"f32[768, 256]\", p_transformer_encoder_layers_1_self_attn_in_proj_bias: \"f32[768]\", p_transformer_encoder_layers_1_self_attn_out_proj_weight: \"f32[256, 256]\", p_transformer_encoder_layers_1_self_attn_out_proj_bias: \"f32[256]\", p_transformer_encoder_layers_1_linear1_weight: \"f32[1024, 256]\", p_transformer_encoder_layers_1_linear1_bias: \"f32[1024]\", p_transformer_encoder_layers_1_linear2_weight: \"f32[256, 1024]\", p_transformer_encoder_layers_1_linear2_bias: \"f32[256]\", p_transformer_encoder_layers_1_norm1_weight: \"f32[256]\", p_transformer_encoder_layers_1_norm1_bias: \"f32[256]\", p_transformer_encoder_layers_1_norm2_weight: \"f32[256]\", p_transformer_encoder_layers_1_norm2_bias: \"f32[256]\", p_transformer_encoder_norm_weight: \"f32[256]\", p_transformer_encoder_norm_bias: \"f32[256]\", p_transformer_decoder_layers_0_self_attn_in_proj_weight: \"f32[768, 256]\", p_transformer_decoder_layers_0_self_attn_in_proj_bias: \"f32[768]\", p_transformer_decoder_layers_0_self_attn_out_proj_weight: \"f32[256, 256]\", p_transformer_decoder_layers_0_self_attn_out_proj_bias: \"f32[256]\", p_transformer_decoder_layers_0_multihead_attn_in_proj_weight: \"f32[768, 256]\", p_transformer_decoder_layers_0_multihead_attn_in_proj_bias: \"f32[768]\", p_transformer_decoder_layers_0_multihead_attn_out_proj_weight: \"f32[256, 256]\", p_transformer_decoder_layers_0_multihead_attn_out_proj_bias: \"f32[256]\", p_transformer_decoder_layers_0_linear1_weight: \"f32[1024, 256]\", p_transformer_decoder_layers_0_linear1_bias: \"f32[1024]\", p_transformer_decoder_layers_0_linear2_weight: \"f32[256, 1024]\", p_transformer_decoder_layers_0_linear2_bias: \"f32[256]\", p_transformer_decoder_layers_0_norm1_weight: \"f32[256]\", p_transformer_decoder_layers_0_norm1_bias: \"f32[256]\", p_transformer_decoder_layers_0_norm2_weight: \"f32[256]\", p_transformer_decoder_layers_0_norm2_bias: \"f32[256]\", p_transformer_decoder_layers_0_norm3_weight: \"f32[256]\", p_transformer_decoder_layers_0_norm3_bias: \"f32[256]\", p_transformer_decoder_layers_1_self_attn_in_proj_weight: \"f32[768, 256]\", p_transformer_decoder_layers_1_self_attn_in_proj_bias: \"f32[768]\", p_transformer_decoder_layers_1_self_attn_out_proj_weight: \"f32[256, 256]\", p_transformer_decoder_layers_1_self_attn_out_proj_bias: \"f32[256]\", p_transformer_decoder_layers_1_multihead_attn_in_proj_weight: \"f32[768, 256]\", p_transformer_decoder_layers_1_multihead_attn_in_proj_bias: \"f32[768]\", p_transformer_decoder_layers_1_multihead_attn_out_proj_weight: \"f32[256, 256]\", p_transformer_decoder_layers_1_multihead_attn_out_proj_bias: \"f32[256]\", p_transformer_decoder_layers_1_linear1_weight: \"f32[1024, 256]\", p_transformer_decoder_layers_1_linear1_bias: \"f32[1024]\", p_transformer_decoder_layers_1_linear2_weight: \"f32[256, 1024]\", p_transformer_decoder_layers_1_linear2_bias: \"f32[256]\", p_transformer_decoder_layers_1_norm1_weight: \"f32[256]\", p_transformer_decoder_layers_1_norm1_bias: \"f32[256]\", p_transformer_decoder_layers_1_norm2_weight: \"f32[256]\", p_transformer_decoder_layers_1_norm2_bias: \"f32[256]\", p_transformer_decoder_layers_1_norm3_weight: \"f32[256]\", p_transformer_decoder_layers_1_norm3_bias: \"f32[256]\", p_transformer_decoder_norm_weight: \"f32[256]\", p_transformer_decoder_norm_bias: \"f32[256]\", p_embedding_weight: \"f32[40, 256]\", p_fc_out_weight: \"f32[40, 256]\", p_fc_out_bias: \"f32[40]\", b_feature_extractor_backbone_1_running_mean: \"f32[64]\", b_feature_extractor_backbone_1_running_var: \"f32[64]\", b_feature_extractor_backbone_1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_4_0_bn1_running_mean: \"f32[64]\", b_feature_extractor_backbone_4_0_bn1_running_var: \"f32[64]\", b_feature_extractor_backbone_4_0_bn1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_4_0_bn2_running_mean: \"f32[64]\", b_feature_extractor_backbone_4_0_bn2_running_var: \"f32[64]\", b_feature_extractor_backbone_4_0_bn2_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_4_1_bn1_running_mean: \"f32[64]\", b_feature_extractor_backbone_4_1_bn1_running_var: \"f32[64]\", b_feature_extractor_backbone_4_1_bn1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_4_1_bn2_running_mean: \"f32[64]\", b_feature_extractor_backbone_4_1_bn2_running_var: \"f32[64]\", b_feature_extractor_backbone_4_1_bn2_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_5_0_bn1_running_mean: \"f32[128]\", b_feature_extractor_backbone_5_0_bn1_running_var: \"f32[128]\", b_feature_extractor_backbone_5_0_bn1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_5_0_bn2_running_mean: \"f32[128]\", b_feature_extractor_backbone_5_0_bn2_running_var: \"f32[128]\", b_feature_extractor_backbone_5_0_bn2_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_5_0_downsample_1_running_mean: \"f32[128]\", b_feature_extractor_backbone_5_0_downsample_1_running_var: \"f32[128]\", b_feature_extractor_backbone_5_0_downsample_1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_5_1_bn1_running_mean: \"f32[128]\", b_feature_extractor_backbone_5_1_bn1_running_var: \"f32[128]\", b_feature_extractor_backbone_5_1_bn1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_5_1_bn2_running_mean: \"f32[128]\", b_feature_extractor_backbone_5_1_bn2_running_var: \"f32[128]\", b_feature_extractor_backbone_5_1_bn2_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_6_0_bn1_running_mean: \"f32[256]\", b_feature_extractor_backbone_6_0_bn1_running_var: \"f32[256]\", b_feature_extractor_backbone_6_0_bn1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_6_0_bn2_running_mean: \"f32[256]\", b_feature_extractor_backbone_6_0_bn2_running_var: \"f32[256]\", b_feature_extractor_backbone_6_0_bn2_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_6_0_downsample_1_running_mean: \"f32[256]\", b_feature_extractor_backbone_6_0_downsample_1_running_var: \"f32[256]\", b_feature_extractor_backbone_6_0_downsample_1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_6_1_bn1_running_mean: \"f32[256]\", b_feature_extractor_backbone_6_1_bn1_running_var: \"f32[256]\", b_feature_extractor_backbone_6_1_bn1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_6_1_bn2_running_mean: \"f32[256]\", b_feature_extractor_backbone_6_1_bn2_running_var: \"f32[256]\", b_feature_extractor_backbone_6_1_bn2_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_7_0_bn1_running_mean: \"f32[512]\", b_feature_extractor_backbone_7_0_bn1_running_var: \"f32[512]\", b_feature_extractor_backbone_7_0_bn1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_7_0_bn2_running_mean: \"f32[512]\", b_feature_extractor_backbone_7_0_bn2_running_var: \"f32[512]\", b_feature_extractor_backbone_7_0_bn2_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_7_0_downsample_1_running_mean: \"f32[512]\", b_feature_extractor_backbone_7_0_downsample_1_running_var: \"f32[512]\", b_feature_extractor_backbone_7_0_downsample_1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_7_1_bn1_running_mean: \"f32[512]\", b_feature_extractor_backbone_7_1_bn1_running_var: \"f32[512]\", b_feature_extractor_backbone_7_1_bn1_num_batches_tracked: \"i64[]\", b_feature_extractor_backbone_7_1_bn2_running_mean: \"f32[512]\", b_feature_extractor_backbone_7_1_bn2_running_var: \"f32[512]\", b_feature_extractor_backbone_7_1_bn2_num_batches_tracked: \"i64[]\", b_pos_encoder_pe: \"f32[32, 1, 256]\", images: \"f32[s34, 3, 64, 256]\", tgt: \"i64[s34, s59]\"):\n",
                            "                     # \n",
                            "                    sym_size_int_14: \"Sym(s34)\" = torch.ops.aten.sym_size.int(images, 0)\n",
                            "                    sym_size_int_16: \"Sym(s59)\" = torch.ops.aten.sym_size.int(tgt, 1)\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d: \"f32[s34, 64, 32, 128]\" = torch.ops.aten.conv2d.default(images, p_feature_extractor_backbone_0_weight, None, [2, 2], [3, 3]);  images = p_feature_extractor_backbone_0_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d, p_feature_extractor_backbone_1_weight, p_feature_extractor_backbone_1_bias, b_feature_extractor_backbone_1_running_mean, b_feature_extractor_backbone_1_running_var, 0.1, 1e-05);  conv2d = p_feature_extractor_backbone_1_weight = p_feature_extractor_backbone_1_bias = b_feature_extractor_backbone_1_running_mean = b_feature_extractor_backbone_1_running_var = None\n",
                            "                    getitem: \"f32[1, 64, 32, 128]\" = _native_batch_norm_legit_no_training[0];  _native_batch_norm_legit_no_training = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu: \"f32[1, 64, 32, 128]\" = torch.ops.aten.relu.default(getitem);  getitem = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(\n",
                            "                    max_pool2d: \"f32[1, 64, 16, 64]\" = torch.ops.aten.max_pool2d.default(relu, [3, 3], [2, 2], [1, 1]);  relu = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_1: \"f32[1, 64, 16, 64]\" = torch.ops.aten.conv2d.default(max_pool2d, p_feature_extractor_backbone_4_0_conv1_weight, None, [1, 1], [1, 1]);  p_feature_extractor_backbone_4_0_conv1_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_1 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_1, p_feature_extractor_backbone_4_0_bn1_weight, p_feature_extractor_backbone_4_0_bn1_bias, b_feature_extractor_backbone_4_0_bn1_running_mean, b_feature_extractor_backbone_4_0_bn1_running_var, 0.1, 1e-05);  conv2d_1 = p_feature_extractor_backbone_4_0_bn1_weight = p_feature_extractor_backbone_4_0_bn1_bias = b_feature_extractor_backbone_4_0_bn1_running_mean = b_feature_extractor_backbone_4_0_bn1_running_var = None\n",
                            "                    getitem_3: \"f32[1, 64, 16, 64]\" = _native_batch_norm_legit_no_training_1[0];  _native_batch_norm_legit_no_training_1 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_1: \"f32[1, 64, 16, 64]\" = torch.ops.aten.relu.default(getitem_3);  getitem_3 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_2: \"f32[1, 64, 16, 64]\" = torch.ops.aten.conv2d.default(relu_1, p_feature_extractor_backbone_4_0_conv2_weight, None, [1, 1], [1, 1]);  relu_1 = p_feature_extractor_backbone_4_0_conv2_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_2 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_2, p_feature_extractor_backbone_4_0_bn2_weight, p_feature_extractor_backbone_4_0_bn2_bias, b_feature_extractor_backbone_4_0_bn2_running_mean, b_feature_extractor_backbone_4_0_bn2_running_var, 0.1, 1e-05);  conv2d_2 = p_feature_extractor_backbone_4_0_bn2_weight = p_feature_extractor_backbone_4_0_bn2_bias = b_feature_extractor_backbone_4_0_bn2_running_mean = b_feature_extractor_backbone_4_0_bn2_running_var = None\n",
                            "                    getitem_6: \"f32[1, 64, 16, 64]\" = _native_batch_norm_legit_no_training_2[0];  _native_batch_norm_legit_no_training_2 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/resnet.py:102 in forward, code: out += identity\n",
                            "                    add_5: \"f32[1, 64, 16, 64]\" = torch.ops.aten.add.Tensor(getitem_6, max_pool2d);  getitem_6 = max_pool2d = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_2: \"f32[1, 64, 16, 64]\" = torch.ops.aten.relu.default(add_5);  add_5 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_3: \"f32[1, 64, 16, 64]\" = torch.ops.aten.conv2d.default(relu_2, p_feature_extractor_backbone_4_1_conv1_weight, None, [1, 1], [1, 1]);  p_feature_extractor_backbone_4_1_conv1_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_3 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_3, p_feature_extractor_backbone_4_1_bn1_weight, p_feature_extractor_backbone_4_1_bn1_bias, b_feature_extractor_backbone_4_1_bn1_running_mean, b_feature_extractor_backbone_4_1_bn1_running_var, 0.1, 1e-05);  conv2d_3 = p_feature_extractor_backbone_4_1_bn1_weight = p_feature_extractor_backbone_4_1_bn1_bias = b_feature_extractor_backbone_4_1_bn1_running_mean = b_feature_extractor_backbone_4_1_bn1_running_var = None\n",
                            "                    getitem_9: \"f32[1, 64, 16, 64]\" = _native_batch_norm_legit_no_training_3[0];  _native_batch_norm_legit_no_training_3 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_3: \"f32[1, 64, 16, 64]\" = torch.ops.aten.relu.default(getitem_9);  getitem_9 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_4: \"f32[1, 64, 16, 64]\" = torch.ops.aten.conv2d.default(relu_3, p_feature_extractor_backbone_4_1_conv2_weight, None, [1, 1], [1, 1]);  relu_3 = p_feature_extractor_backbone_4_1_conv2_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_4 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_4, p_feature_extractor_backbone_4_1_bn2_weight, p_feature_extractor_backbone_4_1_bn2_bias, b_feature_extractor_backbone_4_1_bn2_running_mean, b_feature_extractor_backbone_4_1_bn2_running_var, 0.1, 1e-05);  conv2d_4 = p_feature_extractor_backbone_4_1_bn2_weight = p_feature_extractor_backbone_4_1_bn2_bias = b_feature_extractor_backbone_4_1_bn2_running_mean = b_feature_extractor_backbone_4_1_bn2_running_var = None\n",
                            "                    getitem_12: \"f32[1, 64, 16, 64]\" = _native_batch_norm_legit_no_training_4[0];  _native_batch_norm_legit_no_training_4 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/resnet.py:102 in forward, code: out += identity\n",
                            "                    add_6: \"f32[1, 64, 16, 64]\" = torch.ops.aten.add.Tensor(getitem_12, relu_2);  getitem_12 = relu_2 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_4: \"f32[1, 64, 16, 64]\" = torch.ops.aten.relu.default(add_6);  add_6 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_5: \"f32[1, 128, 8, 32]\" = torch.ops.aten.conv2d.default(relu_4, p_feature_extractor_backbone_5_0_conv1_weight, None, [2, 2], [1, 1]);  p_feature_extractor_backbone_5_0_conv1_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_5 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_5, p_feature_extractor_backbone_5_0_bn1_weight, p_feature_extractor_backbone_5_0_bn1_bias, b_feature_extractor_backbone_5_0_bn1_running_mean, b_feature_extractor_backbone_5_0_bn1_running_var, 0.1, 1e-05);  conv2d_5 = p_feature_extractor_backbone_5_0_bn1_weight = p_feature_extractor_backbone_5_0_bn1_bias = b_feature_extractor_backbone_5_0_bn1_running_mean = b_feature_extractor_backbone_5_0_bn1_running_var = None\n",
                            "                    getitem_15: \"f32[1, 128, 8, 32]\" = _native_batch_norm_legit_no_training_5[0];  _native_batch_norm_legit_no_training_5 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_5: \"f32[1, 128, 8, 32]\" = torch.ops.aten.relu.default(getitem_15);  getitem_15 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_6: \"f32[1, 128, 8, 32]\" = torch.ops.aten.conv2d.default(relu_5, p_feature_extractor_backbone_5_0_conv2_weight, None, [1, 1], [1, 1]);  relu_5 = p_feature_extractor_backbone_5_0_conv2_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_6 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_6, p_feature_extractor_backbone_5_0_bn2_weight, p_feature_extractor_backbone_5_0_bn2_bias, b_feature_extractor_backbone_5_0_bn2_running_mean, b_feature_extractor_backbone_5_0_bn2_running_var, 0.1, 1e-05);  conv2d_6 = p_feature_extractor_backbone_5_0_bn2_weight = p_feature_extractor_backbone_5_0_bn2_bias = b_feature_extractor_backbone_5_0_bn2_running_mean = b_feature_extractor_backbone_5_0_bn2_running_var = None\n",
                            "                    getitem_18: \"f32[1, 128, 8, 32]\" = _native_batch_norm_legit_no_training_6[0];  _native_batch_norm_legit_no_training_6 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_7: \"f32[1, 128, 8, 32]\" = torch.ops.aten.conv2d.default(relu_4, p_feature_extractor_backbone_5_0_downsample_0_weight, None, [2, 2]);  relu_4 = p_feature_extractor_backbone_5_0_downsample_0_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_7 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_7, p_feature_extractor_backbone_5_0_downsample_1_weight, p_feature_extractor_backbone_5_0_downsample_1_bias, b_feature_extractor_backbone_5_0_downsample_1_running_mean, b_feature_extractor_backbone_5_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_7 = p_feature_extractor_backbone_5_0_downsample_1_weight = p_feature_extractor_backbone_5_0_downsample_1_bias = b_feature_extractor_backbone_5_0_downsample_1_running_mean = b_feature_extractor_backbone_5_0_downsample_1_running_var = None\n",
                            "                    getitem_21: \"f32[1, 128, 8, 32]\" = _native_batch_norm_legit_no_training_7[0];  _native_batch_norm_legit_no_training_7 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/resnet.py:102 in forward, code: out += identity\n",
                            "                    add_7: \"f32[1, 128, 8, 32]\" = torch.ops.aten.add.Tensor(getitem_18, getitem_21);  getitem_18 = getitem_21 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_6: \"f32[1, 128, 8, 32]\" = torch.ops.aten.relu.default(add_7);  add_7 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_8: \"f32[1, 128, 8, 32]\" = torch.ops.aten.conv2d.default(relu_6, p_feature_extractor_backbone_5_1_conv1_weight, None, [1, 1], [1, 1]);  p_feature_extractor_backbone_5_1_conv1_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_8 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_8, p_feature_extractor_backbone_5_1_bn1_weight, p_feature_extractor_backbone_5_1_bn1_bias, b_feature_extractor_backbone_5_1_bn1_running_mean, b_feature_extractor_backbone_5_1_bn1_running_var, 0.1, 1e-05);  conv2d_8 = p_feature_extractor_backbone_5_1_bn1_weight = p_feature_extractor_backbone_5_1_bn1_bias = b_feature_extractor_backbone_5_1_bn1_running_mean = b_feature_extractor_backbone_5_1_bn1_running_var = None\n",
                            "                    getitem_24: \"f32[1, 128, 8, 32]\" = _native_batch_norm_legit_no_training_8[0];  _native_batch_norm_legit_no_training_8 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_7: \"f32[1, 128, 8, 32]\" = torch.ops.aten.relu.default(getitem_24);  getitem_24 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_9: \"f32[1, 128, 8, 32]\" = torch.ops.aten.conv2d.default(relu_7, p_feature_extractor_backbone_5_1_conv2_weight, None, [1, 1], [1, 1]);  relu_7 = p_feature_extractor_backbone_5_1_conv2_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_9 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_9, p_feature_extractor_backbone_5_1_bn2_weight, p_feature_extractor_backbone_5_1_bn2_bias, b_feature_extractor_backbone_5_1_bn2_running_mean, b_feature_extractor_backbone_5_1_bn2_running_var, 0.1, 1e-05);  conv2d_9 = p_feature_extractor_backbone_5_1_bn2_weight = p_feature_extractor_backbone_5_1_bn2_bias = b_feature_extractor_backbone_5_1_bn2_running_mean = b_feature_extractor_backbone_5_1_bn2_running_var = None\n",
                            "                    getitem_27: \"f32[1, 128, 8, 32]\" = _native_batch_norm_legit_no_training_9[0];  _native_batch_norm_legit_no_training_9 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/resnet.py:102 in forward, code: out += identity\n",
                            "                    add_8: \"f32[1, 128, 8, 32]\" = torch.ops.aten.add.Tensor(getitem_27, relu_6);  getitem_27 = relu_6 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_8: \"f32[1, 128, 8, 32]\" = torch.ops.aten.relu.default(add_8);  add_8 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_10: \"f32[1, 256, 4, 16]\" = torch.ops.aten.conv2d.default(relu_8, p_feature_extractor_backbone_6_0_conv1_weight, None, [2, 2], [1, 1]);  p_feature_extractor_backbone_6_0_conv1_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_10 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_10, p_feature_extractor_backbone_6_0_bn1_weight, p_feature_extractor_backbone_6_0_bn1_bias, b_feature_extractor_backbone_6_0_bn1_running_mean, b_feature_extractor_backbone_6_0_bn1_running_var, 0.1, 1e-05);  conv2d_10 = p_feature_extractor_backbone_6_0_bn1_weight = p_feature_extractor_backbone_6_0_bn1_bias = b_feature_extractor_backbone_6_0_bn1_running_mean = b_feature_extractor_backbone_6_0_bn1_running_var = None\n",
                            "                    getitem_30: \"f32[1, 256, 4, 16]\" = _native_batch_norm_legit_no_training_10[0];  _native_batch_norm_legit_no_training_10 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_9: \"f32[1, 256, 4, 16]\" = torch.ops.aten.relu.default(getitem_30);  getitem_30 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_11: \"f32[1, 256, 4, 16]\" = torch.ops.aten.conv2d.default(relu_9, p_feature_extractor_backbone_6_0_conv2_weight, None, [1, 1], [1, 1]);  relu_9 = p_feature_extractor_backbone_6_0_conv2_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_11 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_11, p_feature_extractor_backbone_6_0_bn2_weight, p_feature_extractor_backbone_6_0_bn2_bias, b_feature_extractor_backbone_6_0_bn2_running_mean, b_feature_extractor_backbone_6_0_bn2_running_var, 0.1, 1e-05);  conv2d_11 = p_feature_extractor_backbone_6_0_bn2_weight = p_feature_extractor_backbone_6_0_bn2_bias = b_feature_extractor_backbone_6_0_bn2_running_mean = b_feature_extractor_backbone_6_0_bn2_running_var = None\n",
                            "                    getitem_33: \"f32[1, 256, 4, 16]\" = _native_batch_norm_legit_no_training_11[0];  _native_batch_norm_legit_no_training_11 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_12: \"f32[1, 256, 4, 16]\" = torch.ops.aten.conv2d.default(relu_8, p_feature_extractor_backbone_6_0_downsample_0_weight, None, [2, 2]);  relu_8 = p_feature_extractor_backbone_6_0_downsample_0_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_12 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_12, p_feature_extractor_backbone_6_0_downsample_1_weight, p_feature_extractor_backbone_6_0_downsample_1_bias, b_feature_extractor_backbone_6_0_downsample_1_running_mean, b_feature_extractor_backbone_6_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_12 = p_feature_extractor_backbone_6_0_downsample_1_weight = p_feature_extractor_backbone_6_0_downsample_1_bias = b_feature_extractor_backbone_6_0_downsample_1_running_mean = b_feature_extractor_backbone_6_0_downsample_1_running_var = None\n",
                            "                    getitem_36: \"f32[1, 256, 4, 16]\" = _native_batch_norm_legit_no_training_12[0];  _native_batch_norm_legit_no_training_12 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/resnet.py:102 in forward, code: out += identity\n",
                            "                    add_9: \"f32[1, 256, 4, 16]\" = torch.ops.aten.add.Tensor(getitem_33, getitem_36);  getitem_33 = getitem_36 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_10: \"f32[1, 256, 4, 16]\" = torch.ops.aten.relu.default(add_9);  add_9 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_13: \"f32[1, 256, 4, 16]\" = torch.ops.aten.conv2d.default(relu_10, p_feature_extractor_backbone_6_1_conv1_weight, None, [1, 1], [1, 1]);  p_feature_extractor_backbone_6_1_conv1_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_13 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_13, p_feature_extractor_backbone_6_1_bn1_weight, p_feature_extractor_backbone_6_1_bn1_bias, b_feature_extractor_backbone_6_1_bn1_running_mean, b_feature_extractor_backbone_6_1_bn1_running_var, 0.1, 1e-05);  conv2d_13 = p_feature_extractor_backbone_6_1_bn1_weight = p_feature_extractor_backbone_6_1_bn1_bias = b_feature_extractor_backbone_6_1_bn1_running_mean = b_feature_extractor_backbone_6_1_bn1_running_var = None\n",
                            "                    getitem_39: \"f32[1, 256, 4, 16]\" = _native_batch_norm_legit_no_training_13[0];  _native_batch_norm_legit_no_training_13 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_11: \"f32[1, 256, 4, 16]\" = torch.ops.aten.relu.default(getitem_39);  getitem_39 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_14: \"f32[1, 256, 4, 16]\" = torch.ops.aten.conv2d.default(relu_11, p_feature_extractor_backbone_6_1_conv2_weight, None, [1, 1], [1, 1]);  relu_11 = p_feature_extractor_backbone_6_1_conv2_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_14 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_14, p_feature_extractor_backbone_6_1_bn2_weight, p_feature_extractor_backbone_6_1_bn2_bias, b_feature_extractor_backbone_6_1_bn2_running_mean, b_feature_extractor_backbone_6_1_bn2_running_var, 0.1, 1e-05);  conv2d_14 = p_feature_extractor_backbone_6_1_bn2_weight = p_feature_extractor_backbone_6_1_bn2_bias = b_feature_extractor_backbone_6_1_bn2_running_mean = b_feature_extractor_backbone_6_1_bn2_running_var = None\n",
                            "                    getitem_42: \"f32[1, 256, 4, 16]\" = _native_batch_norm_legit_no_training_14[0];  _native_batch_norm_legit_no_training_14 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/resnet.py:102 in forward, code: out += identity\n",
                            "                    add_10: \"f32[1, 256, 4, 16]\" = torch.ops.aten.add.Tensor(getitem_42, relu_10);  getitem_42 = relu_10 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_12: \"f32[1, 256, 4, 16]\" = torch.ops.aten.relu.default(add_10);  add_10 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_15: \"f32[1, 512, 2, 8]\" = torch.ops.aten.conv2d.default(relu_12, p_feature_extractor_backbone_7_0_conv1_weight, None, [2, 2], [1, 1]);  p_feature_extractor_backbone_7_0_conv1_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_15 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_15, p_feature_extractor_backbone_7_0_bn1_weight, p_feature_extractor_backbone_7_0_bn1_bias, b_feature_extractor_backbone_7_0_bn1_running_mean, b_feature_extractor_backbone_7_0_bn1_running_var, 0.1, 1e-05);  conv2d_15 = p_feature_extractor_backbone_7_0_bn1_weight = p_feature_extractor_backbone_7_0_bn1_bias = b_feature_extractor_backbone_7_0_bn1_running_mean = b_feature_extractor_backbone_7_0_bn1_running_var = None\n",
                            "                    getitem_45: \"f32[1, 512, 2, 8]\" = _native_batch_norm_legit_no_training_15[0];  _native_batch_norm_legit_no_training_15 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_13: \"f32[1, 512, 2, 8]\" = torch.ops.aten.relu.default(getitem_45);  getitem_45 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_16: \"f32[1, 512, 2, 8]\" = torch.ops.aten.conv2d.default(relu_13, p_feature_extractor_backbone_7_0_conv2_weight, None, [1, 1], [1, 1]);  relu_13 = p_feature_extractor_backbone_7_0_conv2_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_16 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_16, p_feature_extractor_backbone_7_0_bn2_weight, p_feature_extractor_backbone_7_0_bn2_bias, b_feature_extractor_backbone_7_0_bn2_running_mean, b_feature_extractor_backbone_7_0_bn2_running_var, 0.1, 1e-05);  conv2d_16 = p_feature_extractor_backbone_7_0_bn2_weight = p_feature_extractor_backbone_7_0_bn2_bias = b_feature_extractor_backbone_7_0_bn2_running_mean = b_feature_extractor_backbone_7_0_bn2_running_var = None\n",
                            "                    getitem_48: \"f32[1, 512, 2, 8]\" = _native_batch_norm_legit_no_training_16[0];  _native_batch_norm_legit_no_training_16 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_17: \"f32[1, 512, 2, 8]\" = torch.ops.aten.conv2d.default(relu_12, p_feature_extractor_backbone_7_0_downsample_0_weight, None, [2, 2]);  relu_12 = p_feature_extractor_backbone_7_0_downsample_0_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_17 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_17, p_feature_extractor_backbone_7_0_downsample_1_weight, p_feature_extractor_backbone_7_0_downsample_1_bias, b_feature_extractor_backbone_7_0_downsample_1_running_mean, b_feature_extractor_backbone_7_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_17 = p_feature_extractor_backbone_7_0_downsample_1_weight = p_feature_extractor_backbone_7_0_downsample_1_bias = b_feature_extractor_backbone_7_0_downsample_1_running_mean = b_feature_extractor_backbone_7_0_downsample_1_running_var = None\n",
                            "                    getitem_51: \"f32[1, 512, 2, 8]\" = _native_batch_norm_legit_no_training_17[0];  _native_batch_norm_legit_no_training_17 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/resnet.py:102 in forward, code: out += identity\n",
                            "                    add_11: \"f32[1, 512, 2, 8]\" = torch.ops.aten.add.Tensor(getitem_48, getitem_51);  getitem_48 = getitem_51 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_14: \"f32[1, 512, 2, 8]\" = torch.ops.aten.relu.default(add_11);  add_11 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_18: \"f32[1, 512, 2, 8]\" = torch.ops.aten.conv2d.default(relu_14, p_feature_extractor_backbone_7_1_conv1_weight, None, [1, 1], [1, 1]);  p_feature_extractor_backbone_7_1_conv1_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_18 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_18, p_feature_extractor_backbone_7_1_bn1_weight, p_feature_extractor_backbone_7_1_bn1_bias, b_feature_extractor_backbone_7_1_bn1_running_mean, b_feature_extractor_backbone_7_1_bn1_running_var, 0.1, 1e-05);  conv2d_18 = p_feature_extractor_backbone_7_1_bn1_weight = p_feature_extractor_backbone_7_1_bn1_bias = b_feature_extractor_backbone_7_1_bn1_running_mean = b_feature_extractor_backbone_7_1_bn1_running_var = None\n",
                            "                    getitem_54: \"f32[1, 512, 2, 8]\" = _native_batch_norm_legit_no_training_18[0];  _native_batch_norm_legit_no_training_18 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_15: \"f32[1, 512, 2, 8]\" = torch.ops.aten.relu.default(getitem_54);  getitem_54 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_19: \"f32[1, 512, 2, 8]\" = torch.ops.aten.conv2d.default(relu_15, p_feature_extractor_backbone_7_1_conv2_weight, None, [1, 1], [1, 1]);  relu_15 = p_feature_extractor_backbone_7_1_conv2_weight = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
                            "                    _native_batch_norm_legit_no_training_19 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_19, p_feature_extractor_backbone_7_1_bn2_weight, p_feature_extractor_backbone_7_1_bn2_bias, b_feature_extractor_backbone_7_1_bn2_running_mean, b_feature_extractor_backbone_7_1_bn2_running_var, 0.1, 1e-05);  conv2d_19 = p_feature_extractor_backbone_7_1_bn2_weight = p_feature_extractor_backbone_7_1_bn2_bias = b_feature_extractor_backbone_7_1_bn2_running_mean = b_feature_extractor_backbone_7_1_bn2_running_var = None\n",
                            "                    getitem_57: \"f32[1, 512, 2, 8]\" = _native_batch_norm_legit_no_training_19[0];  _native_batch_norm_legit_no_training_19 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/resnet.py:102 in forward, code: out += identity\n",
                            "                    add_12: \"f32[1, 512, 2, 8]\" = torch.ops.aten.add.Tensor(getitem_57, relu_14);  getitem_57 = relu_14 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
                            "                    relu_16: \"f32[1, 512, 2, 8]\" = torch.ops.aten.relu.default(add_12);  add_12 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
                            "                    conv2d_20: \"f32[1, 256, 2, 8]\" = torch.ops.aten.conv2d.default(relu_16, p_adapter_weight, p_adapter_bias);  relu_16 = p_adapter_weight = p_adapter_bias = None\n",
                            "            \n",
                            "                     # File: /Users/nikhil/Desktop/Leegality_task/model.py:114 in forward, code: features = torch.max(features, dim=2)[0]\n",
                            "                    max_1 = torch.ops.aten.max.dim(conv2d_20, 2);  conv2d_20 = None\n",
                            "                    getitem_60: \"f32[1, 256, 8]\" = max_1[0];  max_1 = None\n",
                            "            \n",
                            "                     # File: /Users/nikhil/Desktop/Leegality_task/model.py:117 in forward, code: src = features.permute(2, 0, 1)\n",
                            "                    permute: \"f32[8, 1, 256]\" = torch.ops.aten.permute.default(getitem_60, [2, 0, 1]);  getitem_60 = None\n",
                            "            \n",
                            "                     # File: /Users/nikhil/Desktop/Leegality_task/model.py:56 in forward, code: x = x + self.pe[:x.size(0)]\n",
                            "                    slice_1: \"f32[8, 1, 256]\" = torch.ops.aten.slice.Tensor(b_pos_encoder_pe, 0, 0, 8)\n",
                            "                    add_13: \"f32[8, 1, 256]\" = torch.ops.aten.add.Tensor(permute, slice_1);  permute = slice_1 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone: \"f32[8, 1, 256]\" = torch.ops.aten.clone.default(add_13);  add_13 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
                            "                    embedding: \"f32[s34, s59, 256]\" = torch.ops.aten.embedding.default(p_embedding_weight, tgt);  p_embedding_weight = tgt = None\n",
                            "            \n",
                            "                     # File: /Users/nikhil/Desktop/Leegality_task/model.py:124 in forward, code: tgt_emb = self.embedding(tgt).permute(1, 0, 2)\n",
                            "                    permute_1: \"f32[s59, s34, 256]\" = torch.ops.aten.permute.default(embedding, [1, 0, 2]);  embedding = None\n",
                            "            \n",
                            "                     # File: /Users/nikhil/Desktop/Leegality_task/model.py:56 in forward, code: x = x + self.pe[:x.size(0)]\n",
                            "                    slice_2: \"f32[s59, 1, 256]\" = torch.ops.aten.slice.Tensor(b_pos_encoder_pe, 0, None, sym_size_int_16);  b_pos_encoder_pe = None\n",
                            "                    add_26: \"f32[s59, 1, 256]\" = torch.ops.aten.add.Tensor(permute_1, slice_2);  permute_1 = slice_2 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_1: \"f32[s59, 1, 256]\" = torch.ops.aten.clone.default(add_26);  add_26 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:1488 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
                            "                    linear: \"f32[8, 1, 768]\" = torch.ops.aten.linear.default(clone, p_transformer_encoder_layers_0_self_attn_in_proj_weight, p_transformer_encoder_layers_0_self_attn_in_proj_bias);  p_transformer_encoder_layers_0_self_attn_in_proj_weight = p_transformer_encoder_layers_0_self_attn_in_proj_bias = None\n",
                            "                    view: \"f32[8, 1, 3, 256]\" = torch.ops.aten.view.default(linear, [8, 1, 3, 256]);  linear = None\n",
                            "                    unsqueeze: \"f32[1, 8, 1, 3, 256]\" = torch.ops.aten.unsqueeze.default(view, 0);  view = None\n",
                            "                    transpose: \"f32[3, 8, 1, 1, 256]\" = torch.ops.aten.transpose.int(unsqueeze, 0, -2);  unsqueeze = None\n",
                            "                    squeeze: \"f32[3, 8, 1, 256]\" = torch.ops.aten.squeeze.dim(transpose, -2);  transpose = None\n",
                            "                    clone_2: \"f32[3, 8, 1, 256]\" = torch.ops.aten.clone.default(squeeze, memory_format = torch.contiguous_format);  squeeze = None\n",
                            "                    select: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_2, 0, 0)\n",
                            "                    select_1: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_2, 0, 1)\n",
                            "                    select_2: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_2, 0, 2);  clone_2 = None\n",
                            "                    mul_17: \"Sym(4*s34)\" = sym_size_int_14 * 4\n",
                            "                    view_1: \"f32[8, 4*s34, (64//s34)]\" = torch.ops.aten.view.default(select, [8, mul_17, 64]);  select = None\n",
                            "                    transpose_1: \"f32[4*s34, 8, (64//s34)]\" = torch.ops.aten.transpose.int(view_1, 0, 1);  view_1 = None\n",
                            "                    view_2: \"f32[8, 4*s34, (64//s34)]\" = torch.ops.aten.view.default(select_1, [8, mul_17, 64]);  select_1 = None\n",
                            "                    transpose_2: \"f32[4*s34, 8, (64//s34)]\" = torch.ops.aten.transpose.int(view_2, 0, 1);  view_2 = None\n",
                            "                    view_3: \"f32[8, 4*s34, (64//s34)]\" = torch.ops.aten.view.default(select_2, [8, mul_17, 64]);  select_2 = None\n",
                            "                    transpose_3: \"f32[4*s34, 8, (64//s34)]\" = torch.ops.aten.transpose.int(view_3, 0, 1);  view_3 = None\n",
                            "                    view_4: \"f32[s34, 4, 8, (64//s34)]\" = torch.ops.aten.view.default(transpose_1, [sym_size_int_14, 4, 8, 64]);  transpose_1 = None\n",
                            "                    view_5: \"f32[s34, 4, 8, (64//s34)]\" = torch.ops.aten.view.default(transpose_2, [sym_size_int_14, 4, 8, 64]);  transpose_2 = None\n",
                            "                    view_6: \"f32[s34, 4, 8, (64//s34)]\" = torch.ops.aten.view.default(transpose_3, [sym_size_int_14, 4, 8, 64]);  transpose_3 = None\n",
                            "                    scaled_dot_product_attention: \"f32[s34, 4, 8, (64//s34)]\" = torch.ops.aten.scaled_dot_product_attention.default(view_4, view_5, view_6);  view_4 = view_5 = view_6 = None\n",
                            "                    permute_2: \"f32[8, s34, 4, (64//s34)]\" = torch.ops.aten.permute.default(scaled_dot_product_attention, [2, 0, 1, 3]);  scaled_dot_product_attention = None\n",
                            "                    mul_47: \"Sym(8*s34)\" = sym_size_int_14 * 8\n",
                            "                    view_7: \"f32[8, 4*((64//s34))]\" = torch.ops.aten.view.default(permute_2, [mul_47, 256]);  permute_2 = mul_47 = None\n",
                            "                    linear_1: \"f32[8, 256]\" = torch.ops.aten.linear.default(view_7, p_transformer_encoder_layers_0_self_attn_out_proj_weight, p_transformer_encoder_layers_0_self_attn_out_proj_bias);  view_7 = p_transformer_encoder_layers_0_self_attn_out_proj_weight = p_transformer_encoder_layers_0_self_attn_out_proj_bias = None\n",
                            "                    view_8: \"f32[8, s34, (256//s34)]\" = torch.ops.aten.view.default(linear_1, [8, sym_size_int_14, 256]);  linear_1 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_3: \"f32[8, s34, (256//s34)]\" = torch.ops.aten.clone.default(view_8);  view_8 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:934 in forward, code: x\n",
                            "                    add_91: \"f32[8, 1, 256]\" = torch.ops.aten.add.Tensor(clone, clone_3);  clone = clone_3 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm: \"f32[8, 1, 256]\" = torch.ops.aten.layer_norm.default(add_91, [256], p_transformer_encoder_layers_0_norm1_weight, p_transformer_encoder_layers_0_norm1_bias);  add_91 = p_transformer_encoder_layers_0_norm1_weight = p_transformer_encoder_layers_0_norm1_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_2: \"f32[8, 1, 1024]\" = torch.ops.aten.linear.default(layer_norm, p_transformer_encoder_layers_0_linear1_weight, p_transformer_encoder_layers_0_linear1_bias);  p_transformer_encoder_layers_0_linear1_weight = p_transformer_encoder_layers_0_linear1_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:937 in forward, code: x = self.norm2(x + self._ff_block(x))\n",
                            "                    relu_17: \"f32[8, 1, 1024]\" = torch.ops.aten.relu.default(linear_2);  linear_2 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_4: \"f32[8, 1, 1024]\" = torch.ops.aten.clone.default(relu_17);  relu_17 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_3: \"f32[8, 1, 256]\" = torch.ops.aten.linear.default(clone_4, p_transformer_encoder_layers_0_linear2_weight, p_transformer_encoder_layers_0_linear2_bias);  clone_4 = p_transformer_encoder_layers_0_linear2_weight = p_transformer_encoder_layers_0_linear2_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_5: \"f32[8, 1, 256]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:937 in forward, code: x = self.norm2(x + self._ff_block(x))\n",
                            "                    add_92: \"f32[8, 1, 256]\" = torch.ops.aten.add.Tensor(layer_norm, clone_5);  layer_norm = clone_5 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_1: \"f32[8, 1, 256]\" = torch.ops.aten.layer_norm.default(add_92, [256], p_transformer_encoder_layers_0_norm2_weight, p_transformer_encoder_layers_0_norm2_bias);  add_92 = p_transformer_encoder_layers_0_norm2_weight = p_transformer_encoder_layers_0_norm2_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:1488 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
                            "                    linear_4: \"f32[8, 1, 768]\" = torch.ops.aten.linear.default(layer_norm_1, p_transformer_encoder_layers_1_self_attn_in_proj_weight, p_transformer_encoder_layers_1_self_attn_in_proj_bias);  p_transformer_encoder_layers_1_self_attn_in_proj_weight = p_transformer_encoder_layers_1_self_attn_in_proj_bias = None\n",
                            "                    view_9: \"f32[8, 1, 3, 256]\" = torch.ops.aten.view.default(linear_4, [8, 1, 3, 256]);  linear_4 = None\n",
                            "                    unsqueeze_1: \"f32[1, 8, 1, 3, 256]\" = torch.ops.aten.unsqueeze.default(view_9, 0);  view_9 = None\n",
                            "                    transpose_4: \"f32[3, 8, 1, 1, 256]\" = torch.ops.aten.transpose.int(unsqueeze_1, 0, -2);  unsqueeze_1 = None\n",
                            "                    squeeze_1: \"f32[3, 8, 1, 256]\" = torch.ops.aten.squeeze.dim(transpose_4, -2);  transpose_4 = None\n",
                            "                    clone_6: \"f32[3, 8, 1, 256]\" = torch.ops.aten.clone.default(squeeze_1, memory_format = torch.contiguous_format);  squeeze_1 = None\n",
                            "                    select_3: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_6, 0, 0)\n",
                            "                    select_4: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_6, 0, 1)\n",
                            "                    select_5: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_6, 0, 2);  clone_6 = None\n",
                            "                    view_10: \"f32[8, 4, 64]\" = torch.ops.aten.view.default(select_3, [8, 4, 64]);  select_3 = None\n",
                            "                    transpose_5: \"f32[4, 8, 64]\" = torch.ops.aten.transpose.int(view_10, 0, 1);  view_10 = None\n",
                            "                    view_11: \"f32[8, 4, 64]\" = torch.ops.aten.view.default(select_4, [8, 4, 64]);  select_4 = None\n",
                            "                    transpose_6: \"f32[4, 8, 64]\" = torch.ops.aten.transpose.int(view_11, 0, 1);  view_11 = None\n",
                            "                    view_12: \"f32[8, 4, 64]\" = torch.ops.aten.view.default(select_5, [8, 4, 64]);  select_5 = None\n",
                            "                    transpose_7: \"f32[4, 8, 64]\" = torch.ops.aten.transpose.int(view_12, 0, 1);  view_12 = None\n",
                            "                    view_13: \"f32[1, 4, 8, 64]\" = torch.ops.aten.view.default(transpose_5, [1, 4, 8, 64]);  transpose_5 = None\n",
                            "                    view_14: \"f32[1, 4, 8, 64]\" = torch.ops.aten.view.default(transpose_6, [1, 4, 8, 64]);  transpose_6 = None\n",
                            "                    view_15: \"f32[1, 4, 8, 64]\" = torch.ops.aten.view.default(transpose_7, [1, 4, 8, 64]);  transpose_7 = None\n",
                            "                    scaled_dot_product_attention_1: \"f32[1, 4, 8, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(view_13, view_14, view_15);  view_13 = view_14 = view_15 = None\n",
                            "                    permute_3: \"f32[8, 1, 4, 64]\" = torch.ops.aten.permute.default(scaled_dot_product_attention_1, [2, 0, 1, 3]);  scaled_dot_product_attention_1 = None\n",
                            "                    view_16: \"f32[8, 256]\" = torch.ops.aten.view.default(permute_3, [8, 256]);  permute_3 = None\n",
                            "                    linear_5: \"f32[8, 256]\" = torch.ops.aten.linear.default(view_16, p_transformer_encoder_layers_1_self_attn_out_proj_weight, p_transformer_encoder_layers_1_self_attn_out_proj_bias);  view_16 = p_transformer_encoder_layers_1_self_attn_out_proj_weight = p_transformer_encoder_layers_1_self_attn_out_proj_bias = None\n",
                            "                    view_17: \"f32[8, 1, 256]\" = torch.ops.aten.view.default(linear_5, [8, 1, 256]);  linear_5 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_7: \"f32[8, 1, 256]\" = torch.ops.aten.clone.default(view_17);  view_17 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:934 in forward, code: x\n",
                            "                    add_93: \"f32[8, 1, 256]\" = torch.ops.aten.add.Tensor(layer_norm_1, clone_7);  layer_norm_1 = clone_7 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_2: \"f32[8, 1, 256]\" = torch.ops.aten.layer_norm.default(add_93, [256], p_transformer_encoder_layers_1_norm1_weight, p_transformer_encoder_layers_1_norm1_bias);  add_93 = p_transformer_encoder_layers_1_norm1_weight = p_transformer_encoder_layers_1_norm1_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_6: \"f32[8, 1, 1024]\" = torch.ops.aten.linear.default(layer_norm_2, p_transformer_encoder_layers_1_linear1_weight, p_transformer_encoder_layers_1_linear1_bias);  p_transformer_encoder_layers_1_linear1_weight = p_transformer_encoder_layers_1_linear1_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:937 in forward, code: x = self.norm2(x + self._ff_block(x))\n",
                            "                    relu_18: \"f32[8, 1, 1024]\" = torch.ops.aten.relu.default(linear_6);  linear_6 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_8: \"f32[8, 1, 1024]\" = torch.ops.aten.clone.default(relu_18);  relu_18 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_7: \"f32[8, 1, 256]\" = torch.ops.aten.linear.default(clone_8, p_transformer_encoder_layers_1_linear2_weight, p_transformer_encoder_layers_1_linear2_bias);  clone_8 = p_transformer_encoder_layers_1_linear2_weight = p_transformer_encoder_layers_1_linear2_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_9: \"f32[8, 1, 256]\" = torch.ops.aten.clone.default(linear_7);  linear_7 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:937 in forward, code: x = self.norm2(x + self._ff_block(x))\n",
                            "                    add_94: \"f32[8, 1, 256]\" = torch.ops.aten.add.Tensor(layer_norm_2, clone_9);  layer_norm_2 = clone_9 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_3: \"f32[8, 1, 256]\" = torch.ops.aten.layer_norm.default(add_94, [256], p_transformer_encoder_layers_1_norm2_weight, p_transformer_encoder_layers_1_norm2_bias);  add_94 = p_transformer_encoder_layers_1_norm2_weight = p_transformer_encoder_layers_1_norm2_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_4: \"f32[8, 1, 256]\" = torch.ops.aten.layer_norm.default(layer_norm_3, [256], p_transformer_encoder_norm_weight, p_transformer_encoder_norm_bias);  layer_norm_3 = p_transformer_encoder_norm_weight = p_transformer_encoder_norm_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:1488 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
                            "                    linear_8: \"f32[s59, 1, 768]\" = torch.ops.aten.linear.default(clone_1, p_transformer_decoder_layers_0_self_attn_in_proj_weight, p_transformer_decoder_layers_0_self_attn_in_proj_bias);  p_transformer_decoder_layers_0_self_attn_in_proj_weight = p_transformer_decoder_layers_0_self_attn_in_proj_bias = None\n",
                            "                    view_18: \"f32[s59, 1, 3, 256]\" = torch.ops.aten.view.default(linear_8, [sym_size_int_16, 1, 3, 256]);  linear_8 = None\n",
                            "                    unsqueeze_2: \"f32[1, s59, 1, 3, 256]\" = torch.ops.aten.unsqueeze.default(view_18, 0);  view_18 = None\n",
                            "                    transpose_8: \"f32[3, s59, 1, 1, 256]\" = torch.ops.aten.transpose.int(unsqueeze_2, 0, -2);  unsqueeze_2 = None\n",
                            "                    squeeze_2: \"f32[3, s59, 1, 256]\" = torch.ops.aten.squeeze.dim(transpose_8, -2);  transpose_8 = None\n",
                            "                    clone_10: \"f32[3, s59, 1, 256]\" = torch.ops.aten.clone.default(squeeze_2, memory_format = torch.contiguous_format);  squeeze_2 = None\n",
                            "                    select_6: \"f32[s59, 1, 256]\" = torch.ops.aten.select.int(clone_10, 0, 0)\n",
                            "                    select_7: \"f32[s59, 1, 256]\" = torch.ops.aten.select.int(clone_10, 0, 1)\n",
                            "                    select_8: \"f32[s59, 1, 256]\" = torch.ops.aten.select.int(clone_10, 0, 2);  clone_10 = None\n",
                            "                    view_19: \"f32[s59, 4*s34, (64//s34)]\" = torch.ops.aten.view.default(select_6, [sym_size_int_16, mul_17, 64]);  select_6 = None\n",
                            "                    transpose_9: \"f32[4*s34, s59, (64//s34)]\" = torch.ops.aten.transpose.int(view_19, 0, 1);  view_19 = None\n",
                            "                    view_20: \"f32[s59, 4*s34, (64//s34)]\" = torch.ops.aten.view.default(select_7, [sym_size_int_16, mul_17, 64]);  select_7 = None\n",
                            "                    transpose_10: \"f32[4*s34, s59, (64//s34)]\" = torch.ops.aten.transpose.int(view_20, 0, 1);  view_20 = None\n",
                            "                    view_21: \"f32[s59, 4*s34, (64//s34)]\" = torch.ops.aten.view.default(select_8, [sym_size_int_16, mul_17, 64]);  select_8 = mul_17 = None\n",
                            "                    transpose_11: \"f32[4*s34, s59, (64//s34)]\" = torch.ops.aten.transpose.int(view_21, 0, 1);  view_21 = None\n",
                            "                    view_22: \"f32[s34, 4, s59, (64//s34)]\" = torch.ops.aten.view.default(transpose_9, [sym_size_int_14, 4, sym_size_int_16, 64]);  transpose_9 = None\n",
                            "                    view_23: \"f32[s34, 4, s59, (64//s34)]\" = torch.ops.aten.view.default(transpose_10, [sym_size_int_14, 4, sym_size_int_16, 64]);  transpose_10 = None\n",
                            "                    view_24: \"f32[s34, 4, s59, (64//s34)]\" = torch.ops.aten.view.default(transpose_11, [sym_size_int_14, 4, sym_size_int_16, 64]);  transpose_11 = None\n",
                            "                    scaled_dot_product_attention_2: \"f32[s34, 4, s59, (64//s34)]\" = torch.ops.aten.scaled_dot_product_attention.default(view_22, view_23, view_24);  view_22 = view_23 = view_24 = None\n",
                            "                    permute_4: \"f32[s59, s34, 4, (64//s34)]\" = torch.ops.aten.permute.default(scaled_dot_product_attention_2, [2, 0, 1, 3]);  scaled_dot_product_attention_2 = None\n",
                            "                    mul_118: \"Sym(s34*s59)\" = sym_size_int_14 * sym_size_int_16\n",
                            "                    view_25: \"f32[s59, 4*((64//s34))]\" = torch.ops.aten.view.default(permute_4, [mul_118, 256]);  permute_4 = mul_118 = None\n",
                            "                    linear_9: \"f32[s59, 256]\" = torch.ops.aten.linear.default(view_25, p_transformer_decoder_layers_0_self_attn_out_proj_weight, p_transformer_decoder_layers_0_self_attn_out_proj_bias);  view_25 = p_transformer_decoder_layers_0_self_attn_out_proj_weight = p_transformer_decoder_layers_0_self_attn_out_proj_bias = None\n",
                            "                    view_26: \"f32[s59, s34, (256//s34)]\" = torch.ops.aten.view.default(linear_9, [sym_size_int_16, sym_size_int_14, 256]);  linear_9 = sym_size_int_14 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_11: \"f32[s59, s34, (256//s34)]\" = torch.ops.aten.clone.default(view_26);  view_26 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1123 in forward, code: x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n",
                            "                    add_198: \"f32[s59, s34, 256]\" = torch.ops.aten.add.Tensor(clone_1, clone_11);  clone_1 = clone_11 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_5: \"f32[s59, 1, 256]\" = torch.ops.aten.layer_norm.default(add_198, [256], p_transformer_decoder_layers_0_norm1_weight, p_transformer_decoder_layers_0_norm1_bias);  add_198 = p_transformer_decoder_layers_0_norm1_weight = p_transformer_decoder_layers_0_norm1_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:1488 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
                            "                    split_with_sizes = torch.ops.aten.split_with_sizes.default(p_transformer_decoder_layers_0_multihead_attn_in_proj_weight, [256, 512]);  p_transformer_decoder_layers_0_multihead_attn_in_proj_weight = None\n",
                            "                    getitem_62: \"f32[256, 256]\" = split_with_sizes[0]\n",
                            "                    getitem_63: \"f32[512, 256]\" = split_with_sizes[1];  split_with_sizes = None\n",
                            "                    split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(p_transformer_decoder_layers_0_multihead_attn_in_proj_bias, [256, 512]);  p_transformer_decoder_layers_0_multihead_attn_in_proj_bias = None\n",
                            "                    getitem_64: \"f32[256]\" = split_with_sizes_1[0]\n",
                            "                    getitem_65: \"f32[512]\" = split_with_sizes_1[1];  split_with_sizes_1 = None\n",
                            "                    linear_10: \"f32[s59, 1, 256]\" = torch.ops.aten.linear.default(layer_norm_5, getitem_62, getitem_64);  getitem_62 = getitem_64 = None\n",
                            "                    linear_11: \"f32[8, 1, 512]\" = torch.ops.aten.linear.default(layer_norm_4, getitem_63, getitem_65);  getitem_63 = getitem_65 = None\n",
                            "                    view_27: \"f32[8, 1, 2, 256]\" = torch.ops.aten.view.default(linear_11, [8, 1, 2, 256]);  linear_11 = None\n",
                            "                    unsqueeze_3: \"f32[1, 8, 1, 2, 256]\" = torch.ops.aten.unsqueeze.default(view_27, 0);  view_27 = None\n",
                            "                    transpose_12: \"f32[2, 8, 1, 1, 256]\" = torch.ops.aten.transpose.int(unsqueeze_3, 0, -2);  unsqueeze_3 = None\n",
                            "                    squeeze_3: \"f32[2, 8, 1, 256]\" = torch.ops.aten.squeeze.dim(transpose_12, -2);  transpose_12 = None\n",
                            "                    clone_12: \"f32[2, 8, 1, 256]\" = torch.ops.aten.clone.default(squeeze_3, memory_format = torch.contiguous_format);  squeeze_3 = None\n",
                            "                    select_9: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_12, 0, 0)\n",
                            "                    select_10: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_12, 0, 1);  clone_12 = None\n",
                            "                    view_28: \"f32[s59, 4, 64]\" = torch.ops.aten.view.default(linear_10, [sym_size_int_16, 4, 64]);  linear_10 = None\n",
                            "                    transpose_13: \"f32[4, s59, 64]\" = torch.ops.aten.transpose.int(view_28, 0, 1);  view_28 = None\n",
                            "                    view_29: \"f32[8, 4, 64]\" = torch.ops.aten.view.default(select_9, [8, 4, 64]);  select_9 = None\n",
                            "                    transpose_14: \"f32[4, 8, 64]\" = torch.ops.aten.transpose.int(view_29, 0, 1);  view_29 = None\n",
                            "                    view_30: \"f32[8, 4, 64]\" = torch.ops.aten.view.default(select_10, [8, 4, 64]);  select_10 = None\n",
                            "                    transpose_15: \"f32[4, 8, 64]\" = torch.ops.aten.transpose.int(view_30, 0, 1);  view_30 = None\n",
                            "                    view_31: \"f32[1, 4, s59, 64]\" = torch.ops.aten.view.default(transpose_13, [1, 4, sym_size_int_16, 64]);  transpose_13 = None\n",
                            "                    view_32: \"f32[1, 4, 8, 64]\" = torch.ops.aten.view.default(transpose_14, [1, 4, 8, 64]);  transpose_14 = None\n",
                            "                    view_33: \"f32[1, 4, 8, 64]\" = torch.ops.aten.view.default(transpose_15, [1, 4, 8, 64]);  transpose_15 = None\n",
                            "                    scaled_dot_product_attention_3: \"f32[1, 4, s59, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(view_31, view_32, view_33);  view_31 = view_32 = view_33 = None\n",
                            "                    permute_5: \"f32[s59, 1, 4, 64]\" = torch.ops.aten.permute.default(scaled_dot_product_attention_3, [2, 0, 1, 3]);  scaled_dot_product_attention_3 = None\n",
                            "                    view_34: \"f32[s59, 256]\" = torch.ops.aten.view.default(permute_5, [sym_size_int_16, 256]);  permute_5 = None\n",
                            "                    linear_12: \"f32[s59, 256]\" = torch.ops.aten.linear.default(view_34, p_transformer_decoder_layers_0_multihead_attn_out_proj_weight, p_transformer_decoder_layers_0_multihead_attn_out_proj_bias);  view_34 = p_transformer_decoder_layers_0_multihead_attn_out_proj_weight = p_transformer_decoder_layers_0_multihead_attn_out_proj_bias = None\n",
                            "                    view_35: \"f32[s59, 1, 256]\" = torch.ops.aten.view.default(linear_12, [sym_size_int_16, 1, 256]);  linear_12 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_13: \"f32[s59, 1, 256]\" = torch.ops.aten.clone.default(view_35);  view_35 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1126 in forward, code: x\n",
                            "                    add_243: \"f32[s59, 1, 256]\" = torch.ops.aten.add.Tensor(layer_norm_5, clone_13);  layer_norm_5 = clone_13 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_6: \"f32[s59, 1, 256]\" = torch.ops.aten.layer_norm.default(add_243, [256], p_transformer_decoder_layers_0_norm2_weight, p_transformer_decoder_layers_0_norm2_bias);  add_243 = p_transformer_decoder_layers_0_norm2_weight = p_transformer_decoder_layers_0_norm2_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_13: \"f32[s59, 1, 1024]\" = torch.ops.aten.linear.default(layer_norm_6, p_transformer_decoder_layers_0_linear1_weight, p_transformer_decoder_layers_0_linear1_bias);  p_transformer_decoder_layers_0_linear1_weight = p_transformer_decoder_layers_0_linear1_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1131 in forward, code: x = self.norm3(x + self._ff_block(x))\n",
                            "                    relu_19: \"f32[s59, 1, 1024]\" = torch.ops.aten.relu.default(linear_13);  linear_13 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_14: \"f32[s59, 1, 1024]\" = torch.ops.aten.clone.default(relu_19);  relu_19 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_14: \"f32[s59, 1, 256]\" = torch.ops.aten.linear.default(clone_14, p_transformer_decoder_layers_0_linear2_weight, p_transformer_decoder_layers_0_linear2_bias);  clone_14 = p_transformer_decoder_layers_0_linear2_weight = p_transformer_decoder_layers_0_linear2_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_15: \"f32[s59, 1, 256]\" = torch.ops.aten.clone.default(linear_14);  linear_14 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1131 in forward, code: x = self.norm3(x + self._ff_block(x))\n",
                            "                    add_272: \"f32[s59, 1, 256]\" = torch.ops.aten.add.Tensor(layer_norm_6, clone_15);  layer_norm_6 = clone_15 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_7: \"f32[s59, 1, 256]\" = torch.ops.aten.layer_norm.default(add_272, [256], p_transformer_decoder_layers_0_norm3_weight, p_transformer_decoder_layers_0_norm3_bias);  add_272 = p_transformer_decoder_layers_0_norm3_weight = p_transformer_decoder_layers_0_norm3_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:1488 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
                            "                    linear_15: \"f32[s59, 1, 768]\" = torch.ops.aten.linear.default(layer_norm_7, p_transformer_decoder_layers_1_self_attn_in_proj_weight, p_transformer_decoder_layers_1_self_attn_in_proj_bias);  p_transformer_decoder_layers_1_self_attn_in_proj_weight = p_transformer_decoder_layers_1_self_attn_in_proj_bias = None\n",
                            "                    view_36: \"f32[s59, 1, 3, 256]\" = torch.ops.aten.view.default(linear_15, [sym_size_int_16, 1, 3, 256]);  linear_15 = None\n",
                            "                    unsqueeze_4: \"f32[1, s59, 1, 3, 256]\" = torch.ops.aten.unsqueeze.default(view_36, 0);  view_36 = None\n",
                            "                    transpose_16: \"f32[3, s59, 1, 1, 256]\" = torch.ops.aten.transpose.int(unsqueeze_4, 0, -2);  unsqueeze_4 = None\n",
                            "                    squeeze_4: \"f32[3, s59, 1, 256]\" = torch.ops.aten.squeeze.dim(transpose_16, -2);  transpose_16 = None\n",
                            "                    clone_16: \"f32[3, s59, 1, 256]\" = torch.ops.aten.clone.default(squeeze_4, memory_format = torch.contiguous_format);  squeeze_4 = None\n",
                            "                    select_11: \"f32[s59, 1, 256]\" = torch.ops.aten.select.int(clone_16, 0, 0)\n",
                            "                    select_12: \"f32[s59, 1, 256]\" = torch.ops.aten.select.int(clone_16, 0, 1)\n",
                            "                    select_13: \"f32[s59, 1, 256]\" = torch.ops.aten.select.int(clone_16, 0, 2);  clone_16 = None\n",
                            "                    view_37: \"f32[s59, 4, 64]\" = torch.ops.aten.view.default(select_11, [sym_size_int_16, 4, 64]);  select_11 = None\n",
                            "                    transpose_17: \"f32[4, s59, 64]\" = torch.ops.aten.transpose.int(view_37, 0, 1);  view_37 = None\n",
                            "                    view_38: \"f32[s59, 4, 64]\" = torch.ops.aten.view.default(select_12, [sym_size_int_16, 4, 64]);  select_12 = None\n",
                            "                    transpose_18: \"f32[4, s59, 64]\" = torch.ops.aten.transpose.int(view_38, 0, 1);  view_38 = None\n",
                            "                    view_39: \"f32[s59, 4, 64]\" = torch.ops.aten.view.default(select_13, [sym_size_int_16, 4, 64]);  select_13 = None\n",
                            "                    transpose_19: \"f32[4, s59, 64]\" = torch.ops.aten.transpose.int(view_39, 0, 1);  view_39 = None\n",
                            "                    view_40: \"f32[1, 4, s59, 64]\" = torch.ops.aten.view.default(transpose_17, [1, 4, sym_size_int_16, 64]);  transpose_17 = None\n",
                            "                    view_41: \"f32[1, 4, s59, 64]\" = torch.ops.aten.view.default(transpose_18, [1, 4, sym_size_int_16, 64]);  transpose_18 = None\n",
                            "                    view_42: \"f32[1, 4, s59, 64]\" = torch.ops.aten.view.default(transpose_19, [1, 4, sym_size_int_16, 64]);  transpose_19 = None\n",
                            "                    scaled_dot_product_attention_4: \"f32[1, 4, s59, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(view_40, view_41, view_42);  view_40 = view_41 = view_42 = None\n",
                            "                    permute_6: \"f32[s59, 1, 4, 64]\" = torch.ops.aten.permute.default(scaled_dot_product_attention_4, [2, 0, 1, 3]);  scaled_dot_product_attention_4 = None\n",
                            "                    view_43: \"f32[s59, 256]\" = torch.ops.aten.view.default(permute_6, [sym_size_int_16, 256]);  permute_6 = None\n",
                            "                    linear_16: \"f32[s59, 256]\" = torch.ops.aten.linear.default(view_43, p_transformer_decoder_layers_1_self_attn_out_proj_weight, p_transformer_decoder_layers_1_self_attn_out_proj_bias);  view_43 = p_transformer_decoder_layers_1_self_attn_out_proj_weight = p_transformer_decoder_layers_1_self_attn_out_proj_bias = None\n",
                            "                    view_44: \"f32[s59, 1, 256]\" = torch.ops.aten.view.default(linear_16, [sym_size_int_16, 1, 256]);  linear_16 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_17: \"f32[s59, 1, 256]\" = torch.ops.aten.clone.default(view_44);  view_44 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1123 in forward, code: x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n",
                            "                    add_373: \"f32[s59, 1, 256]\" = torch.ops.aten.add.Tensor(layer_norm_7, clone_17);  layer_norm_7 = clone_17 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_8: \"f32[s59, 1, 256]\" = torch.ops.aten.layer_norm.default(add_373, [256], p_transformer_decoder_layers_1_norm1_weight, p_transformer_decoder_layers_1_norm1_bias);  add_373 = p_transformer_decoder_layers_1_norm1_weight = p_transformer_decoder_layers_1_norm1_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:1488 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
                            "                    split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(p_transformer_decoder_layers_1_multihead_attn_in_proj_weight, [256, 512]);  p_transformer_decoder_layers_1_multihead_attn_in_proj_weight = None\n",
                            "                    getitem_66: \"f32[256, 256]\" = split_with_sizes_2[0]\n",
                            "                    getitem_67: \"f32[512, 256]\" = split_with_sizes_2[1];  split_with_sizes_2 = None\n",
                            "                    split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(p_transformer_decoder_layers_1_multihead_attn_in_proj_bias, [256, 512]);  p_transformer_decoder_layers_1_multihead_attn_in_proj_bias = None\n",
                            "                    getitem_68: \"f32[256]\" = split_with_sizes_3[0]\n",
                            "                    getitem_69: \"f32[512]\" = split_with_sizes_3[1];  split_with_sizes_3 = None\n",
                            "                    linear_17: \"f32[s59, 1, 256]\" = torch.ops.aten.linear.default(layer_norm_8, getitem_66, getitem_68);  getitem_66 = getitem_68 = None\n",
                            "                    linear_18: \"f32[8, 1, 512]\" = torch.ops.aten.linear.default(layer_norm_4, getitem_67, getitem_69);  layer_norm_4 = getitem_67 = getitem_69 = None\n",
                            "                    view_45: \"f32[8, 1, 2, 256]\" = torch.ops.aten.view.default(linear_18, [8, 1, 2, 256]);  linear_18 = None\n",
                            "                    unsqueeze_5: \"f32[1, 8, 1, 2, 256]\" = torch.ops.aten.unsqueeze.default(view_45, 0);  view_45 = None\n",
                            "                    transpose_20: \"f32[2, 8, 1, 1, 256]\" = torch.ops.aten.transpose.int(unsqueeze_5, 0, -2);  unsqueeze_5 = None\n",
                            "                    squeeze_5: \"f32[2, 8, 1, 256]\" = torch.ops.aten.squeeze.dim(transpose_20, -2);  transpose_20 = None\n",
                            "                    clone_18: \"f32[2, 8, 1, 256]\" = torch.ops.aten.clone.default(squeeze_5, memory_format = torch.contiguous_format);  squeeze_5 = None\n",
                            "                    select_14: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_18, 0, 0)\n",
                            "                    select_15: \"f32[8, 1, 256]\" = torch.ops.aten.select.int(clone_18, 0, 1);  clone_18 = None\n",
                            "                    view_46: \"f32[s59, 4, 64]\" = torch.ops.aten.view.default(linear_17, [sym_size_int_16, 4, 64]);  linear_17 = None\n",
                            "                    transpose_21: \"f32[4, s59, 64]\" = torch.ops.aten.transpose.int(view_46, 0, 1);  view_46 = None\n",
                            "                    view_47: \"f32[8, 4, 64]\" = torch.ops.aten.view.default(select_14, [8, 4, 64]);  select_14 = None\n",
                            "                    transpose_22: \"f32[4, 8, 64]\" = torch.ops.aten.transpose.int(view_47, 0, 1);  view_47 = None\n",
                            "                    view_48: \"f32[8, 4, 64]\" = torch.ops.aten.view.default(select_15, [8, 4, 64]);  select_15 = None\n",
                            "                    transpose_23: \"f32[4, 8, 64]\" = torch.ops.aten.transpose.int(view_48, 0, 1);  view_48 = None\n",
                            "                    view_49: \"f32[1, 4, s59, 64]\" = torch.ops.aten.view.default(transpose_21, [1, 4, sym_size_int_16, 64]);  transpose_21 = None\n",
                            "                    view_50: \"f32[1, 4, 8, 64]\" = torch.ops.aten.view.default(transpose_22, [1, 4, 8, 64]);  transpose_22 = None\n",
                            "                    view_51: \"f32[1, 4, 8, 64]\" = torch.ops.aten.view.default(transpose_23, [1, 4, 8, 64]);  transpose_23 = None\n",
                            "                    scaled_dot_product_attention_5: \"f32[1, 4, s59, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(view_49, view_50, view_51);  view_49 = view_50 = view_51 = None\n",
                            "                    permute_7: \"f32[s59, 1, 4, 64]\" = torch.ops.aten.permute.default(scaled_dot_product_attention_5, [2, 0, 1, 3]);  scaled_dot_product_attention_5 = None\n",
                            "                    view_52: \"f32[s59, 256]\" = torch.ops.aten.view.default(permute_7, [sym_size_int_16, 256]);  permute_7 = None\n",
                            "                    linear_19: \"f32[s59, 256]\" = torch.ops.aten.linear.default(view_52, p_transformer_decoder_layers_1_multihead_attn_out_proj_weight, p_transformer_decoder_layers_1_multihead_attn_out_proj_bias);  view_52 = p_transformer_decoder_layers_1_multihead_attn_out_proj_weight = p_transformer_decoder_layers_1_multihead_attn_out_proj_bias = None\n",
                            "                    view_53: \"f32[s59, 1, 256]\" = torch.ops.aten.view.default(linear_19, [sym_size_int_16, 1, 256]);  linear_19 = sym_size_int_16 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_19: \"f32[s59, 1, 256]\" = torch.ops.aten.clone.default(view_53);  view_53 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1126 in forward, code: x\n",
                            "                    add_418: \"f32[s59, 1, 256]\" = torch.ops.aten.add.Tensor(layer_norm_8, clone_19);  layer_norm_8 = clone_19 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_9: \"f32[s59, 1, 256]\" = torch.ops.aten.layer_norm.default(add_418, [256], p_transformer_decoder_layers_1_norm2_weight, p_transformer_decoder_layers_1_norm2_bias);  add_418 = p_transformer_decoder_layers_1_norm2_weight = p_transformer_decoder_layers_1_norm2_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_20: \"f32[s59, 1, 1024]\" = torch.ops.aten.linear.default(layer_norm_9, p_transformer_decoder_layers_1_linear1_weight, p_transformer_decoder_layers_1_linear1_bias);  p_transformer_decoder_layers_1_linear1_weight = p_transformer_decoder_layers_1_linear1_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1131 in forward, code: x = self.norm3(x + self._ff_block(x))\n",
                            "                    relu_20: \"f32[s59, 1, 1024]\" = torch.ops.aten.relu.default(linear_20);  linear_20 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_20: \"f32[s59, 1, 1024]\" = torch.ops.aten.clone.default(relu_20);  relu_20 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_21: \"f32[s59, 1, 256]\" = torch.ops.aten.linear.default(clone_20, p_transformer_decoder_layers_1_linear2_weight, p_transformer_decoder_layers_1_linear2_bias);  clone_20 = p_transformer_decoder_layers_1_linear2_weight = p_transformer_decoder_layers_1_linear2_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
                            "                    clone_21: \"f32[s59, 1, 256]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1131 in forward, code: x = self.norm3(x + self._ff_block(x))\n",
                            "                    add_447: \"f32[s59, 1, 256]\" = torch.ops.aten.add.Tensor(layer_norm_9, clone_21);  layer_norm_9 = clone_21 = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_10: \"f32[s59, 1, 256]\" = torch.ops.aten.layer_norm.default(add_447, [256], p_transformer_decoder_layers_1_norm3_weight, p_transformer_decoder_layers_1_norm3_bias);  add_447 = p_transformer_decoder_layers_1_norm3_weight = p_transformer_decoder_layers_1_norm3_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
                            "                    layer_norm_11: \"f32[s59, 1, 256]\" = torch.ops.aten.layer_norm.default(layer_norm_10, [256], p_transformer_decoder_norm_weight, p_transformer_decoder_norm_bias);  layer_norm_10 = p_transformer_decoder_norm_weight = p_transformer_decoder_norm_bias = None\n",
                            "            \n",
                            "                     # File: /opt/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
                            "                    linear_22: \"f32[s59, 1, 40]\" = torch.ops.aten.linear.default(layer_norm_11, p_fc_out_weight, p_fc_out_bias);  layer_norm_11 = p_fc_out_weight = p_fc_out_bias = None\n",
                            "            \n",
                            "                     # File: /Users/nikhil/Desktop/Leegality_task/model.py:142 in forward, code: return prediction.permute(1, 0, 2)\n",
                            "                    permute_8: \"f32[1, s59, 40]\" = torch.ops.aten.permute.default(linear_22, [1, 0, 2]);  linear_22 = None\n",
                            "                    return (permute_8,)\n",
                            "            \n",
                            "        Graph signature: \n",
                            "            # inputs\n",
                            "            p_feature_extractor_backbone_0_weight: PARAMETER target='feature_extractor.backbone.0.weight'\n",
                            "            p_feature_extractor_backbone_1_weight: PARAMETER target='feature_extractor.backbone.1.weight'\n",
                            "            p_feature_extractor_backbone_1_bias: PARAMETER target='feature_extractor.backbone.1.bias'\n",
                            "            p_feature_extractor_backbone_4_0_conv1_weight: PARAMETER target='feature_extractor.backbone.4.0.conv1.weight'\n",
                            "            p_feature_extractor_backbone_4_0_bn1_weight: PARAMETER target='feature_extractor.backbone.4.0.bn1.weight'\n",
                            "            p_feature_extractor_backbone_4_0_bn1_bias: PARAMETER target='feature_extractor.backbone.4.0.bn1.bias'\n",
                            "            p_feature_extractor_backbone_4_0_conv2_weight: PARAMETER target='feature_extractor.backbone.4.0.conv2.weight'\n",
                            "            p_feature_extractor_backbone_4_0_bn2_weight: PARAMETER target='feature_extractor.backbone.4.0.bn2.weight'\n",
                            "            p_feature_extractor_backbone_4_0_bn2_bias: PARAMETER target='feature_extractor.backbone.4.0.bn2.bias'\n",
                            "            p_feature_extractor_backbone_4_1_conv1_weight: PARAMETER target='feature_extractor.backbone.4.1.conv1.weight'\n",
                            "            p_feature_extractor_backbone_4_1_bn1_weight: PARAMETER target='feature_extractor.backbone.4.1.bn1.weight'\n",
                            "            p_feature_extractor_backbone_4_1_bn1_bias: PARAMETER target='feature_extractor.backbone.4.1.bn1.bias'\n",
                            "            p_feature_extractor_backbone_4_1_conv2_weight: PARAMETER target='feature_extractor.backbone.4.1.conv2.weight'\n",
                            "            p_feature_extractor_backbone_4_1_bn2_weight: PARAMETER target='feature_extractor.backbone.4.1.bn2.weight'\n",
                            "            p_feature_extractor_backbone_4_1_bn2_bias: PARAMETER target='feature_extractor.backbone.4.1.bn2.bias'\n",
                            "            p_feature_extractor_backbone_5_0_conv1_weight: PARAMETER target='feature_extractor.backbone.5.0.conv1.weight'\n",
                            "            p_feature_extractor_backbone_5_0_bn1_weight: PARAMETER target='feature_extractor.backbone.5.0.bn1.weight'\n",
                            "            p_feature_extractor_backbone_5_0_bn1_bias: PARAMETER target='feature_extractor.backbone.5.0.bn1.bias'\n",
                            "            p_feature_extractor_backbone_5_0_conv2_weight: PARAMETER target='feature_extractor.backbone.5.0.conv2.weight'\n",
                            "            p_feature_extractor_backbone_5_0_bn2_weight: PARAMETER target='feature_extractor.backbone.5.0.bn2.weight'\n",
                            "            p_feature_extractor_backbone_5_0_bn2_bias: PARAMETER target='feature_extractor.backbone.5.0.bn2.bias'\n",
                            "            p_feature_extractor_backbone_5_0_downsample_0_weight: PARAMETER target='feature_extractor.backbone.5.0.downsample.0.weight'\n",
                            "            p_feature_extractor_backbone_5_0_downsample_1_weight: PARAMETER target='feature_extractor.backbone.5.0.downsample.1.weight'\n",
                            "            p_feature_extractor_backbone_5_0_downsample_1_bias: PARAMETER target='feature_extractor.backbone.5.0.downsample.1.bias'\n",
                            "            p_feature_extractor_backbone_5_1_conv1_weight: PARAMETER target='feature_extractor.backbone.5.1.conv1.weight'\n",
                            "            p_feature_extractor_backbone_5_1_bn1_weight: PARAMETER target='feature_extractor.backbone.5.1.bn1.weight'\n",
                            "            p_feature_extractor_backbone_5_1_bn1_bias: PARAMETER target='feature_extractor.backbone.5.1.bn1.bias'\n",
                            "            p_feature_extractor_backbone_5_1_conv2_weight: PARAMETER target='feature_extractor.backbone.5.1.conv2.weight'\n",
                            "            p_feature_extractor_backbone_5_1_bn2_weight: PARAMETER target='feature_extractor.backbone.5.1.bn2.weight'\n",
                            "            p_feature_extractor_backbone_5_1_bn2_bias: PARAMETER target='feature_extractor.backbone.5.1.bn2.bias'\n",
                            "            p_feature_extractor_backbone_6_0_conv1_weight: PARAMETER target='feature_extractor.backbone.6.0.conv1.weight'\n",
                            "            p_feature_extractor_backbone_6_0_bn1_weight: PARAMETER target='feature_extractor.backbone.6.0.bn1.weight'\n",
                            "            p_feature_extractor_backbone_6_0_bn1_bias: PARAMETER target='feature_extractor.backbone.6.0.bn1.bias'\n",
                            "            p_feature_extractor_backbone_6_0_conv2_weight: PARAMETER target='feature_extractor.backbone.6.0.conv2.weight'\n",
                            "            p_feature_extractor_backbone_6_0_bn2_weight: PARAMETER target='feature_extractor.backbone.6.0.bn2.weight'\n",
                            "            p_feature_extractor_backbone_6_0_bn2_bias: PARAMETER target='feature_extractor.backbone.6.0.bn2.bias'\n",
                            "            p_feature_extractor_backbone_6_0_downsample_0_weight: PARAMETER target='feature_extractor.backbone.6.0.downsample.0.weight'\n",
                            "            p_feature_extractor_backbone_6_0_downsample_1_weight: PARAMETER target='feature_extractor.backbone.6.0.downsample.1.weight'\n",
                            "            p_feature_extractor_backbone_6_0_downsample_1_bias: PARAMETER target='feature_extractor.backbone.6.0.downsample.1.bias'\n",
                            "            p_feature_extractor_backbone_6_1_conv1_weight: PARAMETER target='feature_extractor.backbone.6.1.conv1.weight'\n",
                            "            p_feature_extractor_backbone_6_1_bn1_weight: PARAMETER target='feature_extractor.backbone.6.1.bn1.weight'\n",
                            "            p_feature_extractor_backbone_6_1_bn1_bias: PARAMETER target='feature_extractor.backbone.6.1.bn1.bias'\n",
                            "            p_feature_extractor_backbone_6_1_conv2_weight: PARAMETER target='feature_extractor.backbone.6.1.conv2.weight'\n",
                            "            p_feature_extractor_backbone_6_1_bn2_weight: PARAMETER target='feature_extractor.backbone.6.1.bn2.weight'\n",
                            "            p_feature_extractor_backbone_6_1_bn2_bias: PARAMETER target='feature_extractor.backbone.6.1.bn2.bias'\n",
                            "            p_feature_extractor_backbone_7_0_conv1_weight: PARAMETER target='feature_extractor.backbone.7.0.conv1.weight'\n",
                            "            p_feature_extractor_backbone_7_0_bn1_weight: PARAMETER target='feature_extractor.backbone.7.0.bn1.weight'\n",
                            "            p_feature_extractor_backbone_7_0_bn1_bias: PARAMETER target='feature_extractor.backbone.7.0.bn1.bias'\n",
                            "            p_feature_extractor_backbone_7_0_conv2_weight: PARAMETER target='feature_extractor.backbone.7.0.conv2.weight'\n",
                            "            p_feature_extractor_backbone_7_0_bn2_weight: PARAMETER target='feature_extractor.backbone.7.0.bn2.weight'\n",
                            "            p_feature_extractor_backbone_7_0_bn2_bias: PARAMETER target='feature_extractor.backbone.7.0.bn2.bias'\n",
                            "            p_feature_extractor_backbone_7_0_downsample_0_weight: PARAMETER target='feature_extractor.backbone.7.0.downsample.0.weight'\n",
                            "            p_feature_extractor_backbone_7_0_downsample_1_weight: PARAMETER target='feature_extractor.backbone.7.0.downsample.1.weight'\n",
                            "            p_feature_extractor_backbone_7_0_downsample_1_bias: PARAMETER target='feature_extractor.backbone.7.0.downsample.1.bias'\n",
                            "            p_feature_extractor_backbone_7_1_conv1_weight: PARAMETER target='feature_extractor.backbone.7.1.conv1.weight'\n",
                            "            p_feature_extractor_backbone_7_1_bn1_weight: PARAMETER target='feature_extractor.backbone.7.1.bn1.weight'\n",
                            "            p_feature_extractor_backbone_7_1_bn1_bias: PARAMETER target='feature_extractor.backbone.7.1.bn1.bias'\n",
                            "            p_feature_extractor_backbone_7_1_conv2_weight: PARAMETER target='feature_extractor.backbone.7.1.conv2.weight'\n",
                            "            p_feature_extractor_backbone_7_1_bn2_weight: PARAMETER target='feature_extractor.backbone.7.1.bn2.weight'\n",
                            "            p_feature_extractor_backbone_7_1_bn2_bias: PARAMETER target='feature_extractor.backbone.7.1.bn2.bias'\n",
                            "            p_adapter_weight: PARAMETER target='adapter.weight'\n",
                            "            p_adapter_bias: PARAMETER target='adapter.bias'\n",
                            "            p_transformer_encoder_layers_0_self_attn_in_proj_weight: PARAMETER target='transformer.encoder.layers.0.self_attn.in_proj_weight'\n",
                            "            p_transformer_encoder_layers_0_self_attn_in_proj_bias: PARAMETER target='transformer.encoder.layers.0.self_attn.in_proj_bias'\n",
                            "            p_transformer_encoder_layers_0_self_attn_out_proj_weight: PARAMETER target='transformer.encoder.layers.0.self_attn.out_proj.weight'\n",
                            "            p_transformer_encoder_layers_0_self_attn_out_proj_bias: PARAMETER target='transformer.encoder.layers.0.self_attn.out_proj.bias'\n",
                            "            p_transformer_encoder_layers_0_linear1_weight: PARAMETER target='transformer.encoder.layers.0.linear1.weight'\n",
                            "            p_transformer_encoder_layers_0_linear1_bias: PARAMETER target='transformer.encoder.layers.0.linear1.bias'\n",
                            "            p_transformer_encoder_layers_0_linear2_weight: PARAMETER target='transformer.encoder.layers.0.linear2.weight'\n",
                            "            p_transformer_encoder_layers_0_linear2_bias: PARAMETER target='transformer.encoder.layers.0.linear2.bias'\n",
                            "            p_transformer_encoder_layers_0_norm1_weight: PARAMETER target='transformer.encoder.layers.0.norm1.weight'\n",
                            "            p_transformer_encoder_layers_0_norm1_bias: PARAMETER target='transformer.encoder.layers.0.norm1.bias'\n",
                            "            p_transformer_encoder_layers_0_norm2_weight: PARAMETER target='transformer.encoder.layers.0.norm2.weight'\n",
                            "            p_transformer_encoder_layers_0_norm2_bias: PARAMETER target='transformer.encoder.layers.0.norm2.bias'\n",
                            "            p_transformer_encoder_layers_1_self_attn_in_proj_weight: PARAMETER target='transformer.encoder.layers.1.self_attn.in_proj_weight'\n",
                            "            p_transformer_encoder_layers_1_self_attn_in_proj_bias: PARAMETER target='transformer.encoder.layers.1.self_attn.in_proj_bias'\n",
                            "            p_transformer_encoder_layers_1_self_attn_out_proj_weight: PARAMETER target='transformer.encoder.layers.1.self_attn.out_proj.weight'\n",
                            "            p_transformer_encoder_layers_1_self_attn_out_proj_bias: PARAMETER target='transformer.encoder.layers.1.self_attn.out_proj.bias'\n",
                            "            p_transformer_encoder_layers_1_linear1_weight: PARAMETER target='transformer.encoder.layers.1.linear1.weight'\n",
                            "            p_transformer_encoder_layers_1_linear1_bias: PARAMETER target='transformer.encoder.layers.1.linear1.bias'\n",
                            "            p_transformer_encoder_layers_1_linear2_weight: PARAMETER target='transformer.encoder.layers.1.linear2.weight'\n",
                            "            p_transformer_encoder_layers_1_linear2_bias: PARAMETER target='transformer.encoder.layers.1.linear2.bias'\n",
                            "            p_transformer_encoder_layers_1_norm1_weight: PARAMETER target='transformer.encoder.layers.1.norm1.weight'\n",
                            "            p_transformer_encoder_layers_1_norm1_bias: PARAMETER target='transformer.encoder.layers.1.norm1.bias'\n",
                            "            p_transformer_encoder_layers_1_norm2_weight: PARAMETER target='transformer.encoder.layers.1.norm2.weight'\n",
                            "            p_transformer_encoder_layers_1_norm2_bias: PARAMETER target='transformer.encoder.layers.1.norm2.bias'\n",
                            "            p_transformer_encoder_norm_weight: PARAMETER target='transformer.encoder.norm.weight'\n",
                            "            p_transformer_encoder_norm_bias: PARAMETER target='transformer.encoder.norm.bias'\n",
                            "            p_transformer_decoder_layers_0_self_attn_in_proj_weight: PARAMETER target='transformer.decoder.layers.0.self_attn.in_proj_weight'\n",
                            "            p_transformer_decoder_layers_0_self_attn_in_proj_bias: PARAMETER target='transformer.decoder.layers.0.self_attn.in_proj_bias'\n",
                            "            p_transformer_decoder_layers_0_self_attn_out_proj_weight: PARAMETER target='transformer.decoder.layers.0.self_attn.out_proj.weight'\n",
                            "            p_transformer_decoder_layers_0_self_attn_out_proj_bias: PARAMETER target='transformer.decoder.layers.0.self_attn.out_proj.bias'\n",
                            "            p_transformer_decoder_layers_0_multihead_attn_in_proj_weight: PARAMETER target='transformer.decoder.layers.0.multihead_attn.in_proj_weight'\n",
                            "            p_transformer_decoder_layers_0_multihead_attn_in_proj_bias: PARAMETER target='transformer.decoder.layers.0.multihead_attn.in_proj_bias'\n",
                            "            p_transformer_decoder_layers_0_multihead_attn_out_proj_weight: PARAMETER target='transformer.decoder.layers.0.multihead_attn.out_proj.weight'\n",
                            "            p_transformer_decoder_layers_0_multihead_attn_out_proj_bias: PARAMETER target='transformer.decoder.layers.0.multihead_attn.out_proj.bias'\n",
                            "            p_transformer_decoder_layers_0_linear1_weight: PARAMETER target='transformer.decoder.layers.0.linear1.weight'\n",
                            "            p_transformer_decoder_layers_0_linear1_bias: PARAMETER target='transformer.decoder.layers.0.linear1.bias'\n",
                            "            p_transformer_decoder_layers_0_linear2_weight: PARAMETER target='transformer.decoder.layers.0.linear2.weight'\n",
                            "            p_transformer_decoder_layers_0_linear2_bias: PARAMETER target='transformer.decoder.layers.0.linear2.bias'\n",
                            "            p_transformer_decoder_layers_0_norm1_weight: PARAMETER target='transformer.decoder.layers.0.norm1.weight'\n",
                            "            p_transformer_decoder_layers_0_norm1_bias: PARAMETER target='transformer.decoder.layers.0.norm1.bias'\n",
                            "            p_transformer_decoder_layers_0_norm2_weight: PARAMETER target='transformer.decoder.layers.0.norm2.weight'\n",
                            "            p_transformer_decoder_layers_0_norm2_bias: PARAMETER target='transformer.decoder.layers.0.norm2.bias'\n",
                            "            p_transformer_decoder_layers_0_norm3_weight: PARAMETER target='transformer.decoder.layers.0.norm3.weight'\n",
                            "            p_transformer_decoder_layers_0_norm3_bias: PARAMETER target='transformer.decoder.layers.0.norm3.bias'\n",
                            "            p_transformer_decoder_layers_1_self_attn_in_proj_weight: PARAMETER target='transformer.decoder.layers.1.self_attn.in_proj_weight'\n",
                            "            p_transformer_decoder_layers_1_self_attn_in_proj_bias: PARAMETER target='transformer.decoder.layers.1.self_attn.in_proj_bias'\n",
                            "            p_transformer_decoder_layers_1_self_attn_out_proj_weight: PARAMETER target='transformer.decoder.layers.1.self_attn.out_proj.weight'\n",
                            "            p_transformer_decoder_layers_1_self_attn_out_proj_bias: PARAMETER target='transformer.decoder.layers.1.self_attn.out_proj.bias'\n",
                            "            p_transformer_decoder_layers_1_multihead_attn_in_proj_weight: PARAMETER target='transformer.decoder.layers.1.multihead_attn.in_proj_weight'\n",
                            "            p_transformer_decoder_layers_1_multihead_attn_in_proj_bias: PARAMETER target='transformer.decoder.layers.1.multihead_attn.in_proj_bias'\n",
                            "            p_transformer_decoder_layers_1_multihead_attn_out_proj_weight: PARAMETER target='transformer.decoder.layers.1.multihead_attn.out_proj.weight'\n",
                            "            p_transformer_decoder_layers_1_multihead_attn_out_proj_bias: PARAMETER target='transformer.decoder.layers.1.multihead_attn.out_proj.bias'\n",
                            "            p_transformer_decoder_layers_1_linear1_weight: PARAMETER target='transformer.decoder.layers.1.linear1.weight'\n",
                            "            p_transformer_decoder_layers_1_linear1_bias: PARAMETER target='transformer.decoder.layers.1.linear1.bias'\n",
                            "            p_transformer_decoder_layers_1_linear2_weight: PARAMETER target='transformer.decoder.layers.1.linear2.weight'\n",
                            "            p_transformer_decoder_layers_1_linear2_bias: PARAMETER target='transformer.decoder.layers.1.linear2.bias'\n",
                            "            p_transformer_decoder_layers_1_norm1_weight: PARAMETER target='transformer.decoder.layers.1.norm1.weight'\n",
                            "            p_transformer_decoder_layers_1_norm1_bias: PARAMETER target='transformer.decoder.layers.1.norm1.bias'\n",
                            "            p_transformer_decoder_layers_1_norm2_weight: PARAMETER target='transformer.decoder.layers.1.norm2.weight'\n",
                            "            p_transformer_decoder_layers_1_norm2_bias: PARAMETER target='transformer.decoder.layers.1.norm2.bias'\n",
                            "            p_transformer_decoder_layers_1_norm3_weight: PARAMETER target='transformer.decoder.layers.1.norm3.weight'\n",
                            "            p_transformer_decoder_layers_1_norm3_bias: PARAMETER target='transformer.decoder.layers.1.norm3.bias'\n",
                            "            p_transformer_decoder_norm_weight: PARAMETER target='transformer.decoder.norm.weight'\n",
                            "            p_transformer_decoder_norm_bias: PARAMETER target='transformer.decoder.norm.bias'\n",
                            "            p_embedding_weight: PARAMETER target='embedding.weight'\n",
                            "            p_fc_out_weight: PARAMETER target='fc_out.weight'\n",
                            "            p_fc_out_bias: PARAMETER target='fc_out.bias'\n",
                            "            b_feature_extractor_backbone_1_running_mean: BUFFER target='feature_extractor.backbone.1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_1_running_var: BUFFER target='feature_extractor.backbone.1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_1_num_batches_tracked: BUFFER target='feature_extractor.backbone.1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_4_0_bn1_running_mean: BUFFER target='feature_extractor.backbone.4.0.bn1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_4_0_bn1_running_var: BUFFER target='feature_extractor.backbone.4.0.bn1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_4_0_bn1_num_batches_tracked: BUFFER target='feature_extractor.backbone.4.0.bn1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_4_0_bn2_running_mean: BUFFER target='feature_extractor.backbone.4.0.bn2.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_4_0_bn2_running_var: BUFFER target='feature_extractor.backbone.4.0.bn2.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_4_0_bn2_num_batches_tracked: BUFFER target='feature_extractor.backbone.4.0.bn2.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_4_1_bn1_running_mean: BUFFER target='feature_extractor.backbone.4.1.bn1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_4_1_bn1_running_var: BUFFER target='feature_extractor.backbone.4.1.bn1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_4_1_bn1_num_batches_tracked: BUFFER target='feature_extractor.backbone.4.1.bn1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_4_1_bn2_running_mean: BUFFER target='feature_extractor.backbone.4.1.bn2.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_4_1_bn2_running_var: BUFFER target='feature_extractor.backbone.4.1.bn2.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_4_1_bn2_num_batches_tracked: BUFFER target='feature_extractor.backbone.4.1.bn2.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_bn1_running_mean: BUFFER target='feature_extractor.backbone.5.0.bn1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_bn1_running_var: BUFFER target='feature_extractor.backbone.5.0.bn1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_bn1_num_batches_tracked: BUFFER target='feature_extractor.backbone.5.0.bn1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_bn2_running_mean: BUFFER target='feature_extractor.backbone.5.0.bn2.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_bn2_running_var: BUFFER target='feature_extractor.backbone.5.0.bn2.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_bn2_num_batches_tracked: BUFFER target='feature_extractor.backbone.5.0.bn2.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_downsample_1_running_mean: BUFFER target='feature_extractor.backbone.5.0.downsample.1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_downsample_1_running_var: BUFFER target='feature_extractor.backbone.5.0.downsample.1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_5_0_downsample_1_num_batches_tracked: BUFFER target='feature_extractor.backbone.5.0.downsample.1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_5_1_bn1_running_mean: BUFFER target='feature_extractor.backbone.5.1.bn1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_5_1_bn1_running_var: BUFFER target='feature_extractor.backbone.5.1.bn1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_5_1_bn1_num_batches_tracked: BUFFER target='feature_extractor.backbone.5.1.bn1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_5_1_bn2_running_mean: BUFFER target='feature_extractor.backbone.5.1.bn2.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_5_1_bn2_running_var: BUFFER target='feature_extractor.backbone.5.1.bn2.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_5_1_bn2_num_batches_tracked: BUFFER target='feature_extractor.backbone.5.1.bn2.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_bn1_running_mean: BUFFER target='feature_extractor.backbone.6.0.bn1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_bn1_running_var: BUFFER target='feature_extractor.backbone.6.0.bn1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_bn1_num_batches_tracked: BUFFER target='feature_extractor.backbone.6.0.bn1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_bn2_running_mean: BUFFER target='feature_extractor.backbone.6.0.bn2.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_bn2_running_var: BUFFER target='feature_extractor.backbone.6.0.bn2.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_bn2_num_batches_tracked: BUFFER target='feature_extractor.backbone.6.0.bn2.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_downsample_1_running_mean: BUFFER target='feature_extractor.backbone.6.0.downsample.1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_downsample_1_running_var: BUFFER target='feature_extractor.backbone.6.0.downsample.1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_6_0_downsample_1_num_batches_tracked: BUFFER target='feature_extractor.backbone.6.0.downsample.1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_6_1_bn1_running_mean: BUFFER target='feature_extractor.backbone.6.1.bn1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_6_1_bn1_running_var: BUFFER target='feature_extractor.backbone.6.1.bn1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_6_1_bn1_num_batches_tracked: BUFFER target='feature_extractor.backbone.6.1.bn1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_6_1_bn2_running_mean: BUFFER target='feature_extractor.backbone.6.1.bn2.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_6_1_bn2_running_var: BUFFER target='feature_extractor.backbone.6.1.bn2.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_6_1_bn2_num_batches_tracked: BUFFER target='feature_extractor.backbone.6.1.bn2.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_bn1_running_mean: BUFFER target='feature_extractor.backbone.7.0.bn1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_bn1_running_var: BUFFER target='feature_extractor.backbone.7.0.bn1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_bn1_num_batches_tracked: BUFFER target='feature_extractor.backbone.7.0.bn1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_bn2_running_mean: BUFFER target='feature_extractor.backbone.7.0.bn2.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_bn2_running_var: BUFFER target='feature_extractor.backbone.7.0.bn2.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_bn2_num_batches_tracked: BUFFER target='feature_extractor.backbone.7.0.bn2.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_downsample_1_running_mean: BUFFER target='feature_extractor.backbone.7.0.downsample.1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_downsample_1_running_var: BUFFER target='feature_extractor.backbone.7.0.downsample.1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_7_0_downsample_1_num_batches_tracked: BUFFER target='feature_extractor.backbone.7.0.downsample.1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_7_1_bn1_running_mean: BUFFER target='feature_extractor.backbone.7.1.bn1.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_7_1_bn1_running_var: BUFFER target='feature_extractor.backbone.7.1.bn1.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_7_1_bn1_num_batches_tracked: BUFFER target='feature_extractor.backbone.7.1.bn1.num_batches_tracked' persistent=True\n",
                            "            b_feature_extractor_backbone_7_1_bn2_running_mean: BUFFER target='feature_extractor.backbone.7.1.bn2.running_mean' persistent=True\n",
                            "            b_feature_extractor_backbone_7_1_bn2_running_var: BUFFER target='feature_extractor.backbone.7.1.bn2.running_var' persistent=True\n",
                            "            b_feature_extractor_backbone_7_1_bn2_num_batches_tracked: BUFFER target='feature_extractor.backbone.7.1.bn2.num_batches_tracked' persistent=True\n",
                            "            b_pos_encoder_pe: BUFFER target='pos_encoder.pe' persistent=True\n",
                            "            images: USER_INPUT\n",
                            "            tgt: USER_INPUT\n",
                            "    \n",
                            "            # outputs\n",
                            "            permute_8: USER_OUTPUT\n",
                            "    \n",
                            "        Range constraints: {s34: VR[1, 2], s59: VR[2, 32]}\n",
                            "\n",
                            ")"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "# 2. Create dummy inputs\n",
                "batch_size = 1\n",
                "dummy_image = torch.randn(batch_size, 3, 64, 256)\n",
                "# Use a length > 1 for the dummy target to help ONNX trace the sequence dimension\n",
                "dummy_tgt = torch.zeros((batch_size, 5), dtype=torch.long) \n",
                "\n",
                "# 3. Export with Dynamic Axes\n",
                "torch.onnx.export(\n",
                "    model, \n",
                "    (dummy_image, dummy_tgt), \n",
                "    \"ocr_model.onnx\",\n",
                "    export_params=True,\n",
                "    opset_version=14,          # Opset 14+ handles dynamic Transformer reshapes better\n",
                "    do_constant_folding=True,\n",
                "    input_names=['images', 'tgt'], \n",
                "    output_names=['output'],\n",
                "    dynamic_axes={\n",
                "        'images': {0: 'batch_size'}, # Image width/height can also be made dynamic if needed\n",
                "        'tgt': {0: 'batch_size', 1: 'seq_len'}, # THIS is the critical fix\n",
                "        'output': {0: 'batch_size', 1: 'seq_len'}\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "myenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
